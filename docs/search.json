[
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "I have journeyed the inner sanctums within the realm of Data Analytics for 8 years. In the spirit of fellowship, I have collaborated with cross-functional teams, ensuring that every stakeholder, from the boardroom to the front lines, understands the story that the data tells. Through trials and tribulations, much like the battles against Sauron’s forces, I have developed a keen eye for detecting anomalies and opportunities, ensuring the integrity and accuracy of the insights provided.\nMy hope is to glean any insights or experiences you wish to share on this quest.\nA peculiar fact about me is that I have no food allergies.\n\n\n\nClick icon to view Holly from the Shire\n\n\nMuch like the hidden gems of Middle-earth, this subreddit is filled with data visualizations guaranteed to captivate your interests: Link"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "Placeholder file for the future R coding exercise."
  },
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Data Exercise - Assignment #4",
    "section": "",
    "text": "For this assigment, I am choosing Option 1: Using a complex data (in this case a Text dataset)\nFirst, I need to install the tidytext package and in this case I’ll be using a text dataset from the janeaustenr package available in R that contains the 6 different novels written by Jane Austen. I’m using the example available in the Complex Data Types unit.\n\nlibrary(tidytext)\n\nWarning: package 'tidytext' was built under R version 4.3.3\n\nlibrary(janeaustenr)\n\nWarning: package 'janeaustenr' was built under R version 4.3.3\n\n\nI will also need the dplyr package for some of the functions it offers (such as pipes) in making data manipulation easier\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nhead(austen_books)\n\n                                                                                                            \n1 function ()                                                                                               \n2 {                                                                                                         \n3     books &lt;- list(`Sense & Sensibility` = janeaustenr::sensesensibility,                                  \n4         `Pride & Prejudice` = janeaustenr::prideprejudice, `Mansfield Park` = janeaustenr::mansfieldpark, \n5         Emma = janeaustenr::emma, `Northanger Abbey` = janeaustenr::northangerabbey,                      \n6         Persuasion = janeaustenr::persuasion)                                                             \n\n\nI’m going to create two new columns for ‘book’ and ‘line’ from the dataset by first grouping the data by book,and using ‘mutate’ to transform the grouped books into a line number within each book. Basically numbering the lines within the books. We can then ungroup the dataset as we have our lines:\n\noriginal_books &lt;- austen_books() %&gt;%\n  group_by(book) %&gt;%\n  mutate(line = row_number()) %&gt;%\n  ungroup()\n\noriginal_books\n\n# A tibble: 73,422 × 3\n   text                    book                 line\n   &lt;chr&gt;                   &lt;fct&gt;               &lt;int&gt;\n 1 \"SENSE AND SENSIBILITY\" Sense & Sensibility     1\n 2 \"\"                      Sense & Sensibility     2\n 3 \"by Jane Austen\"        Sense & Sensibility     3\n 4 \"\"                      Sense & Sensibility     4\n 5 \"(1811)\"                Sense & Sensibility     5\n 6 \"\"                      Sense & Sensibility     6\n 7 \"\"                      Sense & Sensibility     7\n 8 \"\"                      Sense & Sensibility     8\n 9 \"\"                      Sense & Sensibility     9\n10 \"CHAPTER 1\"             Sense & Sensibility    10\n# ℹ 73,412 more rows\n\n\nUsing the ‘unnest_tokens’ function from tidytext we can convert the text into “tokens” or individual words:\n\ntidy_books &lt;- original_books %&gt;%\n  unnest_tokens(word, text)\n\ntidy_books\n\n# A tibble: 725,055 × 3\n   book                 line word       \n   &lt;fct&gt;               &lt;int&gt; &lt;chr&gt;      \n 1 Sense & Sensibility     1 sense      \n 2 Sense & Sensibility     1 and        \n 3 Sense & Sensibility     1 sensibility\n 4 Sense & Sensibility     3 by         \n 5 Sense & Sensibility     3 jane       \n 6 Sense & Sensibility     3 austen     \n 7 Sense & Sensibility     5 1811       \n 8 Sense & Sensibility    10 chapter    \n 9 Sense & Sensibility    10 1          \n10 Sense & Sensibility    13 the        \n# ℹ 725,045 more rows\n\n\nI use the ‘anti_join(get_stopwords())’ functions together next to remove “stop words” from the dataset such as “the”, “in”, and “is”. For this you will have to use the “stopwords” package to identify those words.\n\nlibrary(stopwords)\n\nWarning: package 'stopwords' was built under R version 4.3.3\n\n\n\ntidy_books &lt;- tidy_books %&gt;%\n  anti_join(get_stopwords(), by = \"word\")\n\nNow to perform some exploratory analysis with the data set.\nlet’s install ‘ggplot2’ for our visualizations\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nIn this resource example, we can assign sentiment (positive, negative, and neutral) to the words used within the novels to perhaps get an overall idea of the emotion/feelings/theme within the novels - for this we need the ‘bing’ package for lexicon/sentiment:\n\nlibrary(tidyr)\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\nget_sentiments(\"bing\")\n\n# A tibble: 6,786 × 2\n   word        sentiment\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 2-faces     negative \n 2 abnormal    negative \n 3 abolish     negative \n 4 abominable  negative \n 5 abominably  negative \n 6 abominate   negative \n 7 abomination negative \n 8 abort       negative \n 9 aborted     negative \n10 aborts      negative \n# ℹ 6,776 more rows\n\n\nApplying the lexicon/sentiment dataset to the text dataset using an inner join and creating new columns to represent the count of negative/positive and sentiment.\n\njaneaustensentiment &lt;- tidy_books %&gt;%\n  inner_join(get_sentiments(\"bing\"), by = \"word\", relationship = \"many-to-many\") %&gt;% \n  count(book, index = line %/% 80, sentiment) %&gt;% \n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% \n  mutate(sentiment = positive - negative)\n\njaneaustensentiment\n\n# A tibble: 920 × 5\n   book                index negative positive sentiment\n   &lt;fct&gt;               &lt;dbl&gt;    &lt;int&gt;    &lt;int&gt;     &lt;int&gt;\n 1 Sense & Sensibility     0       16       32        16\n 2 Sense & Sensibility     1       19       53        34\n 3 Sense & Sensibility     2       12       31        19\n 4 Sense & Sensibility     3       15       31        16\n 5 Sense & Sensibility     4       16       34        18\n 6 Sense & Sensibility     5       16       51        35\n 7 Sense & Sensibility     6       24       40        16\n 8 Sense & Sensibility     7       23       51        28\n 9 Sense & Sensibility     8       30       40        10\n10 Sense & Sensibility     9       15       19         4\n# ℹ 910 more rows\n\n\nWe can now graph our sentiment count results using ggplot:\n\nggplot(janeaustensentiment, aes(index, sentiment, fill = book)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(vars(book), ncol = 2, scales = \"free_x\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My website and data analysis portfolio",
    "section": "",
    "text": "Greetings, friend! Welcome to my little corner of the web.\n\nI am Holly Milazzo, and I’m delighted your journey has led you here.\nPlease, feel free to explore my website and data analysis portfolio.\n\nUse the Menu Bar above to look around.\nI’ve gathered many stories and wonders along my travels.\n\nMay your visit be filled with joy and discovery, and may you always find what you seek!"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html",
    "href": "presentation-exercise/presentation-exercise.html",
    "title": "Presentation Exercise",
    "section": "",
    "text": "In this exercise, I recreated the bar graph that illustrates the share of political donations from team owners in six major sports leagues (NFL, NBA, WNBA, NHL, MLB, and NASCAR) to the Republican and Democratic parties over three election years: 2016, 2018, and 2020 from the FiveThirtyEight page here Link. The graph shows a significant majority of donations going to the Republican party in all three years. The annotation highlights that Giants owner Charles Johnson’s contributions constitute a substantial portion of the total Republican donations, underscoring the influence of individual donors in political contributions.\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\n\n# Important\n# Define the data location\ndata_location_sports &lt;- here::here(\"presentation-exercise\", \"sports-political-donations.csv\")\n\n# Read the CSV data\nrawdata_sports &lt;- read_csv(data_location_sports, col_names = FALSE)\n\nRows: 2799 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): X1, X2, X3, X4, X5, X6, X7\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Preview the data\nhead(rawdata_sports)\n\n# A tibble: 6 × 7\n  X1          X2           X3     X4                           X5    X6    X7   \n  &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt;                        &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 Owner       Team         League Recipient                    Amou… Elec… Party\n2 Adam Silver Commissioner NBA    WRIGHT 2016                  $4,0… 2016  Demo…\n3 Adam Silver Commissioner NBA    BIDEN FOR PRESIDENT          $2,8… 2020  Demo…\n4 Adam Silver Commissioner NBA    CORY 2020                    $2,7… 2020  Demo…\n5 Adam Silver Commissioner NBA    Kamala Harris for the People $2,7… 2020  Demo…\n6 Adam Silver Commissioner NBA    Win The Era PAC              $2,7… 2020  Demo…\n\n\n\n# Assign proper column names\ncolnames(rawdata_sports) &lt;- c(\"Owner\", \"Team\", \"League\", \"Recipient\", \"Amount\", \"Election_Year\", \"Party\")\n\n# Clean up the Amount column (remove '$' and convert to numeric)\nrawdata_sports$Amount &lt;- as.numeric(gsub(\"[\\\\$,]\", \"\", rawdata_sports$Amount))\n\nWarning: NAs introduced by coercion\n\n# Filter out rows where Party is not 'Democrat' or 'Republican'\nrawdata_sports &lt;- rawdata_sports[rawdata_sports$Party %in% c(\"Democrat\", \"Republican\"), ]\n\n# Summarize data to calculate total donations by Party and Year\ndonations_summary &lt;- aggregate(rawdata_sports$Amount, \n                               by = list(Election_Year = rawdata_sports$Election_Year, Party = rawdata_sports$Party), \n                               FUN = sum)\ncolnames(donations_summary)[3] &lt;- \"Total_Amount\"\n\n# Convert Party to factor and specify order (Democrat first for better visualization)\ndonations_summary$Party &lt;- factor(donations_summary$Party, levels = c(\"Democrat\", \"Republican\"))\n\n# Calculate the proportion of total donations by year\ntotal_by_year &lt;- aggregate(donations_summary$Total_Amount, \n                           by = list(Election_Year = donations_summary$Election_Year), \n                           FUN = sum)\ncolnames(total_by_year)[2] &lt;- \"Yearly_Total\"\n\ndonations_summary &lt;- merge(donations_summary, total_by_year, by = \"Election_Year\")\ndonations_summary$Prop_Amount &lt;- donations_summary$Total_Amount / donations_summary$Yearly_Total\n\n# Plot the data\nggplot(donations_summary, aes(x = as.factor(Election_Year), y = Prop_Amount, fill = Party)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  scale_y_continuous(labels = scales::percent) +\n  scale_fill_manual(values = c(\"Democrat\" = \"blue\", \"Republican\" = \"red\")) +\n  labs(\n    title = \"Team owners give largely to the GOP\",\n    subtitle = \"Share of donations from team owners in six leagues, per year and party\",\n    x = \"\",\n    y = \"Share of Donations\",\n    fill = \"Donations to\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.background = element_rect(fill = \"gray90\"),\n    plot.background = element_rect(fill = \"gray90\"),\n    axis.text.x = element_text(angle = 0, hjust = 0.5),\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  ) +\n  annotate(\"text\", x = 1.5, y = 1.05, label = \"Giants owner Charles Johnson’s total contributions make up 32.1% of all Republican contributions.\", size = 3, hjust = 0.3, vjust = 0)\n\n\n\n\nTable: Partisan Contributions by League\nThe table breaks down the total political contributions from owners and commissioners in six sports leagues (MLB, NBA, NHL, NFL, WNBA, and NASCAR) to the Republican and Democratic parties from 2016 to 2020. It reveals that MLB owners have donated the most overall, with significant amounts going to both parties, but predominantly to Republicans. The table provides a clear comparison of partisan contributions across different leagues, highlighting the disparity in political support within the sports industry.\nBoth visuals emphasize the substantial financial support sports team owners provide to political parties, predominantly favoring the Republican party, and the considerable influence of a few key donors.\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(ggtext)\n\n# Define the data for the table\ndata_table &lt;- data.frame(\n  League = c(\"MLB\", \"NBA\", \"NHL\", \"NFL\", \"WNBA\", \"NASCAR\"),\n  To_Republicans = c(\"$15,181,761\", \"$8,372,300\", \"$7,087,116\", \"$5,032,470\", \"$1,338,459\", \"$576,110\"),\n  To_Democrats = c(\"$5,184,604\", \"$2,641,487\", \"$1,726,733\", \"$873,500\", \"$1,634,153\", \"$93,983\"),\n  Total = c(\"$20,366,365\", \"$11,013,787\", \"$8,813,849\", \"$5,905,970\", \"$2,972,612\", \"$670,093\")\n)\n\n# Create a base plot\np &lt;- ggplot(data_table, aes(x = 1, y = League)) +\n  geom_text(aes(label = To_Republicans, x = 2), hjust = 0, color = \"red\") +\n  geom_text(aes(label = To_Democrats, x = 3), hjust = 0, color = \"blue\") +\n  geom_text(aes(label = Total, x = 4), hjust = 0) +\n  geom_text(aes(label = League, x = 0), hjust = 0, fontface = \"bold\") +\n  scale_x_continuous(limits = c(-0.5, 4.5), breaks = 0:4, labels = c(\"LEAGUE\", \"TO REPUBLICANS\", \"TO DEMOCRATS\", \"TOTAL\", \"\")) +\n  theme_void() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 10, hjust = 0.5),\n    plot.caption = element_text(size = 8, hjust = 0.5),\n    axis.text.x = element_text(size = 10, face = \"bold\"),\n    plot.background = element_rect(fill = \"white\", color = \"white\"),\n    panel.background = element_rect(fill = \"white\", color = \"white\")\n  ) +\n  labs(\n    title = \"MLB owners have donated the most\",\n    subtitle = \"Specifically partisan contributions from owners and commissioners in the NFL, NBA, WNBA, NHL, MLB and NASCAR, by party, 2016-20\",\n    caption = \"SOURCE: FEDERAL ELECTION COMMISSION, OPENSECRETS\"\n  )\n\n# Print the plot\nprint(p)"
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "Holly Milazzo Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at C:/Users/holly/OneDrive/Desktop/Data_Repository/Practicum II/HollyMilazzo-P2-portfolio\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.3.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          3     \n_______________________          \nColumn type frequency:           \n  factor                   1     \n  numeric                  2     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "Holly Milazzo Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\nlibrary(dplyr) #for data processing/cleaning\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\nlibrary(skimr) #for nice visualization of data \n\nWarning: package 'skimr' was built under R version 4.3.3\n\nlibrary(here) #to set paths\n\nhere() starts at C:/Users/holly/OneDrive/Desktop/Data_Repository/Practicum II/HollyMilazzo-P2-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\nhead(rawdata)\n\n# A tibble: 6 × 12\n  show_id type    title  director cast  country date_added          release_year\n  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   &lt;dttm&gt;                     &lt;dbl&gt;\n1 s1      Movie   Dick … Kirsten… &lt;NA&gt;  United… 2021-09-25 00:00:00         2020\n2 s2      TV Show Blood… &lt;NA&gt;     Ama … South … 2021-09-24 00:00:00         2021\n3 s3      TV Show Gangl… Julien … Sami… &lt;NA&gt;    2021-09-24 00:00:00         2021\n4 s4      TV Show Jailb… &lt;NA&gt;     &lt;NA&gt;  &lt;NA&gt;    2021-09-24 00:00:00         2021\n5 s5      TV Show Kota … &lt;NA&gt;     Mayu… India   2021-09-24 00:00:00         2021\n6 s6      TV Show Midni… Mike Fl… Kate… &lt;NA&gt;    2021-09-24 00:00:00         2021\n# ℹ 4 more variables: rating &lt;chr&gt;, duration &lt;chr&gt;, listed_in &lt;chr&gt;,\n#   description &lt;chr&gt;\n\n\n\n\nCheck data\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 8,811\nColumns: 12\n$ show_id      &lt;chr&gt; \"s1\", \"s2\", \"s3\", \"s4\", \"s5\", \"s6\", \"s7\", \"s8\", \"s9\", \"s1…\n$ type         &lt;chr&gt; \"Movie\", \"TV Show\", \"TV Show\", \"TV Show\", \"TV Show\", \"TV …\n$ title        &lt;chr&gt; \"Dick Johnson Is Dead\", \"Blood & Water\", \"Ganglands\", \"Ja…\n$ director     &lt;chr&gt; \"Kirsten Johnson\", NA, \"Julien Leclercq\", NA, NA, \"Mike F…\n$ cast         &lt;chr&gt; NA, \"Ama Qamata, Khosi Ngema, Gail Mabalane, Thabang Mola…\n$ country      &lt;chr&gt; \"United States\", \"South Africa\", NA, NA, \"India\", NA, NA,…\n$ date_added   &lt;dttm&gt; 2021-09-25, 2021-09-24, 2021-09-24, 2021-09-24, 2021-09-…\n$ release_year &lt;dbl&gt; 2020, 2021, 2021, 2021, 2021, 2021, 2021, 1993, 2021, 202…\n$ rating       &lt;chr&gt; \"PG-13\", \"TV-MA\", \"TV-MA\", \"TV-MA\", \"TV-MA\", \"TV-MA\", \"PG…\n$ duration     &lt;chr&gt; \"90 min\", \"2 Seasons\", \"1 Season\", \"1 Season\", \"2 Seasons…\n$ listed_in    &lt;chr&gt; \"Documentaries\", \"International TV Shows, TV Dramas, TV M…\n$ description  &lt;chr&gt; \"As her father nears the end of his life, filmmaker Kirst…\n\nsummary(rawdata)\n\n   show_id              type              title             director        \n Length:8811        Length:8811        Length:8811        Length:8811       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n     cast             country            date_added                    \n Length:8811        Length:8811        Min.   :2008-01-01 00:00:00.00  \n Class :character   Class :character   1st Qu.:2018-04-06 00:00:00.00  \n Mode  :character   Mode  :character   Median :2019-07-02 00:00:00.00  \n                                       Mean   :2019-05-17 17:50:35.32  \n                                       3rd Qu.:2020-08-19 18:00:00.00  \n                                       Max.   :2024-04-05 00:00:00.00  \n                                       NA's   :13                      \n  release_year     rating            duration          listed_in        \n Min.   :1925   Length:8811        Length:8811        Length:8811       \n 1st Qu.:2013   Class :character   Class :character   Class :character  \n Median :2017   Mode  :character   Mode  :character   Mode  :character  \n Mean   :2014                                                           \n 3rd Qu.:2019                                                           \n Max.   :2024                                                           \n NA's   :3                                                              \n description       \n Length:8811       \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nhead(rawdata)\n\n# A tibble: 6 × 12\n  show_id type    title  director cast  country date_added          release_year\n  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   &lt;dttm&gt;                     &lt;dbl&gt;\n1 s1      Movie   Dick … Kirsten… &lt;NA&gt;  United… 2021-09-25 00:00:00         2020\n2 s2      TV Show Blood… &lt;NA&gt;     Ama … South … 2021-09-24 00:00:00         2021\n3 s3      TV Show Gangl… Julien … Sami… &lt;NA&gt;    2021-09-24 00:00:00         2021\n4 s4      TV Show Jailb… &lt;NA&gt;     &lt;NA&gt;  &lt;NA&gt;    2021-09-24 00:00:00         2021\n5 s5      TV Show Kota … &lt;NA&gt;     Mayu… India   2021-09-24 00:00:00         2021\n6 s6      TV Show Midni… Mike Fl… Kate… &lt;NA&gt;    2021-09-24 00:00:00         2021\n# ℹ 4 more variables: rating &lt;chr&gt;, duration &lt;chr&gt;, listed_in &lt;chr&gt;,\n#   description &lt;chr&gt;\n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n8811\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n10\n\n\nnumeric\n1\n\n\nPOSIXct\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nshow_id\n0\n1.00\n2\n19\n0\n8811\n0\n\n\ntype\n1\n1.00\n5\n13\n0\n3\n0\n\n\ntitle\n2\n1.00\n1\n104\n0\n8806\n0\n\n\ndirector\n2636\n0.70\n2\n208\n0\n4529\n0\n\n\ncast\n826\n0.91\n3\n771\n0\n7695\n0\n\n\ncountry\n833\n0.91\n4\n123\n0\n749\n0\n\n\nrating\n6\n1.00\n1\n29\n0\n19\n0\n\n\nduration\n5\n1.00\n5\n146\n0\n221\n0\n\n\nlisted_in\n3\n1.00\n6\n79\n0\n516\n0\n\n\ndescription\n3\n1.00\n61\n248\n0\n8776\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nrelease_year\n3\n1\n2014.19\n8.79\n1925\n2013\n2017\n2019\n2024\n▁▁▁▁▇\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ndate_added\n13\n1\n2008-01-01\n2024-04-05\n2019-07-02\n1715\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\n# Handling missing values\nrawdata$director[is.na(rawdata$director)] &lt;- \"Unknown\"\n\n# Converting date formats\nrawdata$date_added &lt;- as.Date(rawdata$date_added, format = \"%m/%d/%Y\")\n\n# Standardizing categorical variables\nrawdata$type &lt;- as.factor(rawdata$type)\n\n# Convert 'type' back to factor after mutation\nrawdata$type &lt;- as.factor(rawdata$type)\n\n# Display the cleaned data\nhead(rawdata)\n\n# A tibble: 6 × 12\n  show_id type    title    director cast  country date_added release_year rating\n  &lt;chr&gt;   &lt;fct&gt;   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   &lt;date&gt;            &lt;dbl&gt; &lt;chr&gt; \n1 s1      Movie   Dick Jo… Kirsten… &lt;NA&gt;  United… 2021-09-25         2020 PG-13 \n2 s2      TV Show Blood &… Unknown  Ama … South … 2021-09-24         2021 TV-MA \n3 s3      TV Show Ganglan… Julien … Sami… &lt;NA&gt;    2021-09-24         2021 TV-MA \n4 s4      TV Show Jailbir… Unknown  &lt;NA&gt;  &lt;NA&gt;    2021-09-24         2021 TV-MA \n5 s5      TV Show Kota Fa… Unknown  Mayu… India   2021-09-24         2021 TV-MA \n6 s6      TV Show Midnigh… Mike Fl… Kate… &lt;NA&gt;    2021-09-24         2021 TV-MA \n# ℹ 3 more variables: duration &lt;chr&gt;, listed_in &lt;chr&gt;, description &lt;chr&gt;\n\n\n\n# Remove rows where 'type' is \"William Wyler\" or NA\ncleaned_data &lt;- rawdata %&gt;%\n  filter(type != \"William Wyler\" & type != \"Unknown\" & !is.na(type))\n\nhead(cleaned_data)\n\n# A tibble: 6 × 12\n  show_id type    title    director cast  country date_added release_year rating\n  &lt;chr&gt;   &lt;fct&gt;   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   &lt;date&gt;            &lt;dbl&gt; &lt;chr&gt; \n1 s1      Movie   Dick Jo… Kirsten… &lt;NA&gt;  United… 2021-09-25         2020 PG-13 \n2 s2      TV Show Blood &… Unknown  Ama … South … 2021-09-24         2021 TV-MA \n3 s3      TV Show Ganglan… Julien … Sami… &lt;NA&gt;    2021-09-24         2021 TV-MA \n4 s4      TV Show Jailbir… Unknown  &lt;NA&gt;  &lt;NA&gt;    2021-09-24         2021 TV-MA \n5 s5      TV Show Kota Fa… Unknown  Mayu… India   2021-09-24         2021 TV-MA \n6 s6      TV Show Midnigh… Mike Fl… Kate… &lt;NA&gt;    2021-09-24         2021 TV-MA \n# ℹ 3 more variables: duration &lt;chr&gt;, listed_in &lt;chr&gt;, description &lt;chr&gt;\n\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"cleaned_data.rds\")\nsaveRDS(cleaned_data, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "Holly Milazzo Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "Holly Milazzo Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "Holly Milazzo Data Analysis Portfolio",
    "section": "",
    "text": "This project uses data-set on Netflix Movies and TV shows from https://www.kaggle.com/datasets/rahulvyasm/netflix-movies-and-tv-shows.\nI first downloaded the netflix_titles.csv dataset and converted it to an xlsx format by opening a new excel file &gt; Click on Data tab &gt; click From Text/CSV &gt; Popup window will open for you to select the netflix_titles.csv file &gt; Select Import &gt; Click Load &gt; Save file\nThis dataset contains 8, 809 records and the following 12 variables:\n\nshow_id: A unique identifier for each title.\ntype: The category of the title, which is either ‘Movie’ or ‘TV Show’\ntitle: The name of the movie or TV show\ndirector: The director(s) of the movie or TV show (Contains null values for some entries, especially TV shows where this information might not be applicable)\ncast: The list of main actors/actresses in the title (Some entries might not have this information.)\ncountry: The country or countries where the movie or TV show was produced.\ndate_added: The date the title was added to Netflix.\nrelease_year: The year the movie or TV show was originally released.\nrating: The age rating of the title.\nduration: The duration of the title, in minutes for movies and seasons for TV shows\nlisted_in: The genres the title falls under.\ndescription: A brief summary of the title.\n\nYou will need to load the following packages in R: library(readxl) library(ggplot2) library(dplyr) library(tidyr) library(skimr)\nlibrary(here)\nNext, to clean the data prior to analysis you will need to…\nHandle the missing values in the ‘director’ variable: &gt;rawdata\\(director[is.na(rawdata\\)director)] &lt;- “Unknown”\nConvert the ‘date_added’ variable into an actual date format: &gt;rawdata\\(date_added &lt;- as.Date(rawdata\\)date_added, format = “%m/%d/%Y”)\nRemove rows where ‘type’ is “William Wyler” or NA and create a ‘cleaned_data’ subset: &gt;cleaned_data &lt;- rawdata %&gt;% &gt;&gt; filter(type != “William Wyler” & type != “Unknown” & !is.na(type))\nand then convert the ‘type’ variable into a factor: &gt;rawdata\\(type &lt;- as.factor(rawdata\\)type)\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  },
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "Holly Milazzo Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "Holly Milazzo Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "This uses MS Word as output format. See here for more information. You can switch #to other formats, like html or pdf. See the Quarto documentation for other formats.*/\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "1.1 General Background Information",
    "text": "1.1 General Background Information\nThe intent of this analysis is to provide insights into how regional production practices and content types align with age rating distributions, offering valuable information for Netflix’s content acquisition and compliance strategies. This research highlights the importance of understanding content rating trends to better cater to diverse audiences and ensure appropriate content delivery."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "1.2 Description of data and data source",
    "text": "1.2 Description of data and data source\nData is on Netflix Movies and TV Shows from Kaggle.com site. The description says: “The Netflix Titles dataset is a comprehensive compilation of movies and TV shows available on Netflix, covering various aspects such as the title type, director, cast, country of production, release year, rating, duration, genres (listed in), and a brief description. This dataset is instrumental for analyzing trends in Netflix content, understanding genre popularity, and examining the distribution of content across different regions and time periods”\nThe dataset contains 8,809 observations and the following 12 variables:\n\nshow_id: A unique identifier for each title.\ntype: The category of the title, which is either ‘Movie’ or ‘TV Show’\ntitle: The name of the movie or TV show\ndirector: The director(s) of the movie or TV show (Contains null values for some entries, especially TV shows where this information might not be applicable)\ncast: The list of main actors/actresses in the title (Some entries might not have this information.)\ncountry: The country or countries where the movie or TV show was produced.\ndate_added: The date the title was added to Netflix.\nrelease_year: The year the movie or TV show was originally released.\nrating: The age rating of the title.\nduration: The duration of the title, in minutes for movies and seasons for TV shows\nlisted_in: The genres the title falls under.\ndescription: A brief summary of the title."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "1.3 Questions/Hypotheses to be addressed",
    "text": "1.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\n“How do the type of content (Movie or TV Show) and the country of origin affect the distribution of age ratings on Netflix titles?”\nThis question focuses on understanding the relationship between content type, country of origin, and age ratings, which can provide valuable insights into regional production practices and content rating trends on Netflix\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the #bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. #Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-acquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-acquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.1 Data Acquisition",
    "text": "2.1 Data Acquisition\nI imported the data for Netflix Movies and TV Shows which was available on Kaggle.com site. My raw data file is available through file path folders: starter-analysis-exercise &gt; data &gt; raw-data &gt; netflix_titles.xlsx"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.2 Data Import and Cleaning",
    "text": "2.2 Data Import and Cleaning\nThe file path to my code file for cleaning my dataset is: starter-analysis-exercise &gt; code &gt; processing-code &gt; processingfile\nFirst I imported the data…\n\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\nhere are some of the initial cleaning techniques and a few reasons why I chose to do them:\n\nFor the country column, I filled missing values with the mode (most frequently occurring country).\nConverted the date_added to a Date format as a crucial step for any potential time series or date-related analysis.\nConverted type to a factor since I am planning on performing statistical tests and/or modeling that may need categorical input features.\n\n\n# Handling missing values\n\nrawdata$director[is.na(rawdata$director)] &lt;- \"Unknown\"\n\n# Fill missing 'country' values with the mode (most frequent value)\nmode_country &lt;- names(sort(table(rawdata$country), decreasing = TRUE))[1]\nrawdata$country[is.na(rawdata$country)] &lt;- mode_country\n\n# Safe conversion of date formats with error handling\nrawdata$date_added &lt;- as.Date(rawdata$date_added, format = \"%m/%d/%Y\")\nif(any(is.na(rawdata$date_added))) {\n  warning(\"There were errors in date conversion. Check date formats.\")\n}\n\nWarning: There were errors in date conversion. Check date formats.\n\n# Standardizing categorical variables\n\nrawdata$type &lt;- as.factor(rawdata$type)\n\n# Display the cleaned data\nhead(rawdata)\n\n# A tibble: 6 × 12\n  show_id type    title    director cast  country date_added release_year rating\n  &lt;chr&gt;   &lt;fct&gt;   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   &lt;date&gt;            &lt;dbl&gt; &lt;chr&gt; \n1 s1      Movie   Dick Jo… Kirsten… &lt;NA&gt;  United… 2021-09-25         2020 PG-13 \n2 s2      TV Show Blood &… Unknown  Ama … South … 2021-09-24         2021 TV-MA \n3 s3      TV Show Ganglan… Julien … Sami… United… 2021-09-24         2021 TV-MA \n4 s4      TV Show Jailbir… Unknown  &lt;NA&gt;  United… 2021-09-24         2021 TV-MA \n5 s5      TV Show Kota Fa… Unknown  Mayu… India   2021-09-24         2021 TV-MA \n6 s6      TV Show Midnigh… Mike Fl… Kate… United… 2021-09-24         2021 TV-MA \n# ℹ 3 more variables: duration &lt;chr&gt;, listed_in &lt;chr&gt;, description &lt;chr&gt;\n\n\nI also needed to do some clean up when it came to content ‘type’ as it included unwanted values…\n\n# Remove rows where 'type' is \"William Wyler\" or NA\ncleaned_data &lt;- rawdata[rawdata$type != \"William Wyler\" & rawdata$type != \"Unknown\" & !is.na(rawdata$type), ]\n\n\nsummary(cleaned_data)\n\n   show_id                     type         title             director        \n Length:8809        Movie        :6132   Length:8809        Length:8809       \n Class :character   TV Show      :2677   Class :character   Class :character  \n Mode  :character   William Wyler:   0   Mode  :character   Mode  :character  \n                                                                              \n                                                                              \n                                                                              \n                                                                              \n     cast             country            date_added          release_year \n Length:8809        Length:8809        Min.   :2008-01-01   Min.   :1925  \n Class :character   Class :character   1st Qu.:2018-04-06   1st Qu.:2013  \n Mode  :character   Mode  :character   Median :2019-07-02   Median :2017  \n                                       Mean   :2019-05-17   Mean   :2014  \n                                       3rd Qu.:2020-08-19   3rd Qu.:2019  \n                                       Max.   :2024-04-05   Max.   :2024  \n                                       NA's   :11           NA's   :1     \n    rating            duration          listed_in         description       \n Length:8809        Length:8809        Length:8809        Length:8809       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.3 Statistical Analysis",
    "text": "2.3 Statistical Analysis\nExplain anything related to your statistical analyses.\nThe relevant variables I’ll be using during my statistical analysis to determine how regional production practices and content types align with age rating distributions will be: Country, Type, and Rating.\nLet’s double check if there is any other missing data in my cleaned_data before I perform my analysis… nothing significant in 3 variables I’ll be using.\n\n# Calculate the number of missing values for each column in cleaned_data\nmissing_data_summary &lt;- sapply(cleaned_data, function(x) sum(is.na(x)))\n\n# Print the summary of missing data\nprint(missing_data_summary)\n\n     show_id         type        title     director         cast      country \n           0            0            0            0          825            0 \n  date_added release_year       rating     duration    listed_in  description \n          11            1            5            4            1            1 \n\n\nI also want to check for any outliers as well…\n\n# Create a boxplot for each numeric variable in the dataframe\nnumeric_vars &lt;- sapply(cleaned_data, is.numeric)\nif(any(numeric_vars)) {\n  # Filter only numeric columns\n  numeric_data &lt;- cleaned_data[, numeric_vars]\n\n  # Melt the data for easy plotting with ggplot2\n  library(reshape2)\n  long_data &lt;- melt(numeric_data)\n\n  # Plot\n  ggplot(long_data, aes(x = variable, y = value)) +\n    geom_boxplot(outlier.colour = \"red\", outlier.shape = 1) +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n    labs(title = \"Boxplot for Each Numeric Variable\", x = \"Variables\", y = \"Values\")\n} else {\n  print(\"No numeric variables found for plotting.\")\n}\n\nWarning: package 'reshape2' was built under R version 4.3.3\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\nGiven my hypothesis, which aims to explore how the type of content (Movie or TV Show) and the country of origin influence the distribution of age ratings on Netflix titles, regression testing does not seem like the best testing method because I using 2 categorical variables (Type and Rating) as target variables.My ‘rating’ variable is not ordinal either which means we’d also leave out performing logistic regression.\nI believe the best testing methods in this case are either Chi-square (to test independence) or Multinomial Logistic Regression. MLR would be useful since the ‘Rating’ has multiple categories without order and would allow me to model the probability of each rating category as a function of ‘Type’ and ‘Country’.\nI will need install the following packages for the next part of my analysis\n\nlibrary(nnet)\nlibrary(forcats)\n\nInitially, when I ran the model it gave an error due to the complexity in the number of parameters it created based on the variations of categories I have in my variables.\nChatGPT recommended I use the code below to create a decay term for regularization, which helps to manage the complexity of the model by shrinking the regression coefficients.\nWith the convergence of my multinomial logistic regression as indicated by “converged” in the output below, the next steps involve interpreting the model’s results and using them to validate my hypothesis or make further decisions.\n\n# Assuming 'Country' has many categories, we reduce them\ncleaned_data$country &lt;- fct_lump_n(cleaned_data$country, n = 10)  # Keeps the top 10 countries, others lumped into \"Other\"\ncleaned_data$country &lt;- factor(cleaned_data$country)\n\n# Fit the model with increased decay for regularization\n\nfit &lt;- multinom(type ~ country + rating, data = cleaned_data, MaxNWts = 10000, decay = 0.1)\n\nWarning in multinom(type ~ country + rating, data = cleaned_data, MaxNWts =\n10000, : group 'William Wyler' is empty\n\n\n# weights:  29 (28 variable)\ninitial  value 6102.467778 \niter  10 value 4551.137216\niter  20 value 4300.674269\niter  30 value 4276.943332\nfinal  value 4276.909270 \nconverged\n\n\nNow to run my multinomial model….\n\ncleaned_data$country &lt;- as.factor(cleaned_data$country)\ncleaned_data$type &lt;- as.factor(cleaned_data$type)\ncleaned_data$rating &lt;- as.factor(cleaned_data$rating)\n\n\nmultinom_model &lt;- multinom(rating ~ country + type, data = cleaned_data)\n\n# weights:  252 (221 variable)\ninitial  value 25446.832957 \niter  10 value 18728.030041\niter  20 value 16906.955079\niter  30 value 15526.132442\niter  40 value 14980.668736\niter  50 value 14724.154902\niter  60 value 14656.735665\niter  70 value 14624.490886\niter  80 value 14615.654822\niter  90 value 14612.398803\niter 100 value 14611.228013\nfinal  value 14611.228013 \nstopped after 100 iterations\n\n# Summary of the model\nhead(multinom_model)\n\n$n\n[1] 13  0 18\n\n$nunits\n[1] 32\n\n$nconn\n [1]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  14  28  42  56\n[20]  70  84  98 112 126 140 154 168 182 196 210 224 238 252\n\n$conn\n  [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13  0  1  2  3  4  5  6  7  8  9 10\n [26] 11 12 13  0  1  2  3  4  5  6  7  8  9 10 11 12 13  0  1  2  3  4  5  6  7\n [51]  8  9 10 11 12 13  0  1  2  3  4  5  6  7  8  9 10 11 12 13  0  1  2  3  4\n [76]  5  6  7  8  9 10 11 12 13  0  1  2  3  4  5  6  7  8  9 10 11 12 13  0  1\n[101]  2  3  4  5  6  7  8  9 10 11 12 13  0  1  2  3  4  5  6  7  8  9 10 11 12\n[126] 13  0  1  2  3  4  5  6  7  8  9 10 11 12 13  0  1  2  3  4  5  6  7  8  9\n[151] 10 11 12 13  0  1  2  3  4  5  6  7  8  9 10 11 12 13  0  1  2  3  4  5  6\n[176]  7  8  9 10 11 12 13  0  1  2  3  4  5  6  7  8  9 10 11 12 13  0  1  2  3\n[201]  4  5  6  7  8  9 10 11 12 13  0  1  2  3  4  5  6  7  8  9 10 11 12 13  0\n[226]  1  2  3  4  5  6  7  8  9 10 11 12 13  0  1  2  3  4  5  6  7  8  9 10 11\n[251] 12 13\n\n$nsunits\n[1] 14\n\n$decay\n[1] 0\n\n\n\nprobabilities &lt;- predict(multinom_model, type = \"probs\")\n\nhead(probabilities)\n\n        66 min       74 min       84 min            A            G        NC-17\n1 2.197696e-04 3.977743e-04 3.977743e-04 3.977743e-04 1.149018e-02 3.866288e-04\n2 1.046572e-07 2.200441e-08 2.200441e-08 2.200441e-08 1.654515e-06 1.734750e-06\n3 1.258220e-05 5.534332e-06 5.534332e-06 5.534332e-06 3.762838e-06 1.485806e-06\n4 1.258220e-05 5.534332e-06 5.534332e-06 5.534332e-06 3.762838e-06 1.485806e-06\n5 4.468941e-08 6.704574e-10 6.704574e-10 6.704574e-10 2.364318e-10 2.754968e-10\n6 1.258220e-05 5.534332e-06 5.534332e-06 5.534332e-06 3.762838e-06 1.485806e-06\n            NR           PG        PG-13            R     TV-14       TV-G\n1 0.0131985545 6.803521e-02 1.189387e-01 1.802553e-01 0.1459130 0.02467179\n2 0.0016490449 1.866344e-06 4.479769e-06 8.648112e-04 0.2888059 0.02839096\n3 0.0018656069 2.736761e-06 6.625572e-06 1.151475e-03 0.2363820 0.04348793\n4 0.0018656069 2.736761e-06 6.625572e-06 1.151475e-03 0.2363820 0.04348793\n5 0.0004001956 5.796439e-08 1.573624e-07 4.749477e-06 0.5875946 0.01035897\n6 0.0018656069 2.736761e-06 6.625572e-06 1.151475e-03 0.2363820 0.04348793\n      TV-MA     TV-PG        TV-Y      TV-Y7     TV-Y7-FV           UR\n1 0.2964644 0.0793353 0.027272531 0.03160530 0.0006127349 4.072383e-04\n2 0.4582647 0.1051285 0.059006494 0.05739596 0.0004821809 1.587896e-06\n3 0.4033705 0.1232277 0.085883178 0.10420632 0.0003801792 1.367047e-06\n4 0.4033705 0.1232277 0.085883178 0.10420632 0.0003801792 1.367047e-06\n5 0.2257690 0.1376917 0.009731089 0.02802596 0.0004235554 1.988237e-10\n6 0.4033705 0.1232277 0.085883178 0.10420632 0.0003801792 1.367047e-06"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 Exploratory/Descriptive Analysis",
    "text": "3.1 Exploratory/Descriptive Analysis\nDistribution between Movies and TV Shows:\n\n# Perform the group_by and summarise operations\ntype_distribution &lt;- aggregate(. ~ type, data = cleaned_data, FUN = length)\nnames(type_distribution)[2] &lt;- \"count\"\n\n# Plot the distribution of content types\nbarplot(height = type_distribution$count,\n        names.arg = type_distribution$type,\n        col = c(\"purple\", \"orange\"),\n        main = \"Distribution of Content Types - Movies v TV Shows\",\n        xlab = \"Type\",\n        ylab = \"Count\",\n        las = 1) # las = 1 makes axis labels horizontal\n\n\n\n\nWe see from the distribution that it appears movies are being streamed substantially more than TV shows, but to get a better sense of this let’s represent it as a percentage instead\n\ntype_distribution &lt;- aggregate(. ~ type, data = cleaned_data, FUN = length)\nnames(type_distribution)[2] &lt;- \"count\"\n\ntype_distribution$percentage &lt;- round((type_distribution$count / sum(type_distribution$count)) * 100, 1)\n\nlabels &lt;- paste(type_distribution$type, type_distribution$percentage, \"%\")\n\npie(type_distribution$count,\n    labels = labels,\n    col = c(\"purple\", \"orange\"),\n    main = \"Percentage of Movies vs TV Shows\")\n\n\n\n\nNow I’d like to see which countries the Movie/TV show content originate from\n\nlibrary(ggplot2)\n\ncountry_counts &lt;- head(sort(table(cleaned_data$country), decreasing = TRUE), 10)\n\n# Convert to data frame for ggplot\ncountry_df &lt;- data.frame(\n  country = names(country_counts),\n  count = as.numeric(country_counts)\n)\n\n# Create the bar chart\np &lt;- ggplot(country_df, aes(x = reorder(country, -count), y = count)) +\n  geom_bar(stat = \"identity\", fill = \"purple\") +\n  geom_text(aes(label = count), vjust = -0.3) +\n  labs(x = \"Country\", y = \"Count\", title = \"Top 10 Countries (Top 3 Highlighted)\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  geom_bar(data = country_df[1:3, ], aes(x = country, y = count), stat = \"identity\", fill = \"orange\")\n\nprint(p)\n\n\n\n\nLet’s also explore what the content consumption is like between countries…\n\n# Count occurrences of each country and type\ncount_data &lt;- count(cleaned_data, country, type)\n\n# Group by country\ngrouped_data &lt;- group_by(count_data, country)\n\n# Calculate percentage within each group\ngrouped_data &lt;- mutate(grouped_data, percentage = n / sum(n) * 100)\n\n# Ungroup the data\npercentage_data &lt;- ungroup(grouped_data)\n\n# Show percentage_data\npercentage_data\n\n# A tibble: 22 × 4\n   country type        n percentage\n   &lt;fct&gt;   &lt;fct&gt;   &lt;int&gt;      &lt;dbl&gt;\n 1 Canada  Movie     122      67.4 \n 2 Canada  TV Show    59      32.6 \n 3 Egypt   Movie      92      86.8 \n 4 Egypt   TV Show    14      13.2 \n 5 France  Movie      75      60.5 \n 6 France  TV Show    49      39.5 \n 7 India   Movie     893      91.9 \n 8 India   TV Show    79       8.13\n 9 Japan   Movie      76      31.0 \n10 Japan   TV Show   169      69.0 \n# ℹ 12 more rows\n\n\n\n# Calculate total count of each country\ncountry_totals &lt;- aggregate(percentage_data$n, by = list(percentage_data$country), FUN = sum)\n\n# Select top 10 countries by total count\ntop_10_countries &lt;- country_totals[order(country_totals$x, decreasing = TRUE), ]$Group.1[1:10]\n\n# Subset data for top 10 countries only\ntop_10_data &lt;- subset(percentage_data, country %in% top_10_countries)\n\n# Create pie chart for top 10 countries with improved readability\nlibrary(ggplot2)\n\n# Plotting the pie chart\npie_plot &lt;- ggplot(top_10_data, aes(x = \"\", y = n, fill = type)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"white\") +\n  coord_polar(theta = \"y\") +\n  facet_wrap(~ country, scales = \"free_y\") +\n  geom_text(aes(label = paste0(round(percentage, 2), \"%\")),\n            position = position_stack(vjust = 0.5), color = \"white\", size = 4, family = \"sans\") +  # Adjust text size, color, and font family\n  theme_void() +\n  scale_fill_manual(values = c(\"purple\", \"orange\"), labels = c(\"Movie\", \"TV Show\")) +  # Adjust colors and labels\n  theme(legend.position = \"bottom\", legend.text = element_text(size = 10, family = \"sans\"), plot.title = element_text(hjust = 0.5, size = 14, family = \"sans\")) +  # Adjust legend and title text\n  labs(fill = \"Type\", title = \"Percentage of Movies vs TV Shows in Top 10 Countries\")  # Adjust title\n\n# Show the plot\nprint(pie_plot)\n\n\n\n\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Basic Statistical Analysis",
    "text": "3.2 Basic Statistical Analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nGiven my hypothesis…\nMy predictor Variables (Independent Variables):\n\ntype (Movie or TV Show)\ncountry (Country of origin)\n\nand my response Variable (Dependent Variable):\n\nrating (Age rating assigned to Netflix titles)\n\nLet’s perform some basic statistics to examine the associations between the outcome variable (rating) and each individual predictor variable (type and country).\n\n# Cross-tabulation of type and rating\ntype_rating_table &lt;- table(rawdata$type, rawdata$rating)\nprint(type_rating_table)\n\n               \n                66 min 74 min 84 min    A Classic Movies, Documentaries    G\n  Movie              1      1      1    1                             0   41\n  TV Show            0      0      0    0                             0    0\n  William Wyler      0      0      0    0                             1    0\n               \n                NC-17   NR   PG PG-13    R TV-14 TV-G TV-MA TV-PG TV-Y TV-Y7\n  Movie             3   75  287   490  797  1427  126  2062   539  131   139\n  TV Show           0    5    0     0    2   733   94  1146   323  176   195\n  William Wyler     0    0    0     0    0     0    0     0     0    0     0\n               \n                TV-Y7-FV   UR\n  Movie                5    3\n  TV Show              1    0\n  William Wyler        0    0\n\n\n\n# Chi-square test of independence\ntype_rating_chi2 &lt;- chisq.test(type_rating_table)\n\nWarning in chisq.test(type_rating_table): Chi-squared approximation may be\nincorrect\n\nprint(type_rating_chi2)\n\n\n    Pearson's Chi-squared test\n\ndata:  type_rating_table\nX-squared = 9853.7, df = 36, p-value &lt; 2.2e-16\n\n\nInterpretation of results:\nThe chi-square test statistic is very large (9853.7), and the p-value is extremely small (less than 2.2e-16). This indicates that there is a significant association between type and rating. In other words, the type of content (Movie or TV Show) and the rating are not independent; they are related\nwhat about the association between country and rating though…\n\n# Frequency distribution of rating by country\ncountry_rating_table &lt;- table(rawdata$country, rawdata$rating)\n\n\ncountry_rating_chi2 &lt;- chisq.test(country_rating_table)\n\nWarning in chisq.test(country_rating_table): Chi-squared approximation may be\nincorrect\n\nprint(country_rating_chi2)\n\n\n    Pearson's Chi-squared test\n\ndata:  country_rating_table\nX-squared = 26495, df = 13464, p-value &lt; 2.2e-16\n\n\nInterpretation of results:\nIt appears, again, that the chi-square test statistic is very large (26495), and the p-value is extremely small (less than 2.2e-16). Which indicates that there is a significant association between country and rating. In other words, the country of origin and the rating are not independent; they are related.\nGiven the high association between all my variables (type, country, and rating), I will do some a bit more exploring.\nFirst, I could visually explore the association between my categorical variables using a stacked bar chart, and then run a random forest model to understand the importance of using different predictors and their relationships\n\nrawdata$country &lt;- sapply(strsplit(rawdata$country, \", \"), `[`, 1)\n\n\nunique(rawdata$country)\n\n [1] \"United States\"        \"South Africa\"         \"India\"               \n [4] \"United Kingdom\"       \"Germany\"              \"Mexico\"              \n [7] \"Turkey\"               \"Australia\"            \"Finland\"             \n[10] \"China\"                \"Nigeria\"              \"Japan\"               \n[13] \"Spain\"                \"France\"               \"Belgium\"             \n[16] \"South Korea\"          \"Argentina\"            \"Russia\"              \n[19] \"Canada\"               \"Hong Kong\"            \"Italy\"               \n[22] \"\"                     \"Ireland\"              \"New Zealand\"         \n[25] \"Jordan\"               \"Colombia\"             \"Switzerland\"         \n[28] \"Israel\"               \"Brazil\"               \"Taiwan\"              \n[31] \"Bulgaria\"             \"Poland\"               \"Saudi Arabia\"        \n[34] \"Thailand\"             \"Indonesia\"            \"Egypt\"               \n[37] \"Kuwait\"               \"Malaysia\"             \"Vietnam\"             \n[40] \"Sweden\"               \"Lebanon\"              \"Romania\"             \n[43] \"Philippines\"          \"Iceland\"              \"Denmark\"             \n[46] \"United Arab Emirates\" \"Netherlands\"          \"Norway\"              \n[49] \"Syria\"                \"Mauritius\"            \"Austria\"             \n[52] \"Czech Republic\"       \"Cameroon\"             \"Uruguay\"             \n[55] \"United Kingdom,\"      \"Kenya\"                \"Chile\"               \n[58] \"Luxembourg\"           \"Bangladesh\"           \"Portugal\"            \n[61] \"Hungary\"              \"Senegal\"              \"Singapore\"           \n[64] \"Serbia\"               \"Namibia\"              \"Peru\"                \n[67] \"Mozambique\"           \"Belarus\"              \"Ghana\"               \n[70] \"Zimbabwe\"             \"Puerto Rico\"          \"Pakistan\"            \n[73] \"Cyprus\"               \"Paraguay\"             \"Croatia\"             \n[76] \"United States,\"       \"Cambodia\"             \"Georgia\"             \n[79] \"Soviet Union\"         \"Greece\"               \"West Germany\"        \n[82] \"Iran\"                 \"Venezuela\"            \"Poland,\"             \n[85] \"Slovenia\"             \"Guatemala\"            \"Ukraine\"             \n[88] \"Jamaica\"              \"1944\"                 \"Somalia\"             \n\n\n\nfiltered_data &lt;- rawdata %&gt;%\n  filter(!is.na(type))\n\n# Optionally, filter the data to include only a subset of countries\n# For example, top 10 countries by count of titles\ntop_countries &lt;- filtered_data %&gt;%\n  count(country, sort = TRUE) %&gt;%\n  top_n(10, n) %&gt;%\n  pull(country)\n\nfiltered_data &lt;- filtered_data %&gt;%\n  filter(country %in% top_countries)\n\n# Create the bar chart\nggplot(filtered_data, aes(x = country, fill = rating)) +\n  geom_bar(position = \"fill\") +\n  facet_wrap(~ type, scales = \"free_y\") +\n  labs(title = \"Proportion of Ratings by Type and Country\", x = \"Country\", y = \"Proportion\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"bottom\")\n\n\n\n\nBefore getting started with our random forest we’ll need to install some necessary packages\n\nlibrary(caret)\n\nWarning: package 'caret' was built under R version 4.3.3\n\n\nLoading required package: lattice\n\nlibrary(randomForest)\n\nWarning: package 'randomForest' was built under R version 4.3.3\n\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nlibrary(mice)\n\nWarning: package 'mice' was built under R version 4.3.3\n\n\n\nAttaching package: 'mice'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following objects are masked from 'package:base':\n\n    cbind, rbind\n\n\nIn this step we convert our categorical variables into factors:\n\nfiltered_data$type &lt;- as.factor(filtered_data$type)\nfiltered_data$country &lt;- as.factor(filtered_data$country)\nfiltered_data$rating &lt;- as.factor(filtered_data$rating)\n\nIn this step we somewhat use imputation (replace ‘na’ in our data with mode):\n\ncalculate_mode &lt;- function(x) {\n  uniq_x &lt;- unique(x)\n  uniq_x[which.max(tabulate(match(x, uniq_x)))]\n}\n\n# Function to impute missing values\nimpute_missing &lt;- function(df) {\n  df[] &lt;- lapply(df, function(col) {\n    if (is.numeric(col)) {\n      # Impute numeric columns with median\n      col[is.na(col)] &lt;- median(col, na.rm = TRUE)\n    } else {\n      # Impute categorical columns with mode\n      col[is.na(col)] &lt;- calculate_mode(col)\n    }\n    return(col)\n  })\n  return(df)\n}\n\nNow to split the data into training and test sets…\n\nimputed_data &lt;- impute_missing(filtered_data)\n\n\nset.seed(123)\ntrain_index &lt;- createDataPartition(imputed_data$rating, p = 0.8, list = FALSE)\n\nWarning in createDataPartition(imputed_data$rating, p = 0.8, list = FALSE):\nSome classes have a single record ( 66 min, 74 min, 84 min, A ) and these will\nbe selected for the sample\n\ntrain_data &lt;- imputed_data[train_index, ]\ntest_data &lt;- imputed_data[-train_index, ]\n\nNow to train the random forest model…\n\nset.seed(321) \nrf_model &lt;- randomForest(rating ~ type + country, data = train_data, ntree = 500, mtry = 2, importance = TRUE)\n\n\nprint(rf_model)\n\n\nCall:\n randomForest(formula = rating ~ type + country, data = train_data,      ntree = 500, mtry = 2, importance = TRUE) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 60.89%\nConfusion matrix:\n         66 min 74 min 84 min A G NC-17 NR PG PG-13 R TV-14 TV-G TV-MA TV-PG\n66 min        0      0      0 0 0     0  0  0     0 0     0    0     1     0\n74 min        0      0      0 0 0     0  0  0     0 0     0    0     1     0\n84 min        0      0      0 0 0     0  0  0     0 0     0    0     1     0\nA             0      0      0 0 0     0  0  0     0 0     0    0     1     0\nG             0      0      0 0 0     0  0  0     0 0     0    0    32     0\nNC-17         0      0      0 0 0     0  0  0     0 0     0    0     3     0\nNR            0      0      0 0 0     0  0  0     0 2     3    0    47     0\nPG            0      0      0 0 0     0  0  0     0 6     4    0   207     0\nPG-13         0      0      0 0 0     0  0  0     0 3     5    0   354     0\nR             0      0      0 0 0     0  0  0     0 6     2    0   566     0\nTV-14         0      0      0 0 0     0  0  0     0 6   555    0   729     0\nTV-G          0      0      0 0 0     0  0  0     0 1     6    0   137     0\nTV-MA         0      0      0 0 0     0  0  0     0 8   266    0  1651     0\nTV-PG         0      0      0 0 0     0  0  0     0 5   131    0   432     0\nTV-Y          0      0      0 0 0     0  0  0     0 4     6    0   216     0\nTV-Y7         0      0      0 0 0     0  0  0     0 1    29    0   222     0\nTV-Y7-FV      0      0      0 0 0     0  0  0     0 0     1    0     3     0\nUR            0      0      0 0 0     0  0  0     0 0     0    0     3     0\n         TV-Y TV-Y7 TV-Y7-FV UR class.error\n66 min      0     0        0  0   1.0000000\n74 min      0     0        0  0   1.0000000\n84 min      0     0        0  0   1.0000000\nA           0     0        0  0   1.0000000\nG           0     0        0  0   1.0000000\nNC-17       0     0        0  0   1.0000000\nNR          0     0        0  0   1.0000000\nPG          0     0        0  0   1.0000000\nPG-13       0     0        0  0   1.0000000\nR           0     0        0  0   0.9895470\nTV-14       0     0        0  0   0.5697674\nTV-G        0     0        0  0   1.0000000\nTV-MA       0     0        0  0   0.1423377\nTV-PG       0     0        0  0   1.0000000\nTV-Y        0     0        0  0   1.0000000\nTV-Y7       0     0        0  0   1.0000000\nTV-Y7-FV    0     0        0  0   1.0000000\nUR          0     0        0  0   1.0000000\n\n\nAnd lastly, let’s evaluate the performance of our random forest:\n\npredictions &lt;- predict(rf_model, test_data)\nconfusion_matrix &lt;- confusionMatrix(predictions, test_data$rating)\n\nprint(confusion_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction 66 min 74 min 84 min   A   G NC-17  NR  PG PG-13   R TV-14 TV-G\n  66 min        0      0      0   0   0     0   0   0     0   0     0    0\n  74 min        0      0      0   0   0     0   0   0     0   0     0    0\n  84 min        0      0      0   0   0     0   0   0     0   0     0    0\n  A             0      0      0   0   0     0   0   0     0   0     0    0\n  G             0      0      0   0   0     0   0   0     0   0     0    0\n  NC-17         0      0      0   0   0     0   0   0     0   0     0    0\n  NR            0      0      0   0   0     0   0   0     0   0     0    0\n  PG            0      0      0   0   0     0   0   0     0   0     0    0\n  PG-13         0      0      0   0   0     0   0   0     0   0     0    0\n  R             0      0      0   0   0     0   0   1     0   0     1    2\n  TV-14         0      0      0   0   0     0   1   0     0   0   137    2\n  TV-G          0      0      0   0   0     0   0   0     0   0     0    0\n  TV-MA         0      0      0   0   7     0  11  53    90 143   184   31\n  TV-PG         0      0      0   0   0     0   0   0     0   0     0    0\n  TV-Y          0      0      0   0   0     0   0   0     0   0     0    0\n  TV-Y7         0      0      0   0   0     0   0   0     0   0     0    0\n  TV-Y7-FV      0      0      0   0   0     0   0   0     0   0     0    0\n  UR            0      0      0   0   0     0   0   0     0   0     0    0\n          Reference\nPrediction TV-MA TV-PG TV-Y TV-Y7 TV-Y7-FV  UR\n  66 min       0     0    0     0        0   0\n  74 min       0     0    0     0        0   0\n  84 min       0     0    0     0        0   0\n  A            0     0    0     0        0   0\n  G            0     0    0     0        0   0\n  NC-17        0     0    0     0        0   0\n  NR           0     0    0     0        0   0\n  PG           0     0    0     0        0   0\n  PG-13        0     0    0     0        0   0\n  R            9     0    1     1        0   0\n  TV-14       80    32    2     4        0   0\n  TV-G         0     0    0     0        0   0\n  TV-MA      392   110   53    58        1   0\n  TV-PG        0     0    0     0        0   0\n  TV-Y         0     0    0     0        0   0\n  TV-Y7        0     0    0     0        0   0\n  TV-Y7-FV     0     0    0     0        0   0\n  UR           0     0    0     0        0   0\n\nOverall Statistics\n                                          \n               Accuracy : 0.3762          \n                 95% CI : (0.3508, 0.4022)\n    No Information Rate : 0.3421          \n    P-Value [Acc &gt; NIR] : 0.003985        \n                                          \n                  Kappa : 0.0843          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: 66 min Class: 74 min Class: 84 min Class: A\nSensitivity                     NA            NA            NA       NA\nSpecificity                      1             1             1        1\nPos Pred Value                  NA            NA            NA       NA\nNeg Pred Value                  NA            NA            NA       NA\nPrevalence                       0             0             0        0\nDetection Rate                   0             0             0        0\nDetection Prevalence             0             0             0        0\nBalanced Accuracy               NA            NA            NA       NA\n                     Class: G Class: NC-17 Class: NR Class: PG Class: PG-13\nSensitivity          0.000000           NA  0.000000   0.00000      0.00000\nSpecificity          1.000000            1  1.000000   1.00000      1.00000\nPos Pred Value            NaN           NA       NaN       NaN          NaN\nNeg Pred Value       0.995021           NA  0.991465   0.96159      0.93599\nPrevalence           0.004979            0  0.008535   0.03841      0.06401\nDetection Rate       0.000000            0  0.000000   0.00000      0.00000\nDetection Prevalence 0.000000            0  0.000000   0.00000      0.00000\nBalanced Accuracy    0.500000           NA  0.500000   0.50000      0.50000\n                     Class: R Class: TV-14 Class: TV-G Class: TV-MA\nSensitivity           0.00000      0.42547     0.00000       0.8150\nSpecificity           0.98812      0.88838     1.00000       0.1989\nPos Pred Value        0.00000      0.53101         NaN       0.3460\nNeg Pred Value        0.89720      0.83885     0.97511       0.6740\nPrevalence            0.10171      0.22902     0.02489       0.3421\nDetection Rate        0.00000      0.09744     0.00000       0.2788\nDetection Prevalence  0.01067      0.18350     0.00000       0.8058\nBalanced Accuracy     0.49406      0.65692     0.50000       0.5069\n                     Class: TV-PG Class: TV-Y Class: TV-Y7 Class: TV-Y7-FV\nSensitivity                 0.000     0.00000      0.00000       0.0000000\nSpecificity                 1.000     1.00000      1.00000       1.0000000\nPos Pred Value                NaN         NaN          NaN             NaN\nNeg Pred Value              0.899     0.96017      0.95519       0.9992888\nPrevalence                  0.101     0.03983      0.04481       0.0007112\nDetection Rate              0.000     0.00000      0.00000       0.0000000\nDetection Prevalence        0.000     0.00000      0.00000       0.0000000\nBalanced Accuracy           0.500     0.50000      0.50000       0.5000000\n                     Class: UR\nSensitivity                 NA\nSpecificity                  1\nPos Pred Value              NA\nNeg Pred Value              NA\nPrevalence                   0\nDetection Rate               0\nDetection Prevalence         0\nBalanced Accuracy           NA\n\n\n\nvarImpPlot(rf_model)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Interpretation",
    "text": "3.3 Interpretation\nThe results from the random forest model’s performance are summarized in the confusion matrix and accompanying statistics. The overall accuracy of the model is 37.62%, indicating that it correctly classified approximately 38% of the instances. This accuracy is slightly better than the No Information Rate (NIR) of 34.21%, which represents the accuracy we would get by always predicting the most frequent class. The confidence interval for the model’s accuracy ranges from 35.08% to 40.22%, suggesting the true accuracy lies within this interval. The p-value (0.003985) indicates that the model’s accuracy is significantly better than random guessing.\nHowever, the Kappa value of 0.0843 reveals poor agreement between the predicted and actual classifications when accounting for chance. The statistics by class show varying performance across different rating categories. For instance, the sensitivity for “TV-14” is 42.55%, meaning the model correctly identified about 43% of “TV-14” ratings. The specificity for “TV-14” is higher at 88.84%, indicating that the model correctly identified about 89% of non-“TV-14” ratings.\nThe precision for “TV-14” is 53.10%, meaning that just over half of the predicted “TV-14” ratings were correct. In contrast, the model struggles with classes that have low prevalence in the dataset, resulting in low sensitivity and precision for those classes.\nOverall, the model performs moderately well, particularly for certain classes like “TV-MA” which have higher sensitivity and balanced accuracy. However, the model’s performance could be improved by addressing class imbalance, adding more features, tuning hyperparameters, and potentially using more complex ensemble methods. These steps could help achieve better classification accuracy and agreement between predicted and actual ratings."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusion",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusion",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.4 Conclusion",
    "text": "3.4 Conclusion\nThe results indicate that the country of origin significantly influences the distribution of age ratings assigned to Netflix titles, as shown by the high importance of the country variable in the random forest model. The substantial decrease in model accuracy and increase in node impurity when the country variable is excluded further supports this finding. While the type of content (Movie or TV Show) also affects the distribution of age ratings, its impact is less pronounced compared to the country of origin. Excluding the type variable results in a smaller decrease in model performance. Overall, both predictor variables—type and country—contribute to the prediction of age ratings, but the country of origin has a more dominant influence, highlighting that while both factors are important, the country’s impact is stronger.\n\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nI did get some of my EDA inspiration from : Link\nHere’s a video tutorial on how to say “Netflix and Chill” in 26 different languages:Link"
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "Holly Milazzo Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "Holly Milazzo Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "Holly Milazzo Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "Placeholder file for the future Tidy Tuesday exercise.\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n\nif (!requireNamespace(\"tidytuesdayR\", quietly = TRUE)) {\n    install.packages(\"tidytuesdayR\")\n}\nlibrary(tidytuesdayR)\n\nWarning: package 'tidytuesdayR' was built under R version 4.3.3\n\n\n\ninstall.packages(\"tidytuesdayR\")\n\nWarning: package 'tidytuesdayR' is in use and will not be installed\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-07-23')\n\n--- Compiling #TidyTuesday Information for 2024-07-23 ----\n\n\n--- There are 6 files available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 6: `auditions.csv`\n    Downloading file 2 of 6: `eliminations.csv`\n    Downloading file 3 of 6: `finalists.csv`\n    Downloading file 4 of 6: `ratings.csv`\n    Downloading file 5 of 6: `seasons.csv`\n    Downloading file 6 of 6: `songs.csv`\n\n\n--- Download complete ---\n\n\nPART 1: Load, wrangle and explore the data.\n\nhead(tuesdata)\n\n$auditions\n# A tibble: 142 × 12\n   season audition_date_start audition_date_end audition_city     audition_venue\n    &lt;dbl&gt; &lt;date&gt;              &lt;date&gt;            &lt;chr&gt;             &lt;chr&gt;         \n 1      1 2002-04-20          2002-04-22        Los Angeles, Cal… Westin Bonave…\n 2      1 2002-04-23          2002-04-25        Seattle, Washing… Hyatt Regency…\n 3      1 2002-04-26          2002-04-28        Chicago, Illinois Congress Plaz…\n 4      1 2002-04-29          2002-05-01        New York City, N… Millenium Hil…\n 5      1 2002-05-03          2002-05-05        Atlanta, Georgia  AmericasMart/…\n 6      1 2002-05-05          2002-05-07        Dallas, Texas     Wyndham Anato…\n 7      1 2002-05-11          2002-05-11        Miami, Florida    Fontainebleau…\n 8      2 2002-10-21          2002-10-21        Detroit, Michigan Atheneum Suit…\n 9      2 2002-10-24          2002-10-28        New York, New Yo… Regent Wall S…\n10      2 2002-10-27          2002-10-27        Atlanta, Georgia  AmericasMart  \n# ℹ 132 more rows\n# ℹ 7 more variables: episodes &lt;chr&gt;, episode_air_date &lt;chr&gt;,\n#   callback_venue &lt;chr&gt;, callback_date_start &lt;date&gt;, callback_date_end &lt;date&gt;,\n#   tickets_to_hollywood &lt;dbl&gt;, guest_judge &lt;chr&gt;\n\n$eliminations\n# A tibble: 456 × 46\n   season place gender contestant       top_36 top_36_2 top_36_3 top_36_4 top_32\n    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;            &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; \n 1      1 1     Female Kelly Clarkson   &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n 2      1 2     Male   Justin Guarini   &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n 3      1 3     Female Nikki McKibbin   &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n 4      1 4     Female Tamyra Gray      &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n 5      1 5     Male   R. J. Helton     &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n 6      1 6     Female Christina Chris… &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n 7      1 7     Female Ryan Starr       &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n 8      1 8     Male   AJ Gil           &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n 9      1 9–10  Male   EJay Day         &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n10      1 9–10  Male   Jim Verraros     &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n# ℹ 446 more rows\n# ℹ 37 more variables: top_32_2 &lt;chr&gt;, top_32_3 &lt;chr&gt;, top_32_4 &lt;chr&gt;,\n#   top_30 &lt;chr&gt;, top_30_2 &lt;chr&gt;, top_30_3 &lt;chr&gt;, top_25 &lt;chr&gt;, top_25_2 &lt;chr&gt;,\n#   top_25_3 &lt;chr&gt;, top_24 &lt;chr&gt;, top_24_2 &lt;chr&gt;, top_24_3 &lt;chr&gt;, top_20 &lt;chr&gt;,\n#   top_20_2 &lt;chr&gt;, top_16 &lt;chr&gt;, top_14 &lt;chr&gt;, top_13 &lt;chr&gt;, top_12 &lt;chr&gt;,\n#   top_11 &lt;chr&gt;, top_11_2 &lt;chr&gt;, wildcard &lt;chr&gt;, comeback &lt;lgl&gt;, top_10 &lt;chr&gt;,\n#   top_9 &lt;chr&gt;, top_9_2 &lt;chr&gt;, top_8 &lt;chr&gt;, top_8_2 &lt;chr&gt;, top_7 &lt;chr&gt;, …\n\n$finalists\n# A tibble: 190 × 6\n   Contestant          Birthday  Birthplace          Hometown Description Season\n   &lt;chr&gt;               &lt;chr&gt;     &lt;chr&gt;               &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt;\n 1 Kelly Clarkson      24-Apr-82 Fort Worth, Texas   Burleso… \"She perfo…      1\n 2 Justin Guarini      28-Oct-78 Columbus, Georgia   Doylest… \"He perfor…      1\n 3 Nikki McKibbin      28-Sep-78 Grand Prairie, Tex… &lt;NA&gt;     \"She had p…      1\n 4 Tamyra Gray         26-Jul-79 Takoma Park, Maryl… Atlanta… \"She had a…      1\n 5 R. J. Helton        17-May-81 Pasadena, Texas     Cumming… \"J. Helton…      1\n 6 Christina Christian 21-Jun-81 Brooklyn, New York  &lt;NA&gt;     \".Christin…      1\n 7 Ryan Starr          21-Nov-82 Sunland, California &lt;NA&gt;     \"Her audit…      1\n 8 AJ Gil              5-Jul-84  San Diego, Califor… Tacoma,…  &lt;NA&gt;            1\n 9 Jim Verraros        8-Feb-83  Chicago, Illinois   &lt;NA&gt;     \"He grew u…      1\n10 EJay Day            13-Sep-81 Lawrenceville, Geo… &lt;NA&gt;     \"He auditi…      1\n# ℹ 180 more rows\n\n$ratings\n# A tibble: 593 × 17\n   season show_number episode   airdate `18_49_rating_share` viewers_in_millions\n    &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;                              &lt;dbl&gt;\n 1      1           1 Auditions June 1… 4.8                                 9.85\n 2      1           2 Hollywoo… June 1… 5.2                                11.2 \n 3      1           3 Top 30: … June 1… 5.2                                10.3 \n 4      1           4 Top 30: … June 1… 4.7                                 9.47\n 5      1           5 Top 30: … June 2… 4.5                                 9.08\n 6      1           6 Top 30: … June 2… 4.2                                 8.53\n 7      1           7 Top 30: … July 2… 5.3                                10.3 \n 8      1           8 Top 30: … July 3… N/A                                 7.5 \n 9      1           9 Wildcard… July 1… 4.1                                 8.97\n10      1          10 Top 10 P… July 1… 5.3                                10.3 \n# ℹ 583 more rows\n# ℹ 11 more variables: timeslot_et &lt;chr&gt;, dvr_18_49 &lt;chr&gt;,\n#   dvr_viewers_millions &lt;chr&gt;, total_18_49 &lt;chr&gt;,\n#   total_viewers_millions &lt;chr&gt;, weekrank &lt;chr&gt;, ref &lt;lgl&gt;, share &lt;chr&gt;,\n#   nightlyrank &lt;dbl&gt;, rating_share_households &lt;chr&gt;, rating_share &lt;chr&gt;\n\n$seasons\n# A tibble: 18 × 10\n   season winner    runner_up original_release original_network hosted_by judges\n    &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;            &lt;chr&gt;     &lt;chr&gt; \n 1      1 Kelly Cl… Justin G… June 11 (2002-0… Fox              Ryan Sea… Paula…\n 2      2 Ruben St… Clay Aik… January 21 (200… Fox              Ryan Sea… Paula…\n 3      3 Fantasia… Diana De… January 19 (200… Fox              Ryan Sea… Paula…\n 4      4 Carrie U… Bo Bice   January 18 (200… Fox              Ryan Sea… Paula…\n 5      5 Taylor H… Katharin… January 17 (200… Fox              Ryan Sea… Paula…\n 6      6 Jordin S… Blake Le… January 16 (200… Fox              Ryan Sea… Paula…\n 7      7 David Co… David Ar… January 15 (200… Fox              Ryan Sea… Paula…\n 8      8 Kris All… Adam Lam… January 13 (200… Fox              Ryan Sea… Paula…\n 9      9 Lee DeWy… Crystal … January 12 (201… Fox              Ryan Sea… Simon…\n10     10 Scotty M… Lauren A… January 19 (201… Fox              Ryan Sea… Randy…\n11     11 Phillip … Jessica … January 18 (201… Fox              Ryan Sea… Randy…\n12     12 Candice … Kree Har… January 16 (201… Fox              Ryan Sea… Randy…\n13     13 Caleb Jo… Jena Ire… January 15 (201… Fox              Ryan Sea… Harry…\n14     14 Nick Fra… Clark Be… January 7 (2015… Fox              Ryan Sea… Harry…\n15     15 Trent Ha… La'Porsh… January 6 (2016… Fox              Ryan Sea… Harry…\n16     16 Maddie P… Caleb Le… March 11 (2018-… ABC              Ryan Sea… Katy …\n17     17 Laine Ha… Alejandr… March 3 (2019-0… ABC              Ryan Sea… Katy …\n18     18 Just Sam  Arthur G… February 16 (20… ABC              Ryan Sea… Katy …\n# ℹ 3 more variables: no_of_episodes &lt;dbl&gt;, finals_venue &lt;chr&gt;, mentor &lt;chr&gt;\n\n$songs\n# A tibble: 2,429 × 8\n   season    week                order contestant song  artist song_theme result\n   &lt;chr&gt;     &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt; \n 1 Season_01 20020618_top_30_gr…     1 Tamyra Gr… And … Jenni… &lt;NA&gt;       Advan…\n 2 Season_01 20020618_top_30_gr…     2 Jim Verra… When… Doris… &lt;NA&gt;       Advan…\n 3 Season_01 20020618_top_30_gr…     3 Adriel He… I'll… Edwin… &lt;NA&gt;       Elimi…\n 4 Season_01 20020618_top_30_gr…     4 Rodesia E… Dayd… The M… &lt;NA&gt;       Elimi…\n 5 Season_01 20020618_top_30_gr…     5 Natalie B… Crazy Patsy… &lt;NA&gt;       Elimi…\n 6 Season_01 20020618_top_30_gr…     6 Brad Estr… Just… James… &lt;NA&gt;       Elimi…\n 7 Season_01 20020618_top_30_gr…     7 Ryan Starr The … The K… &lt;NA&gt;       Advan…\n 8 Season_01 20020618_top_30_gr…     8 Justinn W… When… Percy… &lt;NA&gt;       Elimi…\n 9 Season_01 20020618_top_30_gr…     9 Kelli Glo… I Wi… Dolly… &lt;NA&gt;       Wild …\n10 Season_01 20020618_top_30_gr…    10 Christoph… Stil… Brian… &lt;NA&gt;       Wild …\n# ℹ 2,419 more rows\n\n\n\nsummary(tuesdata)\n\n             Length Class       Mode\nauditions    12     spec_tbl_df list\neliminations 46     spec_tbl_df list\nfinalists     6     spec_tbl_df list\nratings      17     spec_tbl_df list\nseasons      10     spec_tbl_df list\nsongs         8     spec_tbl_df list\n\n\nChecking for missing values\n\nsapply(tuesdata, function(df) sum(is.na(df)))\n\n   auditions eliminations    finalists      ratings      seasons        songs \n         418        16763          107         5185           33         1686 \n\n\nRunning some summary statisics\n\nsapply(tuesdata, function(df) if(is.data.frame(df)) { summary(df) } else { NULL })\n\n$auditions\n     season      audition_date_start  audition_date_end    audition_city     \n Min.   : 1.00   Min.   :2002-04-20   Min.   :2002-04-22   Length:142        \n 1st Qu.: 6.00   1st Qu.:2006-08-11   1st Qu.:2006-08-11   Class :character  \n Median :10.00   Median :2010-09-05   Median :2010-09-05   Mode  :character  \n Mean   :10.37   Mean   :2011-04-14   Mean   :2011-04-14                     \n 3rd Qu.:15.00   3rd Qu.:2015-09-05   3rd Qu.:2015-09-05                     \n Max.   :18.00   Max.   :2019-09-21   Max.   :2019-09-21                     \n                                                                             \n audition_venue       episodes         episode_air_date   callback_venue    \n Length:142         Length:142         Length:142         Length:142        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n callback_date_start  callback_date_end    tickets_to_hollywood\n Min.   :2002-02-06   Min.   :2002-02-06   Min.   :  6.0       \n 1st Qu.:2006-10-02   1st Qu.:2006-10-03   1st Qu.: 20.0       \n Median :2010-11-09   Median :2010-11-10   Median : 29.0       \n Mean   :2011-06-11   Mean   :2011-06-12   Mean   : 41.8       \n 3rd Qu.:2015-09-13   3rd Qu.:2015-09-14   3rd Qu.: 37.0       \n Max.   :2019-09-21   Max.   :2019-09-21   Max.   :561.0       \n NA's   :13           NA's   :13           NA's   :48          \n guest_judge       \n Length:142        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\n$eliminations\n     season         place              gender           contestant       \n Min.   : 1.00   Length:456         Length:456         Length:456        \n 1st Qu.: 4.00   Class :character   Class :character   Class :character  \n Median : 8.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   : 8.86                                                           \n 3rd Qu.:13.00                                                           \n Max.   :18.00                                                           \n    top_36            top_36_2           top_36_3           top_36_4        \n Length:456         Length:456         Length:456         Length:456        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    top_32            top_32_2           top_32_3           top_32_4        \n Length:456         Length:456         Length:456         Length:456        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    top_30            top_30_2           top_30_3            top_25         \n Length:456         Length:456         Length:456         Length:456        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   top_25_2           top_25_3            top_24            top_24_2        \n Length:456         Length:456         Length:456         Length:456        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   top_24_3            top_20            top_20_2            top_16         \n Length:456         Length:456         Length:456         Length:456        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    top_14             top_13             top_12             top_11         \n Length:456         Length:456         Length:456         Length:456        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   top_11_2           wildcard         comeback          top_10         \n Length:456         Length:456         Mode:logical   Length:456        \n Class :character   Class :character   NA's:456       Class :character  \n Mode  :character   Mode  :character                  Mode  :character  \n                                                                        \n                                                                        \n                                                                        \n    top_9             top_9_2             top_8             top_8_2         \n Length:456         Length:456         Length:456         Length:456        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    top_7             top_7_2             top_6             top_6_2         \n Length:456         Length:456         Length:456         Length:456        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    top_5             top_5_2             top_4             top_4_2         \n Length:456         Length:456         Length:456         Length:456        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    top_3              finale         \n Length:456         Length:456        \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\n$finalists\n  Contestant          Birthday          Birthplace          Hometown        \n Length:190         Length:190         Length:190         Length:190        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n Description            Season      \n Length:190         Min.   : 1.000  \n Class :character   1st Qu.: 5.000  \n Mode  :character   Median : 9.000  \n                    Mean   : 8.863  \n                    3rd Qu.:13.000  \n                    Max.   :17.000  \n\n$ratings\n     season        show_number      episode            airdate         \n Min.   : 1.000   Min.   : 1.00   Length:593         Length:593        \n 1st Qu.: 4.000   1st Qu.: 9.00   Class :character   Class :character  \n Median : 8.000   Median :18.00   Mode  :character   Mode  :character  \n Mean   : 8.295   Mean   :19.24                                        \n 3rd Qu.:12.000   3rd Qu.:29.00                                        \n Max.   :18.000   Max.   :44.00                                        \n                                                                       \n 18_49_rating_share viewers_in_millions timeslot_et         dvr_18_49        \n Length:593         Min.   : 5.38       Length:593         Length:593        \n Class :character   1st Qu.:12.57       Class :character   Class :character  \n Mode  :character   Median :21.76       Mode  :character   Mode  :character  \n                    Mean   :19.88                                            \n                    3rd Qu.:26.09                                            \n                    Max.   :38.10                                            \n                    NA's   :3                                                \n dvr_viewers_millions total_18_49        total_viewers_millions\n Length:593           Length:593         Length:593            \n Class :character     Class :character   Class :character      \n Mode  :character     Mode  :character   Mode  :character      \n                                                               \n                                                               \n                                                               \n                                                               \n   weekrank           ref             share            nightlyrank   \n Length:593         Mode:logical   Length:593         Min.   :1.000  \n Class :character   NA's:593       Class :character   1st Qu.:1.000  \n Mode  :character                  Mode  :character   Median :2.000  \n                                                      Mean   :2.083  \n                                                      3rd Qu.:3.000  \n                                                      Max.   :4.000  \n                                                      NA's   :569    \n rating_share_households rating_share      \n Length:593              Length:593        \n Class :character        Class :character  \n Mode  :character        Mode  :character  \n                                           \n                                           \n                                           \n                                           \n\n$seasons\n     season         winner           runner_up         original_release  \n Min.   : 1.00   Length:18          Length:18          Length:18         \n 1st Qu.: 5.25   Class :character   Class :character   Class :character  \n Median : 9.50   Mode  :character   Mode  :character   Mode  :character  \n Mean   : 9.50                                                           \n 3rd Qu.:13.75                                                           \n Max.   :18.00                                                           \n                                                                         \n original_network    hosted_by            judges          no_of_episodes \n Length:18          Length:18          Length:18          Min.   :16.00  \n Class :character   Class :character   Class :character   1st Qu.:18.25  \n Mode  :character   Mode  :character   Mode  :character   Median :19.00  \n                                                          Mean   :19.50  \n                                                          3rd Qu.:20.25  \n                                                          Max.   :24.00  \n                                                          NA's   :14     \n finals_venue          mentor         \n Length:18          Length:18         \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n                                      \n\n$songs\n    season              week               order         contestant       \n Length:2429        Length:2429        Min.   : 1.000   Length:2429       \n Class :character   Class :character   1st Qu.: 3.000   Class :character  \n Mode  :character   Mode  :character   Median : 5.000   Mode  :character  \n                                       Mean   : 5.931                     \n                                       3rd Qu.: 8.000                     \n                                       Max.   :40.000                     \n     song              artist           song_theme           result         \n Length:2429        Length:2429        Length:2429        Length:2429       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n\n\nInstalling some libraries I’ll need for EDA\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\nlibrary(caTools)\n\nWarning: package 'caTools' was built under R version 4.3.3\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\n\nggplot(tuesdata$auditions, aes(x = season)) +\n  geom_histogram(binwidth = 1, fill = \"#2c3e50\", color = \"#ecf0f1\", alpha = 0.8) +\n  theme_minimal() +\n  labs(\n    title = \"Distribution of Auditions by Season\",\n    x = \"Season\",\n    y = \"Count\"\n  ) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    axis.title.x = element_text(size = 12, face = \"bold\"),\n    axis.title.y = element_text(size = 12, face = \"bold\"),\n    axis.text = element_text(size = 10),\n    panel.grid.major = element_line(color = \"#bdc3c7\"),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\nIs there any relationship, let’s say, between ratings and\n\nggplot(tuesdata$ratings, aes(x = viewers_in_millions, y = season)) +\n  geom_point() +\n  theme_minimal() +\n  labs(title = \"Relationship between Viewer and Season\", x = \"viewers_in_millions\", y = \"season\")\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nIt appears from this output that Season 5 was a peak in viewership. From what Wikipedia says: The fifth season of American Idol began on January 17, 2006, and concluded on May 24, 2006. Paula Abdul, Simon Cowell, and Randy Jackson returned as judges, while Ryan Seacrest returned as host. Taylor Hicks was named the winner, while Katharine McPhee was the runner-up. It also says that Season 6 set a new record of “74 million votes were cast in the finale round, and a new record of 609 million votes were cast in the entire season”.\nPART 2: Once you understand the data sufficiently, formulate a question/hypothesis.\nHypothesis:\nThe number of auditions per season is positively correlated with the ratings of the show for that season. Higher audition counts might indicate greater public interest, leading to higher viewership and ratings.Greater participation in auditions might also indicate a more engaged viewer base, which could translate to higher viewership numbers when during viewer voting.\nPART 3: Once you determine the question and thus your outcome and main predictors, further pre-process and clean the data as needed.\nFirst let’s merge ‘auditions’ and ‘ratings’"
  }
]