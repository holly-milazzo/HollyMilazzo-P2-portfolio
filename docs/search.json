[
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "I have journeyed the inner sanctums within the realm of Data Analytics for 8 years. In the spirit of fellowship, I have collaborated with cross-functional teams, ensuring that every stakeholder, from the boardroom to the front lines, understands the story that the data tells. Through trials and tribulations, much like the battles against Sauron’s forces, I have developed a keen eye for detecting anomalies and opportunities, ensuring the integrity and accuracy of the insights provided.\nMy hope is to glean any insights or experiences you wish to share on this quest.\nA peculiar fact about me is that I have no food allergies.\n\n\n\nClick icon to view Holly from the Shire\n\n\nMuch like the hidden gems of Middle-earth, this subreddit is filled with data visualizations guaranteed to captivate your interests: Link"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "DA6813 - Machine Learning Project",
    "section": "",
    "text": "Project Proposal: Understanding Consumer Behavior for Bridgerton Estate Financial Using Online Interaction Data\nBackground Bridgerton Estate Financial is a one-of-a-kind banking institution that offers a range of financial products, including personal loans, credit cards, mortgages, and savings accounts. As a forward-thinking company, Bridgerton Estate Financial has invested heavily in its digital channels to engage with customers online, through mobile applications, and via web portals. Customers can explore products, ask questions, and complete applications directly through these platforms. To optimize the customer journey, Bridgerton Estate Financial tracks key digital events from their customers’ online behaviors. These events include interactions like product page visits, application starts, and completions or abandonments of forms. Our goal is to analyze this event tracking data to better understand consumer behavior and determine which factors drive customers to complete an application for a financial product or abandon the process.\nMotivation As a data-driven organization, Bridgerton Estate Financial wants to enhance its understanding of consumer behavior on digital platforms to: • Improve Conversion Rates: Identify the key factors that lead to successful application completions and reduce application drop-offs. • Enhance the Customer Experience: Pinpoint areas of friction in the application process and improve the user interface to streamline the experience. • Optimize Marketing and Customer Engagement: Gain insights into when and how to engage customers more effectively, such as understanding peak interaction times or preferred devices. By leveraging data from customer interactions, the goal is to answer critical questions about the online behavior of Bridgerton Estate Financial’s customers, which would enable the company to make more informed decisions and improve customer conversion rates.\n\npacman::p_load(readr, here, dplyr, lubridate, corrplot, caret, ggplot2, car, e1071, vcd)\n\n\ndata_location &lt;- here::here(\"data-exercise\",\"Synthetic_Event_Data.csv\")\nrawdata &lt;- read_csv(data_location, show_col_types = FALSE)\n\n\nstr(rawdata)\n\nspc_tbl_ [1,000 × 17] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ EVENT_DT                      : chr [1:1000] \"10/17/2024\" \"10/17/2024\" \"10/17/2024\" \"10/17/2024\" ...\n $ EVENT_EFFECTIVE_GMT_TS        : 'hms' num [1:1000] 11:48:00 13:39:00 13:13:00 06:56:00 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ SOURCE_CHANNEL_CD             : chr [1:1000] \"www\" \"www\" \"www\" \"mob\" ...\n $ SOURCE_EVENT_ID               : chr [1:1000] \"bk-auto-loan-app-0\" \"bk-lending-cloe-fe-closing-1\" \"bk-card-cco-apply-2\" \"bk-auto-loan-app-3\" ...\n $ SOURCE_EVENT_DESC_TXT         : chr [1:1000] \"cco_approve_edit_card\" \"auto_loan_app_approved\" \"auto_loan_app_approved\" \"auto_loan_app_approved\" ...\n $ CORE_EVENT_TYPE_NM            : chr [1:1000] \"CSL\" \"ASL\" \"CSL\" \"CRC\" ...\n $ CORE_EVENT_SUBTYPE_NM         : chr [1:1000] \"DEC\" \"DEC\" \"DEC\" \"ACQ\" ...\n $ CORE_EVENT_NM_NOUN_NM         : num [1:1000] 655 690 760 771 776 603 600 786 661 719 ...\n $ ORIGINATING_CONTACT_POINT     : chr [1:1000] \"contact_point_6\" \"contact_point_5\" \"contact_point_4\" \"contact_point_3\" ...\n $ DESTINATION_CONTACT_POINT     : chr [1:1000] \"contact_point_3\" \"contact_point_4\" \"contact_point_9\" \"contact_point_6\" ...\n $ OTHER_ATTRIBUTES              : chr [1:1000] \"attribute_10\" \"attribute_10\" \"attribute_10\" \"attribute_4\" ...\n $ RELATED_PRODUCT               : chr [1:1000] \"product_4\" \"product_8\" \"product_10\" \"product_6\" ...\n $ COMMUNICATION_EVENT_ATTRIBUTES: chr [1:1000] \"comm_event_9\" \"comm_event_2\" \"comm_event_4\" \"comm_event_4\" ...\n $ CORE_EVENT_SUBTYPE_2_NM       : chr [1:1000] \"CLM\" \"CLM\" \"PRC\" \"CLM\" ...\n $ CORE_EVENT_SUBTYPE_3_NM       : chr [1:1000] \"APPLY FOR LOAN\" \"APPLY FOR LOAN\" \"APPLY FOR CONSUMER LOAN\" \"OPEN CREDIT CARD ACCOUNT\" ...\n $ POST_EVENT                    : chr [1:1000] \"App Not Complete\" \"App Not Complete\" \"App Not Complete\" \"App Not Complete\" ...\n $ CUSTOM_ATTRIBUTE              : chr [1:1000] \"custom_attr_6\" \"custom_attr_3\" \"custom_attr_4\" \"custom_attr_8\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   EVENT_DT = col_character(),\n  ..   EVENT_EFFECTIVE_GMT_TS = col_time(format = \"\"),\n  ..   SOURCE_CHANNEL_CD = col_character(),\n  ..   SOURCE_EVENT_ID = col_character(),\n  ..   SOURCE_EVENT_DESC_TXT = col_character(),\n  ..   CORE_EVENT_TYPE_NM = col_character(),\n  ..   CORE_EVENT_SUBTYPE_NM = col_character(),\n  ..   CORE_EVENT_NM_NOUN_NM = col_double(),\n  ..   ORIGINATING_CONTACT_POINT = col_character(),\n  ..   DESTINATION_CONTACT_POINT = col_character(),\n  ..   OTHER_ATTRIBUTES = col_character(),\n  ..   RELATED_PRODUCT = col_character(),\n  ..   COMMUNICATION_EVENT_ATTRIBUTES = col_character(),\n  ..   CORE_EVENT_SUBTYPE_2_NM = col_character(),\n  ..   CORE_EVENT_SUBTYPE_3_NM = col_character(),\n  ..   POST_EVENT = col_character(),\n  ..   CUSTOM_ATTRIBUTE = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nI will need to convert EVENT_DT to a date type and POST_EVENT to a factor as this is my target variable. I also think there will be some use in know specific day of the week and hour for consumer’s digital activity so I’m going to break this out into their own variables. To do the ‘HOUR’ parsing I will need the package ‘Lubridate’\n\nrawdata &lt;- rawdata %&gt;%\n  mutate(\n    # Converting EVENT_DT to a Date format\n    EVENT_DT = as.Date(EVENT_DT, format = \"%m/%d/%Y\"), #redundant because it's only 1 day of data\n    \n    # Properly encoding POST_EVENT as a binary numeric variable (0 for \"App Not Complete\", 1 for \"App Complete\")\n    POST_EVENT = ifelse(POST_EVENT == \"App Complete\", 1, 0),\n    \n    # Extracting the day of the week from EVENT_DT --for possible future use if I get more observations\n    #DAY_OF_WEEK = weekdays(EVENT_DT),\n    \n    # Extracting the hour of interaction from EVENT_EFFECTIVE_GMT_TS\n    HOUR = hour(EVENT_EFFECTIVE_GMT_TS),\n    \n    # Creating an interaction term between HOUR and SOURCE_CHANNEL_CD\n    HOUR_CHANNEL_INTERACTION = HOUR * as.numeric(as.factor(SOURCE_CHANNEL_CD))\n  ) %&gt;%\n  mutate_if(is.character, as.factor) # Converting all character variables to factors\n\nNow to inspect data for missing values\n\ncolSums(is.na(rawdata))\n\n                      EVENT_DT         EVENT_EFFECTIVE_GMT_TS \n                             0                              0 \n             SOURCE_CHANNEL_CD                SOURCE_EVENT_ID \n                             0                              0 \n         SOURCE_EVENT_DESC_TXT             CORE_EVENT_TYPE_NM \n                             0                              0 \n         CORE_EVENT_SUBTYPE_NM          CORE_EVENT_NM_NOUN_NM \n                             0                              0 \n     ORIGINATING_CONTACT_POINT      DESTINATION_CONTACT_POINT \n                             0                              0 \n              OTHER_ATTRIBUTES                RELATED_PRODUCT \n                             0                              0 \nCOMMUNICATION_EVENT_ATTRIBUTES        CORE_EVENT_SUBTYPE_2_NM \n                             0                              0 \n       CORE_EVENT_SUBTYPE_3_NM                     POST_EVENT \n                             0                              0 \n              CUSTOM_ATTRIBUTE                           HOUR \n                             0                              0 \n      HOUR_CHANNEL_INTERACTION \n                             0 \n\n\nChecking to see how balanced my response variable is\n\npost_event_balance &lt;- rawdata %&gt;%\n  count(POST_EVENT) %&gt;%\n  mutate(Proportion = n / sum(n))\n\nggplot(post_event_balance, aes(x = POST_EVENT, y = Proportion, fill = POST_EVENT)) +\n  geom_bar(stat = \"identity\", alpha = 0.8) +\n  geom_text(aes(label = paste0(round(Proportion * 100, 1), \"%\")), \n            position = position_stack(vjust = 0.5), \n            color = \"white\", \n            size = 5) +\n  labs(title = \"Distribution of Response (POST_EVENT): App Complete v App Not Complete\",\n       x = \"POST_EVENT\",\n       y = \"Proportion\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n#FEATURE ENGINEERING #Principal Component Analysis (PCA) SECTION for reducing and simplifying\n\nsingle_level_cols &lt;- names(Filter(function(x) length(unique(x)) == 1, rawdata))\n\nsingle_level_cols\n\n[1] \"EVENT_DT\"\n\n\nremove variable with only one level, EVENT_DT only had one day anyway\n\n# Removing Event_DT\nrawdata &lt;- rawdata[, !(names(rawdata) %in% single_level_cols)]\n\n\n# Subsetting the numeric variables in prep for PCA\nnumeric_data &lt;- rawdata %&gt;% select_if(is.numeric)\n\nscaled_numeric_data &lt;- scale(numeric_data)\n\n\n# Running PCA\npca_result &lt;- prcomp(scaled_numeric_data, scale. = TRUE)\n\nsummary(pca_result)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4\nStandard deviation     1.3263 1.0065 0.9947 0.48817\nProportion of Variance 0.4398 0.2533 0.2474 0.05958\nCumulative Proportion  0.4398 0.6931 0.9404 1.00000\n\n\nInterpretation: PC1 has the highest standard deviation, meaning it captures the most variance, it explains 43.98% of the variance alone so combining it with PC2 and PCA3 explainS 94.04% of the variance. Running the Scree plot below confirms this, I only need to retain 3.\n\ncumulative_variance &lt;- cumsum(pca_result$sdev^2 / sum(pca_result$sdev^2))\nplot(cumulative_variance, type = \"b\", xlab = \"Number of Components\", \n     ylab = \"Cumulative Variance Explained\", \n     main = \"Scree Plot on Cumulative Variance\")\nabline(h = 0.95, col = \"red\", lty = 2) # shows the 95% threshold\n\n\n\n\nI would like to retain these 3 components moving forward because one of the benefits of doing this would be that the PCA components are uncorrelated (orthogonal), which should resolve any multicollinearity issues. Unfortunately, the downside would be that by transforming these original variables they would no longer be interpretable. While a logistic/SVM model might benefit from this, I don’t think I’ll need such a transformation anyway when using random forest down the line.\n#Checking collinearity in numerical variables\nRunning VIF\n\nvif_model &lt;- lm(POST_EVENT ~ ., data = numeric_data)\nvif_values &lt;- vif(vif_model)\nprint(vif_values)\n\n   CORE_EVENT_NM_NOUN_NM                     HOUR HOUR_CHANNEL_INTERACTION \n                1.002510                 2.359174                 2.360680 \n\n\nThe VIF results indicate that all the variables have very low VIF values, with none exceeding the commonly accepted thresholds for multicollinearity (e.g., 5 for moderate multicollinearity or 10 for high multicollinearity). This means that multicollinearity for the numeric values is not a concern in my dataset and can idealy remain in the model.\n#Checking collinearity in Categorical variables\n\ncategorical_columns &lt;- rawdata %&gt;% select_if(is.factor)\n\n# Running pairwise Chi-Square + Cramer's V to cross-check all highly associated variable pairings\npairwise_cramers_v &lt;- combn(names(categorical_columns), 2, function(cols) {\n  var1 &lt;- categorical_columns[[cols[1]]]\n  var2 &lt;- categorical_columns[[cols[2]]]\n  cramer_val &lt;- assocstats(table(var1, var2))$cramer\n  data.frame(Var1 = cols[1], Var2 = cols[2], CramersV = cramer_val)\n}, simplify = FALSE) %&gt;% bind_rows()\n\nhigh_association &lt;- pairwise_cramers_v %&gt;% filter(CramersV &gt; 0.7) # Threshold for strong association\nprint(high_association)\n\n                Var1                           Var2 CramersV\n1  SOURCE_CHANNEL_CD                SOURCE_EVENT_ID        1\n2    SOURCE_EVENT_ID          SOURCE_EVENT_DESC_TXT        1\n3    SOURCE_EVENT_ID             CORE_EVENT_TYPE_NM        1\n4    SOURCE_EVENT_ID          CORE_EVENT_SUBTYPE_NM        1\n5    SOURCE_EVENT_ID      ORIGINATING_CONTACT_POINT        1\n6    SOURCE_EVENT_ID      DESTINATION_CONTACT_POINT        1\n7    SOURCE_EVENT_ID               OTHER_ATTRIBUTES        1\n8    SOURCE_EVENT_ID                RELATED_PRODUCT        1\n9    SOURCE_EVENT_ID COMMUNICATION_EVENT_ATTRIBUTES        1\n10   SOURCE_EVENT_ID        CORE_EVENT_SUBTYPE_2_NM        1\n11   SOURCE_EVENT_ID        CORE_EVENT_SUBTYPE_3_NM        1\n12   SOURCE_EVENT_ID               CUSTOM_ATTRIBUTE        1\n\n\nInterpretation of results: The results of the Cramer’s V test indicate a perfect associations (Cramer’s V = 1) between SOURCE_EVENT_ID and over half of the other categorical variables. This means that SOURCE_EVENT_ID is highly predictive of these variables or vice versa, suggesting redundancy. I’m going to remove this variable from my dataset moving forward. Dropping EVENT_EFFECTIVE_GMT_TS variable as we have already created an ‘Hour’ variable out of it and don’t want to use both.\n\nrawdata &lt;- rawdata %&gt;% select(-SOURCE_EVENT_ID) # Dropping SOURCE_EVENT_ID from dataset\n\nDouble checking my data set to see if everything is ready for modeling\n\nstr(rawdata)\n\ntibble [1,000 × 17] (S3: tbl_df/tbl/data.frame)\n $ EVENT_EFFECTIVE_GMT_TS        : 'hms' num [1:1000] 11:48:00 13:39:00 13:13:00 06:56:00 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ SOURCE_CHANNEL_CD             : Factor w/ 4 levels \"and\",\"iph\",\"mob\",..: 4 4 4 3 2 2 1 1 2 4 ...\n $ SOURCE_EVENT_DESC_TXT         : Factor w/ 3 levels \"auto_loan_app_approved\",..: 2 1 1 1 3 3 2 1 3 3 ...\n $ CORE_EVENT_TYPE_NM            : Factor w/ 3 levels \"ASL\",\"CRC\",\"CSL\": 3 1 3 2 1 2 1 2 3 1 ...\n $ CORE_EVENT_SUBTYPE_NM         : Factor w/ 3 levels \"ACQ\",\"ACQPL\",..: 3 3 3 1 2 2 1 3 1 1 ...\n $ CORE_EVENT_NM_NOUN_NM         : num [1:1000] 655 690 760 771 776 603 600 786 661 719 ...\n $ ORIGINATING_CONTACT_POINT     : Factor w/ 10 levels \"contact_point_1\",..: 7 6 5 4 4 8 8 3 9 8 ...\n $ DESTINATION_CONTACT_POINT     : Factor w/ 10 levels \"contact_point_1\",..: 4 5 10 7 1 3 4 2 8 7 ...\n $ OTHER_ATTRIBUTES              : Factor w/ 10 levels \"attribute_1\",..: 2 2 2 5 2 10 3 1 7 2 ...\n $ RELATED_PRODUCT               : Factor w/ 10 levels \"product_1\",\"product_10\",..: 5 9 2 7 8 3 3 7 6 1 ...\n $ COMMUNICATION_EVENT_ATTRIBUTES: Factor w/ 10 levels \"comm_event_1\",..: 10 3 5 5 4 1 4 6 10 5 ...\n $ CORE_EVENT_SUBTYPE_2_NM       : Factor w/ 3 levels \"CLM\",\"DEC\",\"PRC\": 1 1 3 1 1 2 3 1 3 2 ...\n $ CORE_EVENT_SUBTYPE_3_NM       : Factor w/ 3 levels \"APPLY FOR CONSUMER LOAN\",..: 2 2 1 3 3 2 1 3 2 3 ...\n $ POST_EVENT                    : num [1:1000] 0 0 0 0 1 0 1 0 1 0 ...\n $ CUSTOM_ATTRIBUTE              : Factor w/ 10 levels \"custom_attr_1\",..: 7 4 5 9 7 2 1 5 5 5 ...\n $ HOUR                          : int [1:1000] 11 13 13 6 0 21 12 10 19 14 ...\n $ HOUR_CHANNEL_INTERACTION      : num [1:1000] 44 52 52 18 0 42 12 10 38 56 ...\n\n\n#BEGIN MODELING SECTION**********************************************************\nIf I’m going to try models like SVM and Logistic Regression I may need to scale my data - (is this step needed?)\n\nrawdata_scaled &lt;- rawdata %&gt;%\n  mutate(across(where(is.numeric) & !all_of(\"POST_EVENT\"), scale))\n\n#SPLIT TRAIN/TEST\n\nset.seed(321)\n\ntrain_indices &lt;- createDataPartition(rawdata_scaled$POST_EVENT, p = 0.8, list = FALSE)\ntrain_data &lt;- rawdata_scaled[train_indices, ]\ntest_data &lt;- rawdata_scaled[-train_indices, ]\n\n#Logistic Regression (using PCA components dataset)\n\nnumeric_data &lt;- rawdata %&gt;% select_if(is.numeric)\nscaled_numeric_data &lt;- scale(numeric_data)\npca_result &lt;- prcomp(scaled_numeric_data, scale. = TRUE)\npca_transformed &lt;- as.data.frame(predict(pca_result, newdata = scaled_numeric_data)[, 1:3]) # Using only top 3 components\n\nrawdata_pca &lt;- cbind(POST_EVENT = rawdata$POST_EVENT, pca_transformed)\n\n\nlogistic_model &lt;- glm(POST_EVENT ~ ., data = rawdata_pca[train_indices, ], family = binomial)\n\nWarning: glm.fit: algorithm did not converge\n\nsummary(logistic_model)\n\n\nCall:\nglm(formula = POST_EVENT ~ ., family = binomial, data = rawdata_pca[train_indices, \n    ])\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)    -0.3743 12465.8147   0.000    1.000\nPC1             0.6449  9243.8807   0.000    1.000\nPC2            19.5005 12317.5677   0.002    0.999\nPC3            18.3588 12766.6379   0.001    0.999\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1.109e+03  on 799  degrees of freedom\nResidual deviance: 4.779e-09  on 796  degrees of freedom\nAIC: 8\n\nNumber of Fisher Scoring iterations: 25\n\n\nResults Logistic Model 1: model shit the bed - Write something here about using PCA components not being a working method for this\n#Logistic Regression (using rawdata set)\n\nset.seed(321)\ntrain_indices &lt;- createDataPartition(rawdata$POST_EVENT, p = 0.8, list = FALSE)\ntrain_data &lt;- rawdata[train_indices, ]\ntest_data &lt;- rawdata[-train_indices, ]\n\n\nlogistic_model2 &lt;- glm(POST_EVENT ~ ., data = train_data, family = binomial)\n\nsummary(logistic_model2)\n\n\nCall:\nglm(formula = POST_EVENT ~ ., family = binomial, data = train_data)\n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                           -5.473e-01  1.203e+00\nEVENT_EFFECTIVE_GMT_TS                                -9.053e-05  7.501e-05\nSOURCE_CHANNEL_CDiph                                   8.395e-02  2.436e-01\nSOURCE_CHANNEL_CDmob                                   1.788e-01  3.220e-01\nSOURCE_CHANNEL_CDwww                                   3.563e-01  4.166e-01\nSOURCE_EVENT_DESC_TXTcco_approve_edit_card            -2.519e-01  1.920e-01\nSOURCE_EVENT_DESC_TXTpul_loan_app_post_bureau_decline -1.706e-01  1.909e-01\nCORE_EVENT_TYPE_NMCRC                                  8.732e-02  1.913e-01\nCORE_EVENT_TYPE_NMCSL                                  2.405e-01  1.951e-01\nCORE_EVENT_SUBTYPE_NMACQPL                             2.132e-02  1.853e-01\nCORE_EVENT_SUBTYPE_NMDEC                               1.075e-01  1.878e-01\nCORE_EVENT_NM_NOUN_NM                                  1.283e-03  1.365e-03\nORIGINATING_CONTACT_POINTcontact_point_10             -5.536e-01  3.373e-01\nORIGINATING_CONTACT_POINTcontact_point_2              -3.244e-01  3.449e-01\nORIGINATING_CONTACT_POINTcontact_point_3              -2.714e-02  3.593e-01\nORIGINATING_CONTACT_POINTcontact_point_4              -4.972e-01  3.684e-01\nORIGINATING_CONTACT_POINTcontact_point_5              -1.950e-01  3.287e-01\nORIGINATING_CONTACT_POINTcontact_point_6              -4.380e-01  3.333e-01\nORIGINATING_CONTACT_POINTcontact_point_7               3.325e-02  3.458e-01\nORIGINATING_CONTACT_POINTcontact_point_8              -8.129e-01  3.503e-01\nORIGINATING_CONTACT_POINTcontact_point_9              -5.126e-01  3.565e-01\nDESTINATION_CONTACT_POINTcontact_point_10             -1.431e-01  3.430e-01\nDESTINATION_CONTACT_POINTcontact_point_2               8.348e-02  3.399e-01\nDESTINATION_CONTACT_POINTcontact_point_3               1.153e-01  3.528e-01\nDESTINATION_CONTACT_POINTcontact_point_4              -2.408e-02  3.574e-01\nDESTINATION_CONTACT_POINTcontact_point_5               3.675e-02  3.535e-01\nDESTINATION_CONTACT_POINTcontact_point_6               5.276e-01  3.597e-01\nDESTINATION_CONTACT_POINTcontact_point_7               8.805e-02  3.567e-01\nDESTINATION_CONTACT_POINTcontact_point_8              -9.194e-02  3.520e-01\nDESTINATION_CONTACT_POINTcontact_point_9              -3.673e-01  3.432e-01\nOTHER_ATTRIBUTESattribute_10                          -6.796e-02  3.212e-01\nOTHER_ATTRIBUTESattribute_2                            2.586e-01  3.425e-01\nOTHER_ATTRIBUTESattribute_3                            1.835e-01  3.105e-01\nOTHER_ATTRIBUTESattribute_4                            8.968e-02  3.374e-01\nOTHER_ATTRIBUTESattribute_5                           -2.108e-01  3.435e-01\nOTHER_ATTRIBUTESattribute_6                           -2.760e-01  3.298e-01\nOTHER_ATTRIBUTESattribute_7                            6.711e-01  3.348e-01\nOTHER_ATTRIBUTESattribute_8                            1.511e-01  3.388e-01\nOTHER_ATTRIBUTESattribute_9                            3.594e-01  3.249e-01\nRELATED_PRODUCTproduct_10                              8.978e-01  3.496e-01\nRELATED_PRODUCTproduct_2                               5.074e-01  3.412e-01\nRELATED_PRODUCTproduct_3                               9.146e-02  3.552e-01\nRELATED_PRODUCTproduct_4                               3.756e-01  3.406e-01\nRELATED_PRODUCTproduct_5                               5.396e-01  3.578e-01\nRELATED_PRODUCTproduct_6                               1.073e+00  3.750e-01\nRELATED_PRODUCTproduct_7                               2.789e-01  3.593e-01\nRELATED_PRODUCTproduct_8                              -2.059e-01  3.619e-01\nRELATED_PRODUCTproduct_9                               3.406e-01  3.756e-01\nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_10           -8.177e-01  3.564e-01\nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_2            -3.083e-01  3.316e-01\nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_3            -2.630e-01  3.579e-01\nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_4            -7.216e-01  3.372e-01\nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_5            -6.123e-01  3.428e-01\nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_6            -8.269e-02  3.501e-01\nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_7            -8.027e-01  3.554e-01\nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_8            -2.135e-01  3.436e-01\nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_9            -5.039e-01  3.486e-01\nCORE_EVENT_SUBTYPE_2_NMDEC                            -3.048e-01  1.960e-01\nCORE_EVENT_SUBTYPE_2_NMPRC                            -6.515e-02  1.931e-01\nCORE_EVENT_SUBTYPE_3_NMAPPLY FOR LOAN                  7.126e-02  1.859e-01\nCORE_EVENT_SUBTYPE_3_NMOPEN CREDIT CARD ACCOUNT        2.286e-01  1.971e-01\nCUSTOM_ATTRIBUTEcustom_attr_10                        -6.731e-02  3.771e-01\nCUSTOM_ATTRIBUTEcustom_attr_2                         -1.335e-01  3.632e-01\nCUSTOM_ATTRIBUTEcustom_attr_3                         -4.031e-01  3.454e-01\nCUSTOM_ATTRIBUTEcustom_attr_4                         -3.308e-01  3.395e-01\nCUSTOM_ATTRIBUTEcustom_attr_5                         -1.720e-01  3.733e-01\nCUSTOM_ATTRIBUTEcustom_attr_6                         -3.132e-01  3.497e-01\nCUSTOM_ATTRIBUTEcustom_attr_7                         -1.263e-01  3.662e-01\nCUSTOM_ATTRIBUTEcustom_attr_8                         -3.190e-01  3.516e-01\nCUSTOM_ATTRIBUTEcustom_attr_9                         -2.851e-02  3.643e-01\nHOUR                                                   3.821e-01  2.723e-01\nHOUR_CHANNEL_INTERACTION                              -1.887e-02  1.010e-02\n                                                      z value Pr(&gt;|z|)   \n(Intercept)                                            -0.455  0.64920   \nEVENT_EFFECTIVE_GMT_TS                                 -1.207  0.22744   \nSOURCE_CHANNEL_CDiph                                    0.345  0.73035   \nSOURCE_CHANNEL_CDmob                                    0.555  0.57866   \nSOURCE_CHANNEL_CDwww                                    0.855  0.39243   \nSOURCE_EVENT_DESC_TXTcco_approve_edit_card             -1.312  0.18943   \nSOURCE_EVENT_DESC_TXTpul_loan_app_post_bureau_decline  -0.893  0.37169   \nCORE_EVENT_TYPE_NMCRC                                   0.457  0.64801   \nCORE_EVENT_TYPE_NMCSL                                   1.233  0.21763   \nCORE_EVENT_SUBTYPE_NMACQPL                              0.115  0.90840   \nCORE_EVENT_SUBTYPE_NMDEC                                0.572  0.56708   \nCORE_EVENT_NM_NOUN_NM                                   0.940  0.34698   \nORIGINATING_CONTACT_POINTcontact_point_10              -1.641  0.10076   \nORIGINATING_CONTACT_POINTcontact_point_2               -0.940  0.34699   \nORIGINATING_CONTACT_POINTcontact_point_3               -0.076  0.93980   \nORIGINATING_CONTACT_POINTcontact_point_4               -1.350  0.17712   \nORIGINATING_CONTACT_POINTcontact_point_5               -0.593  0.55302   \nORIGINATING_CONTACT_POINTcontact_point_6               -1.314  0.18885   \nORIGINATING_CONTACT_POINTcontact_point_7                0.096  0.92339   \nORIGINATING_CONTACT_POINTcontact_point_8               -2.321  0.02029 * \nORIGINATING_CONTACT_POINTcontact_point_9               -1.438  0.15049   \nDESTINATION_CONTACT_POINTcontact_point_10              -0.417  0.67655   \nDESTINATION_CONTACT_POINTcontact_point_2                0.246  0.80598   \nDESTINATION_CONTACT_POINTcontact_point_3                0.327  0.74376   \nDESTINATION_CONTACT_POINTcontact_point_4               -0.067  0.94627   \nDESTINATION_CONTACT_POINTcontact_point_5                0.104  0.91722   \nDESTINATION_CONTACT_POINTcontact_point_6                1.467  0.14241   \nDESTINATION_CONTACT_POINTcontact_point_7                0.247  0.80501   \nDESTINATION_CONTACT_POINTcontact_point_8               -0.261  0.79396   \nDESTINATION_CONTACT_POINTcontact_point_9               -1.070  0.28457   \nOTHER_ATTRIBUTESattribute_10                           -0.212  0.83241   \nOTHER_ATTRIBUTESattribute_2                             0.755  0.45024   \nOTHER_ATTRIBUTESattribute_3                             0.591  0.55459   \nOTHER_ATTRIBUTESattribute_4                             0.266  0.79037   \nOTHER_ATTRIBUTESattribute_5                            -0.614  0.53954   \nOTHER_ATTRIBUTESattribute_6                            -0.837  0.40262   \nOTHER_ATTRIBUTESattribute_7                             2.005  0.04499 * \nOTHER_ATTRIBUTESattribute_8                             0.446  0.65549   \nOTHER_ATTRIBUTESattribute_9                             1.106  0.26862   \nRELATED_PRODUCTproduct_10                               2.568  0.01023 * \nRELATED_PRODUCTproduct_2                                1.487  0.13693   \nRELATED_PRODUCTproduct_3                                0.257  0.79681   \nRELATED_PRODUCTproduct_4                                1.103  0.27019   \nRELATED_PRODUCTproduct_5                                1.508  0.13149   \nRELATED_PRODUCTproduct_6                                2.862  0.00421 **\nRELATED_PRODUCTproduct_7                                0.776  0.43769   \nRELATED_PRODUCTproduct_8                               -0.569  0.56948   \nRELATED_PRODUCTproduct_9                                0.907  0.36454   \nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_10            -2.295  0.02176 * \nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_2             -0.930  0.35251   \nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_3             -0.735  0.46238   \nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_4             -2.140  0.03237 * \nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_5             -1.786  0.07409 . \nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_6             -0.236  0.81330   \nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_7             -2.259  0.02390 * \nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_8             -0.621  0.53433   \nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_9             -1.445  0.14833   \nCORE_EVENT_SUBTYPE_2_NMDEC                             -1.555  0.11992   \nCORE_EVENT_SUBTYPE_2_NMPRC                             -0.337  0.73579   \nCORE_EVENT_SUBTYPE_3_NMAPPLY FOR LOAN                   0.383  0.70155   \nCORE_EVENT_SUBTYPE_3_NMOPEN CREDIT CARD ACCOUNT         1.160  0.24624   \nCUSTOM_ATTRIBUTEcustom_attr_10                         -0.178  0.85834   \nCUSTOM_ATTRIBUTEcustom_attr_2                          -0.368  0.71324   \nCUSTOM_ATTRIBUTEcustom_attr_3                          -1.167  0.24317   \nCUSTOM_ATTRIBUTEcustom_attr_4                          -0.974  0.32990   \nCUSTOM_ATTRIBUTEcustom_attr_5                          -0.461  0.64501   \nCUSTOM_ATTRIBUTEcustom_attr_6                          -0.896  0.37049   \nCUSTOM_ATTRIBUTEcustom_attr_7                          -0.345  0.73021   \nCUSTOM_ATTRIBUTEcustom_attr_8                          -0.907  0.36423   \nCUSTOM_ATTRIBUTEcustom_attr_9                          -0.078  0.93762   \nHOUR                                                    1.403  0.16056   \nHOUR_CHANNEL_INTERACTION                               -1.868  0.06178 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1109.0  on 799  degrees of freedom\nResidual deviance: 1027.6  on 728  degrees of freedom\nAIC: 1171.6\n\nNumber of Fisher Scoring iterations: 4\n\n\nResults Logistic Model 2: There appears to be a lot of variables with no significance, let’s apply some stepwise/backward elimination to our model\n\nlogistic_model3 &lt;- step(logistic_model2, direction = \"backward\")\n\nStart:  AIC=1171.64\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + SOURCE_CHANNEL_CD + SOURCE_EVENT_DESC_TXT + \n    CORE_EVENT_TYPE_NM + CORE_EVENT_SUBTYPE_NM + CORE_EVENT_NM_NOUN_NM + \n    ORIGINATING_CONTACT_POINT + DESTINATION_CONTACT_POINT + OTHER_ATTRIBUTES + \n    RELATED_PRODUCT + COMMUNICATION_EVENT_ATTRIBUTES + CORE_EVENT_SUBTYPE_2_NM + \n    CORE_EVENT_SUBTYPE_3_NM + CUSTOM_ATTRIBUTE + HOUR + HOUR_CHANNEL_INTERACTION\n\n                                 Df Deviance    AIC\n- CUSTOM_ATTRIBUTE                9   1030.8 1156.8\n- DESTINATION_CONTACT_POINT       9   1035.5 1161.5\n- OTHER_ATTRIBUTES                9   1038.7 1164.7\n- ORIGINATING_CONTACT_POINT       9   1039.4 1165.4\n- SOURCE_CHANNEL_CD               3   1028.4 1166.4\n- COMMUNICATION_EVENT_ATTRIBUTES  9   1040.5 1166.5\n- CORE_EVENT_SUBTYPE_NM           2   1028.0 1168.0\n- CORE_EVENT_SUBTYPE_3_NM         2   1029.0 1169.0\n- CORE_EVENT_TYPE_NM              2   1029.2 1169.2\n- SOURCE_EVENT_DESC_TXT           2   1029.4 1169.4\n- CORE_EVENT_SUBTYPE_2_NM         2   1030.4 1170.4\n- CORE_EVENT_NM_NOUN_NM           1   1028.5 1170.5\n- EVENT_EFFECTIVE_GMT_TS          1   1029.1 1171.1\n- HOUR                            1   1029.6 1171.6\n&lt;none&gt;                                1027.6 1171.6\n- HOUR_CHANNEL_INTERACTION        1   1031.2 1173.2\n- RELATED_PRODUCT                 9   1049.5 1175.5\n\nStep:  AIC=1156.76\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + SOURCE_CHANNEL_CD + SOURCE_EVENT_DESC_TXT + \n    CORE_EVENT_TYPE_NM + CORE_EVENT_SUBTYPE_NM + CORE_EVENT_NM_NOUN_NM + \n    ORIGINATING_CONTACT_POINT + DESTINATION_CONTACT_POINT + OTHER_ATTRIBUTES + \n    RELATED_PRODUCT + COMMUNICATION_EVENT_ATTRIBUTES + CORE_EVENT_SUBTYPE_2_NM + \n    CORE_EVENT_SUBTYPE_3_NM + HOUR + HOUR_CHANNEL_INTERACTION\n\n                                 Df Deviance    AIC\n- DESTINATION_CONTACT_POINT       9   1038.7 1146.7\n- OTHER_ATTRIBUTES                9   1042.2 1150.2\n- ORIGINATING_CONTACT_POINT       9   1042.9 1150.9\n- SOURCE_CHANNEL_CD               3   1031.6 1151.6\n- COMMUNICATION_EVENT_ATTRIBUTES  9   1044.0 1152.0\n- CORE_EVENT_SUBTYPE_NM           2   1031.2 1153.2\n- CORE_EVENT_SUBTYPE_3_NM         2   1032.1 1154.1\n- SOURCE_EVENT_DESC_TXT           2   1032.4 1154.4\n- CORE_EVENT_TYPE_NM              2   1032.6 1154.6\n- CORE_EVENT_SUBTYPE_2_NM         2   1033.4 1155.4\n- CORE_EVENT_NM_NOUN_NM           1   1031.8 1155.8\n- EVENT_EFFECTIVE_GMT_TS          1   1032.0 1156.0\n- HOUR                            1   1032.5 1156.5\n&lt;none&gt;                                1030.8 1156.8\n- HOUR_CHANNEL_INTERACTION        1   1034.4 1158.4\n- RELATED_PRODUCT                 9   1052.6 1160.6\n\nStep:  AIC=1146.7\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + SOURCE_CHANNEL_CD + SOURCE_EVENT_DESC_TXT + \n    CORE_EVENT_TYPE_NM + CORE_EVENT_SUBTYPE_NM + CORE_EVENT_NM_NOUN_NM + \n    ORIGINATING_CONTACT_POINT + OTHER_ATTRIBUTES + RELATED_PRODUCT + \n    COMMUNICATION_EVENT_ATTRIBUTES + CORE_EVENT_SUBTYPE_2_NM + \n    CORE_EVENT_SUBTYPE_3_NM + HOUR + HOUR_CHANNEL_INTERACTION\n\n                                 Df Deviance    AIC\n- OTHER_ATTRIBUTES                9   1049.8 1139.8\n- ORIGINATING_CONTACT_POINT       9   1050.8 1140.8\n- COMMUNICATION_EVENT_ATTRIBUTES  9   1051.4 1141.4\n- SOURCE_CHANNEL_CD               3   1040.0 1142.0\n- CORE_EVENT_SUBTYPE_NM           2   1039.0 1143.0\n- SOURCE_EVENT_DESC_TXT           2   1040.2 1144.2\n- CORE_EVENT_SUBTYPE_3_NM         2   1040.4 1144.4\n- CORE_EVENT_TYPE_NM              2   1040.5 1144.5\n- CORE_EVENT_NM_NOUN_NM           1   1039.6 1145.6\n- EVENT_EFFECTIVE_GMT_TS          1   1039.9 1145.9\n- CORE_EVENT_SUBTYPE_2_NM         2   1042.2 1146.2\n- HOUR                            1   1040.4 1146.4\n&lt;none&gt;                                1038.7 1146.7\n- HOUR_CHANNEL_INTERACTION        1   1043.2 1149.2\n- RELATED_PRODUCT                 9   1059.5 1149.5\n\nStep:  AIC=1139.76\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + SOURCE_CHANNEL_CD + SOURCE_EVENT_DESC_TXT + \n    CORE_EVENT_TYPE_NM + CORE_EVENT_SUBTYPE_NM + CORE_EVENT_NM_NOUN_NM + \n    ORIGINATING_CONTACT_POINT + RELATED_PRODUCT + COMMUNICATION_EVENT_ATTRIBUTES + \n    CORE_EVENT_SUBTYPE_2_NM + CORE_EVENT_SUBTYPE_3_NM + HOUR + \n    HOUR_CHANNEL_INTERACTION\n\n                                 Df Deviance    AIC\n- ORIGINATING_CONTACT_POINT       9   1062.0 1134.0\n- SOURCE_CHANNEL_CD               3   1050.6 1134.6\n- COMMUNICATION_EVENT_ATTRIBUTES  9   1063.4 1135.4\n- CORE_EVENT_SUBTYPE_NM           2   1050.3 1136.3\n- SOURCE_EVENT_DESC_TXT           2   1051.1 1137.1\n- CORE_EVENT_TYPE_NM              2   1051.2 1137.2\n- CORE_EVENT_SUBTYPE_3_NM         2   1051.5 1137.5\n- CORE_EVENT_SUBTYPE_2_NM         2   1052.2 1138.2\n- CORE_EVENT_NM_NOUN_NM           1   1050.8 1138.8\n- EVENT_EFFECTIVE_GMT_TS          1   1051.0 1139.0\n- HOUR                            1   1051.5 1139.5\n&lt;none&gt;                                1049.8 1139.8\n- RELATED_PRODUCT                 9   1069.0 1141.0\n- HOUR_CHANNEL_INTERACTION        1   1053.2 1141.2\n\nStep:  AIC=1134.01\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + SOURCE_CHANNEL_CD + SOURCE_EVENT_DESC_TXT + \n    CORE_EVENT_TYPE_NM + CORE_EVENT_SUBTYPE_NM + CORE_EVENT_NM_NOUN_NM + \n    RELATED_PRODUCT + COMMUNICATION_EVENT_ATTRIBUTES + CORE_EVENT_SUBTYPE_2_NM + \n    CORE_EVENT_SUBTYPE_3_NM + HOUR + HOUR_CHANNEL_INTERACTION\n\n                                 Df Deviance    AIC\n- SOURCE_CHANNEL_CD               3   1062.8 1128.8\n- COMMUNICATION_EVENT_ATTRIBUTES  9   1076.1 1130.1\n- CORE_EVENT_SUBTYPE_NM           2   1062.7 1130.7\n- SOURCE_EVENT_DESC_TXT           2   1062.8 1130.8\n- CORE_EVENT_TYPE_NM              2   1063.6 1131.6\n- CORE_EVENT_SUBTYPE_2_NM         2   1064.0 1132.0\n- CORE_EVENT_SUBTYPE_3_NM         2   1064.2 1132.2\n- EVENT_EFFECTIVE_GMT_TS          1   1063.2 1133.2\n- CORE_EVENT_NM_NOUN_NM           1   1063.4 1133.4\n- HOUR                            1   1063.6 1133.6\n&lt;none&gt;                                1062.0 1134.0\n- HOUR_CHANNEL_INTERACTION        1   1065.5 1135.5\n- RELATED_PRODUCT                 9   1081.8 1135.8\n\nStep:  AIC=1128.79\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + SOURCE_EVENT_DESC_TXT + \n    CORE_EVENT_TYPE_NM + CORE_EVENT_SUBTYPE_NM + CORE_EVENT_NM_NOUN_NM + \n    RELATED_PRODUCT + COMMUNICATION_EVENT_ATTRIBUTES + CORE_EVENT_SUBTYPE_2_NM + \n    CORE_EVENT_SUBTYPE_3_NM + HOUR + HOUR_CHANNEL_INTERACTION\n\n                                 Df Deviance    AIC\n- COMMUNICATION_EVENT_ATTRIBUTES  9   1077.2 1125.2\n- CORE_EVENT_SUBTYPE_NM           2   1063.5 1125.5\n- SOURCE_EVENT_DESC_TXT           2   1063.6 1125.6\n- CORE_EVENT_TYPE_NM              2   1064.5 1126.5\n- CORE_EVENT_SUBTYPE_2_NM         2   1064.8 1126.8\n- CORE_EVENT_SUBTYPE_3_NM         2   1065.0 1127.0\n- EVENT_EFFECTIVE_GMT_TS          1   1063.8 1127.8\n- CORE_EVENT_NM_NOUN_NM           1   1064.1 1128.1\n- HOUR                            1   1064.2 1128.2\n&lt;none&gt;                                1062.8 1128.8\n- RELATED_PRODUCT                 9   1082.4 1130.4\n- HOUR_CHANNEL_INTERACTION        1   1068.4 1132.4\n\nStep:  AIC=1125.25\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + SOURCE_EVENT_DESC_TXT + \n    CORE_EVENT_TYPE_NM + CORE_EVENT_SUBTYPE_NM + CORE_EVENT_NM_NOUN_NM + \n    RELATED_PRODUCT + CORE_EVENT_SUBTYPE_2_NM + CORE_EVENT_SUBTYPE_3_NM + \n    HOUR + HOUR_CHANNEL_INTERACTION\n\n                           Df Deviance    AIC\n- CORE_EVENT_SUBTYPE_NM     2   1077.7 1121.7\n- SOURCE_EVENT_DESC_TXT     2   1078.2 1122.2\n- CORE_EVENT_TYPE_NM        2   1078.6 1122.6\n- CORE_EVENT_SUBTYPE_3_NM   2   1079.2 1123.2\n- CORE_EVENT_SUBTYPE_2_NM   2   1080.3 1124.3\n- CORE_EVENT_NM_NOUN_NM     1   1078.3 1124.3\n- EVENT_EFFECTIVE_GMT_TS    1   1078.6 1124.6\n- HOUR                      1   1079.0 1125.0\n&lt;none&gt;                          1077.2 1125.2\n- RELATED_PRODUCT           9   1095.7 1125.7\n- HOUR_CHANNEL_INTERACTION  1   1082.5 1128.5\n\nStep:  AIC=1121.72\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + SOURCE_EVENT_DESC_TXT + \n    CORE_EVENT_TYPE_NM + CORE_EVENT_NM_NOUN_NM + RELATED_PRODUCT + \n    CORE_EVENT_SUBTYPE_2_NM + CORE_EVENT_SUBTYPE_3_NM + HOUR + \n    HOUR_CHANNEL_INTERACTION\n\n                           Df Deviance    AIC\n- SOURCE_EVENT_DESC_TXT     2   1078.7 1118.7\n- CORE_EVENT_TYPE_NM        2   1079.1 1119.1\n- CORE_EVENT_SUBTYPE_3_NM   2   1079.7 1119.7\n- CORE_EVENT_SUBTYPE_2_NM   2   1080.7 1120.7\n- CORE_EVENT_NM_NOUN_NM     1   1078.8 1120.8\n- EVENT_EFFECTIVE_GMT_TS    1   1079.1 1121.1\n- HOUR                      1   1079.5 1121.5\n&lt;none&gt;                          1077.7 1121.7\n- RELATED_PRODUCT           9   1096.0 1122.0\n- HOUR_CHANNEL_INTERACTION  1   1083.1 1125.1\n\nStep:  AIC=1118.73\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + CORE_EVENT_TYPE_NM + CORE_EVENT_NM_NOUN_NM + \n    RELATED_PRODUCT + CORE_EVENT_SUBTYPE_2_NM + CORE_EVENT_SUBTYPE_3_NM + \n    HOUR + HOUR_CHANNEL_INTERACTION\n\n                           Df Deviance    AIC\n- CORE_EVENT_TYPE_NM        2   1080.1 1116.1\n- CORE_EVENT_SUBTYPE_3_NM   2   1080.9 1116.9\n- CORE_EVENT_SUBTYPE_2_NM   2   1081.5 1117.5\n- CORE_EVENT_NM_NOUN_NM     1   1079.8 1117.8\n- EVENT_EFFECTIVE_GMT_TS    1   1080.1 1118.1\n- HOUR                      1   1080.5 1118.5\n&lt;none&gt;                          1078.7 1118.7\n- RELATED_PRODUCT           9   1096.8 1118.8\n- HOUR_CHANNEL_INTERACTION  1   1084.4 1122.4\n\nStep:  AIC=1116.12\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + CORE_EVENT_NM_NOUN_NM + \n    RELATED_PRODUCT + CORE_EVENT_SUBTYPE_2_NM + CORE_EVENT_SUBTYPE_3_NM + \n    HOUR + HOUR_CHANNEL_INTERACTION\n\n                           Df Deviance    AIC\n- CORE_EVENT_SUBTYPE_3_NM   2   1082.3 1114.3\n- CORE_EVENT_SUBTYPE_2_NM   2   1082.9 1114.9\n- CORE_EVENT_NM_NOUN_NM     1   1081.3 1115.3\n- EVENT_EFFECTIVE_GMT_TS    1   1081.4 1115.4\n- HOUR                      1   1081.7 1115.7\n&lt;none&gt;                          1080.1 1116.1\n- RELATED_PRODUCT           9   1098.2 1116.2\n- HOUR_CHANNEL_INTERACTION  1   1085.5 1119.5\n\nStep:  AIC=1114.28\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + CORE_EVENT_NM_NOUN_NM + \n    RELATED_PRODUCT + CORE_EVENT_SUBTYPE_2_NM + HOUR + HOUR_CHANNEL_INTERACTION\n\n                           Df Deviance    AIC\n- CORE_EVENT_SUBTYPE_2_NM   2   1084.7 1112.7\n- EVENT_EFFECTIVE_GMT_TS    1   1083.3 1113.3\n- CORE_EVENT_NM_NOUN_NM     1   1083.4 1113.4\n- HOUR                      1   1083.6 1113.6\n- RELATED_PRODUCT           9   1100.0 1114.0\n&lt;none&gt;                          1082.3 1114.3\n- HOUR_CHANNEL_INTERACTION  1   1087.1 1117.1\n\nStep:  AIC=1112.7\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + CORE_EVENT_NM_NOUN_NM + \n    RELATED_PRODUCT + HOUR + HOUR_CHANNEL_INTERACTION\n\n                           Df Deviance    AIC\n- EVENT_EFFECTIVE_GMT_TS    1   1085.7 1111.7\n- RELATED_PRODUCT           9   1101.9 1111.9\n- HOUR                      1   1086.0 1112.0\n- CORE_EVENT_NM_NOUN_NM     1   1086.0 1112.0\n&lt;none&gt;                          1084.7 1112.7\n- HOUR_CHANNEL_INTERACTION  1   1089.1 1115.1\n\nStep:  AIC=1111.74\nPOST_EVENT ~ CORE_EVENT_NM_NOUN_NM + RELATED_PRODUCT + HOUR + \n    HOUR_CHANNEL_INTERACTION\n\n                           Df Deviance    AIC\n- RELATED_PRODUCT           9   1103.1 1111.1\n- CORE_EVENT_NM_NOUN_NM     1   1087.2 1111.2\n&lt;none&gt;                          1085.7 1111.7\n- HOUR_CHANNEL_INTERACTION  1   1090.3 1114.3\n- HOUR                      1   1090.5 1114.5\n\nStep:  AIC=1111.06\nPOST_EVENT ~ CORE_EVENT_NM_NOUN_NM + HOUR + HOUR_CHANNEL_INTERACTION\n\n                           Df Deviance    AIC\n- CORE_EVENT_NM_NOUN_NM     1   1104.5 1110.5\n&lt;none&gt;                          1103.1 1111.1\n- HOUR_CHANNEL_INTERACTION  1   1107.2 1113.2\n- HOUR                      1   1107.2 1113.2\n\nStep:  AIC=1110.48\nPOST_EVENT ~ HOUR + HOUR_CHANNEL_INTERACTION\n\n                           Df Deviance    AIC\n&lt;none&gt;                          1104.5 1110.5\n- HOUR_CHANNEL_INTERACTION  1   1108.4 1112.4\n- HOUR                      1   1108.5 1112.5\n\nsummary(logistic_model3)\n\n\nCall:\nglm(formula = POST_EVENT ~ HOUR + HOUR_CHANNEL_INTERACTION, family = binomial, \n    data = train_data)\n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)              -0.119246   0.141714  -0.841   0.4001  \nHOUR                      0.031739   0.015996   1.984   0.0472 *\nHOUR_CHANNEL_INTERACTION -0.009245   0.004688  -1.972   0.0486 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1109.0  on 799  degrees of freedom\nResidual deviance: 1104.5  on 797  degrees of freedom\nAIC: 1110.5\n\nNumber of Fisher Scoring iterations: 3\n\n\nResults Logistic Model 3:The backward elimination process significantly simplified the model leaving only two predictors: HOUR and HOUR_CHANNEL_INTERACTION\nModel strength: Predictors are statistically significant, providing meaningful insights into how time-related factors influence application completion Both predictors are statistically significant (𝑝&lt;0.05 p&lt;0.05): HOUR:𝑝=0.0472 HOUR_CHANNEL_INTERACTION:𝑝=0.0486 Model Weakness: The model does not explain much variability for the target variable and only has two predictors and lacks any complexity for achieving any real accuracy\n\nlogistic_pred3 &lt;- predict(logistic_model3, newdata = test_data, type = \"response\")\nlogistic_class3 &lt;- ifelse(logistic_pred3 &gt; 0.5, 1, 0)\n\n\nconfusionMatrix(as.factor(logistic_class3), as.factor(test_data$POST_EVENT))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 75 59\n         1 28 38\n                                          \n               Accuracy : 0.565           \n                 95% CI : (0.4933, 0.6348)\n    No Information Rate : 0.515           \n    P-Value [Acc &gt; NIR] : 0.089291        \n                                          \n                  Kappa : 0.121           \n                                          \n Mcnemar's Test P-Value : 0.001298        \n                                          \n            Sensitivity : 0.7282          \n            Specificity : 0.3918          \n         Pos Pred Value : 0.5597          \n         Neg Pred Value : 0.5758          \n             Prevalence : 0.5150          \n         Detection Rate : 0.3750          \n   Detection Prevalence : 0.6700          \n      Balanced Accuracy : 0.5600          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nRunning this model on the test data and seeing the results in the confusion matrix confirms the logistic regression model achieved an accuracy of 56.5%, which is only slightly better than chance. The model performed well in identifying non-application completers (Class 0), with a sensitivity of 72.8%, but struggled to detect application completers (Class 1), with a specificity of only 39.2%. The positive predictive value (PPV) for non-completers was moderate at 55.97%, while the negative predictive value (NPV) for completers was slightly higher at 57.58%, indicating weakness in predicting the positive class accurately.\nThe Kappa score of 0.121 suggests minimal agreement between predicted and actual classes beyond chance, and a balanced accuracy of 56.0% reflects mediocre performance across both classes.\n#SVM (support vector machine) model\n\nset.seed(321)\n\nsvmfit1 &lt;- svm(POST_EVENT ~ ., \n               data = train_data, \n               type = \"C-classification\", \n               kernel = \"radial\",\n               cost = 1,   # Cost parameter, can adjust for tuning\n               scale = TRUE # Scales data to zero mean and unit variance\n)\n\n\nsummary(svmfit1)\n\n\nCall:\nsvm(formula = POST_EVENT ~ ., data = train_data, type = \"C-classification\", \n    kernel = \"radial\", cost = 1, scale = TRUE)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  1 \n\nNumber of Support Vectors:  782\n\n ( 391 391 )\n\n\nNumber of Classes:  2 \n\nLevels: \n 0 1\n\n\nrunning on test data and reviewing performance\n\ntest_data$POST_EVENT &lt;- as.factor(test_data$POST_EVENT)\n\nsvm_pred &lt;- factor(predict(svmfit1, newdata = test_data), levels = levels(test_data$POST_EVENT))\n\n\nsvm_cm &lt;- confusionMatrix(svm_pred, test_data$POST_EVENT)\nprint(svm_cm)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 60 52\n         1 43 45\n                                          \n               Accuracy : 0.525           \n                 95% CI : (0.4534, 0.5959)\n    No Information Rate : 0.515           \n    P-Value [Acc &gt; NIR] : 0.4163          \n                                          \n                  Kappa : 0.0466          \n                                          \n Mcnemar's Test P-Value : 0.4118          \n                                          \n            Sensitivity : 0.5825          \n            Specificity : 0.4639          \n         Pos Pred Value : 0.5357          \n         Neg Pred Value : 0.5114          \n             Prevalence : 0.5150          \n         Detection Rate : 0.3000          \n   Detection Prevalence : 0.5600          \n      Balanced Accuracy : 0.5232          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nResults SVM Model 1: The SVM model achieved an accuracy of 52.5%, which is only slightly better than random guessing. It showed moderate sensitivity (58.25%) in identifying non-application completers (Class 0) and struggled with specificity (46.39%) which indicates difficulty in detecting application completers (Class 1). The 53.57% shows that the model has limited precision for predicting non-application completers, while the negative predictive value (51.14%) further shows its weak ability to predict application completions. A Kappa score of 0.0466 suggests minimal agreement between predicted and actual classes beyond chance, and the balanced accuracy of 52.32% reflects poor performance for both classes.\nOverall, this model has limited predictive power and may be in need for hyperparameter tuning or exploration of alternative approaches such as Random Forest.\nLet’s first trying tuning the SVM model to see if we can improve the performance…\n\ntune_svm &lt;- tune(\n  svm, \n  POST_EVENT ~ ., \n  data = train_data, \n  kernel = \"radial\", \n  ranges = list(\n    cost = c(0.1, 1, 10, 100),  \n    gamma = c(0.01, 0.1, 1)     \n  )\n)\n\nprint(tune_svm)\n\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n cost gamma\n   10     1\n\n- best performance: 0.2509784 \n\n\n\nsvmfit2 &lt;- svm(\n  POST_EVENT ~ ., \n  data = train_data, \n  type = \"C-classification\", \n  kernel = \"radial\", \n  cost = 10, \n  gamma = 1, \n  scale = TRUE\n)\n\nsvmfit2_pred &lt;- predict(svmfit2, newdata = test_data, type = \"class\")\n\ntest_data$POST_EVENT &lt;- as.factor(test_data$POST_EVENT)\nsvmfit2_pred &lt;- as.factor(svmfit2_pred)\n\n\nsvmfit2_cm &lt;- confusionMatrix(svmfit2_pred, test_data$POST_EVENT)\nprint(svmfit2_cm)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 103  97\n         1   0   0\n                                          \n               Accuracy : 0.515           \n                 95% CI : (0.4435, 0.5861)\n    No Information Rate : 0.515           \n    P-Value [Acc &gt; NIR] : 0.5285          \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 1.000           \n            Specificity : 0.000           \n         Pos Pred Value : 0.515           \n         Neg Pred Value :   NaN           \n             Prevalence : 0.515           \n         Detection Rate : 0.515           \n   Detection Prevalence : 1.000           \n      Balanced Accuracy : 0.500           \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nResults SVM Model 2: look way over-correcting for penalization\n#Ensemble method: Random Forest\n\n# Random Forest on original data\n#rf_model &lt;- randomForest(POST_EVENT ~ ., data = train_data, ntree = 500, mtry = 3, importance = TRUE)\n#print(rf_model)"
  },
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Data Exercise - Assignment #4",
    "section": "",
    "text": "For this assigment, I am choosing Option 1: Using a complex data (in this case a Text dataset)\nFirst, I need to install the tidytext package and in this case I’ll be using a text dataset from the janeaustenr package available in R that contains the 6 different novels written by Jane Austen. I’m using the example available in the Complex Data Types unit.\n\nlibrary(tidytext)\n\nWarning: package 'tidytext' was built under R version 4.3.3\n\nlibrary(janeaustenr)\n\nWarning: package 'janeaustenr' was built under R version 4.3.3\n\n\nI will also need the dplyr package for some of the functions it offers (such as pipes) in making data manipulation easier\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nhead(austen_books)\n\n                                                                                                            \n1 function ()                                                                                               \n2 {                                                                                                         \n3     books &lt;- list(`Sense & Sensibility` = janeaustenr::sensesensibility,                                  \n4         `Pride & Prejudice` = janeaustenr::prideprejudice, `Mansfield Park` = janeaustenr::mansfieldpark, \n5         Emma = janeaustenr::emma, `Northanger Abbey` = janeaustenr::northangerabbey,                      \n6         Persuasion = janeaustenr::persuasion)                                                             \n\n\nI’m going to create two new columns for ‘book’ and ‘line’ from the dataset by first grouping the data by book,and using ‘mutate’ to transform the grouped books into a line number within each book. Basically numbering the lines within the books. We can then ungroup the dataset as we have our lines:\n\noriginal_books &lt;- austen_books() %&gt;%\n  group_by(book) %&gt;%\n  mutate(line = row_number()) %&gt;%\n  ungroup()\n\noriginal_books\n\n# A tibble: 73,422 × 3\n   text                    book                 line\n   &lt;chr&gt;                   &lt;fct&gt;               &lt;int&gt;\n 1 \"SENSE AND SENSIBILITY\" Sense & Sensibility     1\n 2 \"\"                      Sense & Sensibility     2\n 3 \"by Jane Austen\"        Sense & Sensibility     3\n 4 \"\"                      Sense & Sensibility     4\n 5 \"(1811)\"                Sense & Sensibility     5\n 6 \"\"                      Sense & Sensibility     6\n 7 \"\"                      Sense & Sensibility     7\n 8 \"\"                      Sense & Sensibility     8\n 9 \"\"                      Sense & Sensibility     9\n10 \"CHAPTER 1\"             Sense & Sensibility    10\n# ℹ 73,412 more rows\n\n\nUsing the ‘unnest_tokens’ function from tidytext we can convert the text into “tokens” or individual words:\n\ntidy_books &lt;- original_books %&gt;%\n  unnest_tokens(word, text)\n\ntidy_books\n\n# A tibble: 725,055 × 3\n   book                 line word       \n   &lt;fct&gt;               &lt;int&gt; &lt;chr&gt;      \n 1 Sense & Sensibility     1 sense      \n 2 Sense & Sensibility     1 and        \n 3 Sense & Sensibility     1 sensibility\n 4 Sense & Sensibility     3 by         \n 5 Sense & Sensibility     3 jane       \n 6 Sense & Sensibility     3 austen     \n 7 Sense & Sensibility     5 1811       \n 8 Sense & Sensibility    10 chapter    \n 9 Sense & Sensibility    10 1          \n10 Sense & Sensibility    13 the        \n# ℹ 725,045 more rows\n\n\nI use the ‘anti_join(get_stopwords())’ functions together next to remove “stop words” from the dataset such as “the”, “in”, and “is”. For this you will have to use the “stopwords” package to identify those words.\n\nlibrary(stopwords)\n\nWarning: package 'stopwords' was built under R version 4.3.3\n\n\n\ntidy_books &lt;- tidy_books %&gt;%\n  anti_join(get_stopwords(), by = \"word\")\n\nNow to perform some exploratory analysis with the data set.\nlet’s install ‘ggplot2’ for our visualizations\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nIn this resource example, we can assign sentiment (positive, negative, and neutral) to the words used within the novels to perhaps get an overall idea of the emotion/feelings/theme within the novels - for this we need the ‘bing’ package for lexicon/sentiment:\n\nlibrary(tidyr)\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\nget_sentiments(\"bing\")\n\n# A tibble: 6,786 × 2\n   word        sentiment\n   &lt;chr&gt;       &lt;chr&gt;    \n 1 2-faces     negative \n 2 abnormal    negative \n 3 abolish     negative \n 4 abominable  negative \n 5 abominably  negative \n 6 abominate   negative \n 7 abomination negative \n 8 abort       negative \n 9 aborted     negative \n10 aborts      negative \n# ℹ 6,776 more rows\n\n\nApplying the lexicon/sentiment dataset to the text dataset using an inner join and creating new columns to represent the count of negative/positive and sentiment.\n\njaneaustensentiment &lt;- tidy_books %&gt;%\n  inner_join(get_sentiments(\"bing\"), by = \"word\", relationship = \"many-to-many\") %&gt;% \n  count(book, index = line %/% 80, sentiment) %&gt;% \n  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %&gt;% \n  mutate(sentiment = positive - negative)\n\njaneaustensentiment\n\n# A tibble: 920 × 5\n   book                index negative positive sentiment\n   &lt;fct&gt;               &lt;dbl&gt;    &lt;int&gt;    &lt;int&gt;     &lt;int&gt;\n 1 Sense & Sensibility     0       16       32        16\n 2 Sense & Sensibility     1       19       53        34\n 3 Sense & Sensibility     2       12       31        19\n 4 Sense & Sensibility     3       15       31        16\n 5 Sense & Sensibility     4       16       34        18\n 6 Sense & Sensibility     5       16       51        35\n 7 Sense & Sensibility     6       24       40        16\n 8 Sense & Sensibility     7       23       51        28\n 9 Sense & Sensibility     8       30       40        10\n10 Sense & Sensibility     9       15       19         4\n# ℹ 910 more rows\n\n\nWe can now graph our sentiment count results using ggplot:\n\nggplot(janeaustensentiment, aes(index, sentiment, fill = book)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(vars(book), ncol = 2, scales = \"free_x\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My website and data analysis portfolio",
    "section": "",
    "text": "Greetings, friend! Welcome to my little corner of the web.\n\nI am Holly Milazzo, and I’m delighted your journey has led you here.\nPlease, feel free to explore my website and data analysis portfolio.\n\nUse the Menu Bar above to look around.\nI’ve gathered many stories and wonders along my travels.\n\nMay your visit be filled with joy and discovery, and may you always find what you seek!"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html",
    "href": "presentation-exercise/presentation-exercise.html",
    "title": "Presentation Exercise",
    "section": "",
    "text": "In this exercise, I recreated the bar graph that illustrates the share of political donations from team owners in six major sports leagues (NFL, NBA, WNBA, NHL, MLB, and NASCAR) to the Republican and Democratic parties over three election years: 2016, 2018, and 2020 from the FiveThirtyEight page here Link. The graph shows a significant majority of donations going to the Republican party in all three years. The annotation highlights that Giants owner Charles Johnson’s contributions constitute a substantial portion of the total Republican donations, underscoring the influence of individual donors in political contributions.\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\n\n# Important\n# Define the data location\ndata_location_sports &lt;- here::here(\"presentation-exercise\", \"sports-political-donations.csv\")\n\n# Read the CSV data\nrawdata_sports &lt;- read_csv(data_location_sports, col_names = FALSE)\n\nRows: 2799 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): X1, X2, X3, X4, X5, X6, X7\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Preview the data\nhead(rawdata_sports)\n\n# A tibble: 6 × 7\n  X1          X2           X3     X4                           X5    X6    X7   \n  &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt;                        &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 Owner       Team         League Recipient                    Amou… Elec… Party\n2 Adam Silver Commissioner NBA    WRIGHT 2016                  $4,0… 2016  Demo…\n3 Adam Silver Commissioner NBA    BIDEN FOR PRESIDENT          $2,8… 2020  Demo…\n4 Adam Silver Commissioner NBA    CORY 2020                    $2,7… 2020  Demo…\n5 Adam Silver Commissioner NBA    Kamala Harris for the People $2,7… 2020  Demo…\n6 Adam Silver Commissioner NBA    Win The Era PAC              $2,7… 2020  Demo…\n\n\n\n# Assign proper column names\ncolnames(rawdata_sports) &lt;- c(\"Owner\", \"Team\", \"League\", \"Recipient\", \"Amount\", \"Election_Year\", \"Party\")\n\n# Clean up the Amount column (remove '$' and convert to numeric)\nrawdata_sports$Amount &lt;- as.numeric(gsub(\"[\\\\$,]\", \"\", rawdata_sports$Amount))\n\nWarning: NAs introduced by coercion\n\n# Filter out rows where Party is not 'Democrat' or 'Republican'\nrawdata_sports &lt;- rawdata_sports[rawdata_sports$Party %in% c(\"Democrat\", \"Republican\"), ]\n\n# Summarize data to calculate total donations by Party and Year\ndonations_summary &lt;- aggregate(rawdata_sports$Amount, \n                               by = list(Election_Year = rawdata_sports$Election_Year, Party = rawdata_sports$Party), \n                               FUN = sum)\ncolnames(donations_summary)[3] &lt;- \"Total_Amount\"\n\n# Convert Party to factor and specify order (Democrat first for better visualization)\ndonations_summary$Party &lt;- factor(donations_summary$Party, levels = c(\"Democrat\", \"Republican\"))\n\n# Calculate the proportion of total donations by year\ntotal_by_year &lt;- aggregate(donations_summary$Total_Amount, \n                           by = list(Election_Year = donations_summary$Election_Year), \n                           FUN = sum)\ncolnames(total_by_year)[2] &lt;- \"Yearly_Total\"\n\ndonations_summary &lt;- merge(donations_summary, total_by_year, by = \"Election_Year\")\ndonations_summary$Prop_Amount &lt;- donations_summary$Total_Amount / donations_summary$Yearly_Total\n\n# Plot the data\nggplot(donations_summary, aes(x = as.factor(Election_Year), y = Prop_Amount, fill = Party)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  scale_y_continuous(labels = scales::percent) +\n  scale_fill_manual(values = c(\"Democrat\" = \"blue\", \"Republican\" = \"red\")) +\n  labs(\n    title = \"Team owners give largely to the GOP\",\n    subtitle = \"Share of donations from team owners in six leagues, per year and party\",\n    x = \"\",\n    y = \"Share of Donations\",\n    fill = \"Donations to\"\n  ) +\n  theme_minimal() +\n  theme(\n    panel.background = element_rect(fill = \"gray90\"),\n    plot.background = element_rect(fill = \"gray90\"),\n    axis.text.x = element_text(angle = 0, hjust = 0.5),\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  ) +\n  annotate(\"text\", x = 1.5, y = 1.05, label = \"Giants owner Charles Johnson’s total contributions make up 32.1% of all Republican contributions.\", size = 3, hjust = 0.3, vjust = 0)\n\n\n\n\nTable: Partisan Contributions by League\nThe table breaks down the total political contributions from owners and commissioners in six sports leagues (MLB, NBA, NHL, NFL, WNBA, and NASCAR) to the Republican and Democratic parties from 2016 to 2020. It reveals that MLB owners have donated the most overall, with significant amounts going to both parties, but predominantly to Republicans. The table provides a clear comparison of partisan contributions across different leagues, highlighting the disparity in political support within the sports industry.\nBoth visuals emphasize the substantial financial support sports team owners provide to political parties, predominantly favoring the Republican party, and the considerable influence of a few key donors.\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(ggtext)\n\n# Define the data for the table\ndata_table &lt;- data.frame(\n  League = c(\"MLB\", \"NBA\", \"NHL\", \"NFL\", \"WNBA\", \"NASCAR\"),\n  To_Republicans = c(\"$15,181,761\", \"$8,372,300\", \"$7,087,116\", \"$5,032,470\", \"$1,338,459\", \"$576,110\"),\n  To_Democrats = c(\"$5,184,604\", \"$2,641,487\", \"$1,726,733\", \"$873,500\", \"$1,634,153\", \"$93,983\"),\n  Total = c(\"$20,366,365\", \"$11,013,787\", \"$8,813,849\", \"$5,905,970\", \"$2,972,612\", \"$670,093\")\n)\n\n# Create a base plot\np &lt;- ggplot(data_table, aes(x = 1, y = League)) +\n  geom_text(aes(label = To_Republicans, x = 2), hjust = 0, color = \"red\") +\n  geom_text(aes(label = To_Democrats, x = 3), hjust = 0, color = \"blue\") +\n  geom_text(aes(label = Total, x = 4), hjust = 0) +\n  geom_text(aes(label = League, x = 0), hjust = 0, fontface = \"bold\") +\n  scale_x_continuous(limits = c(-0.5, 4.5), breaks = 0:4, labels = c(\"LEAGUE\", \"TO REPUBLICANS\", \"TO DEMOCRATS\", \"TOTAL\", \"\")) +\n  theme_void() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5),\n    plot.subtitle = element_text(size = 10, hjust = 0.5),\n    plot.caption = element_text(size = 8, hjust = 0.5),\n    axis.text.x = element_text(size = 10, face = \"bold\"),\n    plot.background = element_rect(fill = \"white\", color = \"white\"),\n    panel.background = element_rect(fill = \"white\", color = \"white\")\n  ) +\n  labs(\n    title = \"MLB owners have donated the most\",\n    subtitle = \"Specifically partisan contributions from owners and commissioners in the NFL, NBA, WNBA, NHL, MLB and NASCAR, by party, 2016-20\",\n    caption = \"SOURCE: FEDERAL ELECTION COMMISSION, OPENSECRETS\"\n  )\n\n# Print the plot\nprint(p)"
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "Holly Milazzo Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at C:/Users/holly/OneDrive/Desktop/Data_Repository/Practicum II/HollyMilazzo-P2-portfolio\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.3.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          3     \n_______________________          \nColumn type frequency:           \n  factor                   1     \n  numeric                  2     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "Holly Milazzo Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\nlibrary(dplyr) #for data processing/cleaning\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\nlibrary(skimr) #for nice visualization of data \n\nWarning: package 'skimr' was built under R version 4.3.3\n\nlibrary(here) #to set paths\n\nhere() starts at C:/Users/holly/OneDrive/Desktop/Data_Repository/Practicum II/HollyMilazzo-P2-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\nhead(rawdata)\n\n# A tibble: 6 × 12\n  show_id type    title  director cast  country date_added          release_year\n  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   &lt;dttm&gt;                     &lt;dbl&gt;\n1 s1      Movie   Dick … Kirsten… &lt;NA&gt;  United… 2021-09-25 00:00:00         2020\n2 s2      TV Show Blood… &lt;NA&gt;     Ama … South … 2021-09-24 00:00:00         2021\n3 s3      TV Show Gangl… Julien … Sami… &lt;NA&gt;    2021-09-24 00:00:00         2021\n4 s4      TV Show Jailb… &lt;NA&gt;     &lt;NA&gt;  &lt;NA&gt;    2021-09-24 00:00:00         2021\n5 s5      TV Show Kota … &lt;NA&gt;     Mayu… India   2021-09-24 00:00:00         2021\n6 s6      TV Show Midni… Mike Fl… Kate… &lt;NA&gt;    2021-09-24 00:00:00         2021\n# ℹ 4 more variables: rating &lt;chr&gt;, duration &lt;chr&gt;, listed_in &lt;chr&gt;,\n#   description &lt;chr&gt;\n\n\n\n\nCheck data\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 8,811\nColumns: 12\n$ show_id      &lt;chr&gt; \"s1\", \"s2\", \"s3\", \"s4\", \"s5\", \"s6\", \"s7\", \"s8\", \"s9\", \"s1…\n$ type         &lt;chr&gt; \"Movie\", \"TV Show\", \"TV Show\", \"TV Show\", \"TV Show\", \"TV …\n$ title        &lt;chr&gt; \"Dick Johnson Is Dead\", \"Blood & Water\", \"Ganglands\", \"Ja…\n$ director     &lt;chr&gt; \"Kirsten Johnson\", NA, \"Julien Leclercq\", NA, NA, \"Mike F…\n$ cast         &lt;chr&gt; NA, \"Ama Qamata, Khosi Ngema, Gail Mabalane, Thabang Mola…\n$ country      &lt;chr&gt; \"United States\", \"South Africa\", NA, NA, \"India\", NA, NA,…\n$ date_added   &lt;dttm&gt; 2021-09-25, 2021-09-24, 2021-09-24, 2021-09-24, 2021-09-…\n$ release_year &lt;dbl&gt; 2020, 2021, 2021, 2021, 2021, 2021, 2021, 1993, 2021, 202…\n$ rating       &lt;chr&gt; \"PG-13\", \"TV-MA\", \"TV-MA\", \"TV-MA\", \"TV-MA\", \"TV-MA\", \"PG…\n$ duration     &lt;chr&gt; \"90 min\", \"2 Seasons\", \"1 Season\", \"1 Season\", \"2 Seasons…\n$ listed_in    &lt;chr&gt; \"Documentaries\", \"International TV Shows, TV Dramas, TV M…\n$ description  &lt;chr&gt; \"As her father nears the end of his life, filmmaker Kirst…\n\nsummary(rawdata)\n\n   show_id              type              title             director        \n Length:8811        Length:8811        Length:8811        Length:8811       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n     cast             country            date_added                    \n Length:8811        Length:8811        Min.   :2008-01-01 00:00:00.00  \n Class :character   Class :character   1st Qu.:2018-04-06 00:00:00.00  \n Mode  :character   Mode  :character   Median :2019-07-02 00:00:00.00  \n                                       Mean   :2019-05-17 17:50:35.32  \n                                       3rd Qu.:2020-08-19 18:00:00.00  \n                                       Max.   :2024-04-05 00:00:00.00  \n                                       NA's   :13                      \n  release_year     rating            duration          listed_in        \n Min.   :1925   Length:8811        Length:8811        Length:8811       \n 1st Qu.:2013   Class :character   Class :character   Class :character  \n Median :2017   Mode  :character   Mode  :character   Mode  :character  \n Mean   :2014                                                           \n 3rd Qu.:2019                                                           \n Max.   :2024                                                           \n NA's   :3                                                              \n description       \n Length:8811       \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nhead(rawdata)\n\n# A tibble: 6 × 12\n  show_id type    title  director cast  country date_added          release_year\n  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   &lt;dttm&gt;                     &lt;dbl&gt;\n1 s1      Movie   Dick … Kirsten… &lt;NA&gt;  United… 2021-09-25 00:00:00         2020\n2 s2      TV Show Blood… &lt;NA&gt;     Ama … South … 2021-09-24 00:00:00         2021\n3 s3      TV Show Gangl… Julien … Sami… &lt;NA&gt;    2021-09-24 00:00:00         2021\n4 s4      TV Show Jailb… &lt;NA&gt;     &lt;NA&gt;  &lt;NA&gt;    2021-09-24 00:00:00         2021\n5 s5      TV Show Kota … &lt;NA&gt;     Mayu… India   2021-09-24 00:00:00         2021\n6 s6      TV Show Midni… Mike Fl… Kate… &lt;NA&gt;    2021-09-24 00:00:00         2021\n# ℹ 4 more variables: rating &lt;chr&gt;, duration &lt;chr&gt;, listed_in &lt;chr&gt;,\n#   description &lt;chr&gt;\n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n8811\n\n\nNumber of columns\n12\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n10\n\n\nnumeric\n1\n\n\nPOSIXct\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nshow_id\n0\n1.00\n2\n19\n0\n8811\n0\n\n\ntype\n1\n1.00\n5\n13\n0\n3\n0\n\n\ntitle\n2\n1.00\n1\n104\n0\n8806\n0\n\n\ndirector\n2636\n0.70\n2\n208\n0\n4529\n0\n\n\ncast\n826\n0.91\n3\n771\n0\n7695\n0\n\n\ncountry\n833\n0.91\n4\n123\n0\n749\n0\n\n\nrating\n6\n1.00\n1\n29\n0\n19\n0\n\n\nduration\n5\n1.00\n5\n146\n0\n221\n0\n\n\nlisted_in\n3\n1.00\n6\n79\n0\n516\n0\n\n\ndescription\n3\n1.00\n61\n248\n0\n8776\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nrelease_year\n3\n1\n2014.19\n8.79\n1925\n2013\n2017\n2019\n2024\n▁▁▁▁▇\n\n\n\nVariable type: POSIXct\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\ndate_added\n13\n1\n2008-01-01\n2024-04-05\n2019-07-02\n1715\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\n# Handling missing values\nrawdata$director[is.na(rawdata$director)] &lt;- \"Unknown\"\n\n# Converting date formats\nrawdata$date_added &lt;- as.Date(rawdata$date_added, format = \"%m/%d/%Y\")\n\n# Standardizing categorical variables\nrawdata$type &lt;- as.factor(rawdata$type)\n\n# Convert 'type' back to factor after mutation\nrawdata$type &lt;- as.factor(rawdata$type)\n\n# Display the cleaned data\nhead(rawdata)\n\n# A tibble: 6 × 12\n  show_id type    title    director cast  country date_added release_year rating\n  &lt;chr&gt;   &lt;fct&gt;   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   &lt;date&gt;            &lt;dbl&gt; &lt;chr&gt; \n1 s1      Movie   Dick Jo… Kirsten… &lt;NA&gt;  United… 2021-09-25         2020 PG-13 \n2 s2      TV Show Blood &… Unknown  Ama … South … 2021-09-24         2021 TV-MA \n3 s3      TV Show Ganglan… Julien … Sami… &lt;NA&gt;    2021-09-24         2021 TV-MA \n4 s4      TV Show Jailbir… Unknown  &lt;NA&gt;  &lt;NA&gt;    2021-09-24         2021 TV-MA \n5 s5      TV Show Kota Fa… Unknown  Mayu… India   2021-09-24         2021 TV-MA \n6 s6      TV Show Midnigh… Mike Fl… Kate… &lt;NA&gt;    2021-09-24         2021 TV-MA \n# ℹ 3 more variables: duration &lt;chr&gt;, listed_in &lt;chr&gt;, description &lt;chr&gt;\n\n\n\n# Remove rows where 'type' is \"William Wyler\" or NA\ncleaned_data &lt;- rawdata %&gt;%\n  filter(type != \"William Wyler\" & type != \"Unknown\" & !is.na(type))\n\nhead(cleaned_data)\n\n# A tibble: 6 × 12\n  show_id type    title    director cast  country date_added release_year rating\n  &lt;chr&gt;   &lt;fct&gt;   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   &lt;date&gt;            &lt;dbl&gt; &lt;chr&gt; \n1 s1      Movie   Dick Jo… Kirsten… &lt;NA&gt;  United… 2021-09-25         2020 PG-13 \n2 s2      TV Show Blood &… Unknown  Ama … South … 2021-09-24         2021 TV-MA \n3 s3      TV Show Ganglan… Julien … Sami… &lt;NA&gt;    2021-09-24         2021 TV-MA \n4 s4      TV Show Jailbir… Unknown  &lt;NA&gt;  &lt;NA&gt;    2021-09-24         2021 TV-MA \n5 s5      TV Show Kota Fa… Unknown  Mayu… India   2021-09-24         2021 TV-MA \n6 s6      TV Show Midnigh… Mike Fl… Kate… &lt;NA&gt;    2021-09-24         2021 TV-MA \n# ℹ 3 more variables: duration &lt;chr&gt;, listed_in &lt;chr&gt;, description &lt;chr&gt;\n\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"cleaned_data.rds\")\nsaveRDS(cleaned_data, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "Holly Milazzo Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "Holly Milazzo Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "Holly Milazzo Data Analysis Portfolio",
    "section": "",
    "text": "This project uses data-set on Netflix Movies and TV shows from https://www.kaggle.com/datasets/rahulvyasm/netflix-movies-and-tv-shows.\nI first downloaded the netflix_titles.csv dataset and converted it to an xlsx format by opening a new excel file &gt; Click on Data tab &gt; click From Text/CSV &gt; Popup window will open for you to select the netflix_titles.csv file &gt; Select Import &gt; Click Load &gt; Save file\nThis dataset contains 8, 809 records and the following 12 variables:\n\nshow_id: A unique identifier for each title.\ntype: The category of the title, which is either ‘Movie’ or ‘TV Show’\ntitle: The name of the movie or TV show\ndirector: The director(s) of the movie or TV show (Contains null values for some entries, especially TV shows where this information might not be applicable)\ncast: The list of main actors/actresses in the title (Some entries might not have this information.)\ncountry: The country or countries where the movie or TV show was produced.\ndate_added: The date the title was added to Netflix.\nrelease_year: The year the movie or TV show was originally released.\nrating: The age rating of the title.\nduration: The duration of the title, in minutes for movies and seasons for TV shows\nlisted_in: The genres the title falls under.\ndescription: A brief summary of the title.\n\nYou will need to load the following packages in R: library(readxl) library(ggplot2) library(dplyr) library(tidyr) library(skimr)\nlibrary(here)\nNext, to clean the data prior to analysis you will need to…\nHandle the missing values in the ‘director’ variable: &gt;rawdata\\(director[is.na(rawdata\\)director)] &lt;- “Unknown”\nConvert the ‘date_added’ variable into an actual date format: &gt;rawdata\\(date_added &lt;- as.Date(rawdata\\)date_added, format = “%m/%d/%Y”)\nRemove rows where ‘type’ is “William Wyler” or NA and create a ‘cleaned_data’ subset: &gt;cleaned_data &lt;- rawdata %&gt;% &gt;&gt; filter(type != “William Wyler” & type != “Unknown” & !is.na(type))\nand then convert the ‘type’ variable into a factor: &gt;rawdata\\(type &lt;- as.factor(rawdata\\)type)\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  },
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "Holly Milazzo Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "Holly Milazzo Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "This uses MS Word as output format. See here for more information. You can switch #to other formats, like html or pdf. See the Quarto documentation for other formats.*/\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "1.1 General Background Information",
    "text": "1.1 General Background Information\nThe intent of this analysis is to provide insights into how regional production practices and content types align with age rating distributions, offering valuable information for Netflix’s content acquisition and compliance strategies. This research highlights the importance of understanding content rating trends to better cater to diverse audiences and ensure appropriate content delivery."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "1.2 Description of data and data source",
    "text": "1.2 Description of data and data source\nData is on Netflix Movies and TV Shows from Kaggle.com site. The description says: “The Netflix Titles dataset is a comprehensive compilation of movies and TV shows available on Netflix, covering various aspects such as the title type, director, cast, country of production, release year, rating, duration, genres (listed in), and a brief description. This dataset is instrumental for analyzing trends in Netflix content, understanding genre popularity, and examining the distribution of content across different regions and time periods”\nThe dataset contains 8,809 observations and the following 12 variables:\n\nshow_id: A unique identifier for each title.\ntype: The category of the title, which is either ‘Movie’ or ‘TV Show’\ntitle: The name of the movie or TV show\ndirector: The director(s) of the movie or TV show (Contains null values for some entries, especially TV shows where this information might not be applicable)\ncast: The list of main actors/actresses in the title (Some entries might not have this information.)\ncountry: The country or countries where the movie or TV show was produced.\ndate_added: The date the title was added to Netflix.\nrelease_year: The year the movie or TV show was originally released.\nrating: The age rating of the title.\nduration: The duration of the title, in minutes for movies and seasons for TV shows\nlisted_in: The genres the title falls under.\ndescription: A brief summary of the title."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "1.3 Questions/Hypotheses to be addressed",
    "text": "1.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\n“How do the type of content (Movie or TV Show) and the country of origin affect the distribution of age ratings on Netflix titles?”\nThis question focuses on understanding the relationship between content type, country of origin, and age ratings, which can provide valuable insights into regional production practices and content rating trends on Netflix\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the #bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. #Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-acquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-acquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.1 Data Acquisition",
    "text": "2.1 Data Acquisition\nI imported the data for Netflix Movies and TV Shows which was available on Kaggle.com site. My raw data file is available through file path folders: starter-analysis-exercise &gt; data &gt; raw-data &gt; netflix_titles.xlsx"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.2 Data Import and Cleaning",
    "text": "2.2 Data Import and Cleaning\nThe file path to my code file for cleaning my dataset is: starter-analysis-exercise &gt; code &gt; processing-code &gt; processingfile\nFirst I imported the data…\n\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\nhere are some of the initial cleaning techniques and a few reasons why I chose to do them:\n\nFor the country column, I filled missing values with the mode (most frequently occurring country).\nConverted the date_added to a Date format as a crucial step for any potential time series or date-related analysis.\nConverted type to a factor since I am planning on performing statistical tests and/or modeling that may need categorical input features.\n\n\n# Handling missing values\n\nrawdata$director[is.na(rawdata$director)] &lt;- \"Unknown\"\n\n# Fill missing 'country' values with the mode (most frequent value)\nmode_country &lt;- names(sort(table(rawdata$country), decreasing = TRUE))[1]\nrawdata$country[is.na(rawdata$country)] &lt;- mode_country\n\n# Safe conversion of date formats with error handling\nrawdata$date_added &lt;- as.Date(rawdata$date_added, format = \"%m/%d/%Y\")\nif(any(is.na(rawdata$date_added))) {\n  warning(\"There were errors in date conversion. Check date formats.\")\n}\n\nWarning: There were errors in date conversion. Check date formats.\n\n# Standardizing categorical variables\n\nrawdata$type &lt;- as.factor(rawdata$type)\n\n# Display the cleaned data\nhead(rawdata)\n\n# A tibble: 6 × 12\n  show_id type    title    director cast  country date_added release_year rating\n  &lt;chr&gt;   &lt;fct&gt;   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;   &lt;date&gt;            &lt;dbl&gt; &lt;chr&gt; \n1 s1      Movie   Dick Jo… Kirsten… &lt;NA&gt;  United… 2021-09-25         2020 PG-13 \n2 s2      TV Show Blood &… Unknown  Ama … South … 2021-09-24         2021 TV-MA \n3 s3      TV Show Ganglan… Julien … Sami… United… 2021-09-24         2021 TV-MA \n4 s4      TV Show Jailbir… Unknown  &lt;NA&gt;  United… 2021-09-24         2021 TV-MA \n5 s5      TV Show Kota Fa… Unknown  Mayu… India   2021-09-24         2021 TV-MA \n6 s6      TV Show Midnigh… Mike Fl… Kate… United… 2021-09-24         2021 TV-MA \n# ℹ 3 more variables: duration &lt;chr&gt;, listed_in &lt;chr&gt;, description &lt;chr&gt;\n\n\nI also needed to do some clean up when it came to content ‘type’ as it included unwanted values…\n\n# Remove rows where 'type' is \"William Wyler\" or NA\ncleaned_data &lt;- rawdata[rawdata$type != \"William Wyler\" & rawdata$type != \"Unknown\" & !is.na(rawdata$type), ]\n\n\nsummary(cleaned_data)\n\n   show_id                     type         title             director        \n Length:8809        Movie        :6132   Length:8809        Length:8809       \n Class :character   TV Show      :2677   Class :character   Class :character  \n Mode  :character   William Wyler:   0   Mode  :character   Mode  :character  \n                                                                              \n                                                                              \n                                                                              \n                                                                              \n     cast             country            date_added          release_year \n Length:8809        Length:8809        Min.   :2008-01-01   Min.   :1925  \n Class :character   Class :character   1st Qu.:2018-04-06   1st Qu.:2013  \n Mode  :character   Mode  :character   Median :2019-07-02   Median :2017  \n                                       Mean   :2019-05-17   Mean   :2014  \n                                       3rd Qu.:2020-08-19   3rd Qu.:2019  \n                                       Max.   :2024-04-05   Max.   :2024  \n                                       NA's   :11           NA's   :1     \n    rating            duration          listed_in         description       \n Length:8809        Length:8809        Length:8809        Length:8809       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.3 Statistical Analysis",
    "text": "2.3 Statistical Analysis\nExplain anything related to your statistical analyses.\nThe relevant variables I’ll be using during my statistical analysis to determine how regional production practices and content types align with age rating distributions will be: Country, Type, and Rating.\nLet’s double check if there is any other missing data in my cleaned_data before I perform my analysis… nothing significant in 3 variables I’ll be using.\n\n# Calculate the number of missing values for each column in cleaned_data\nmissing_data_summary &lt;- sapply(cleaned_data, function(x) sum(is.na(x)))\n\n# Print the summary of missing data\nprint(missing_data_summary)\n\n     show_id         type        title     director         cast      country \n           0            0            0            0          825            0 \n  date_added release_year       rating     duration    listed_in  description \n          11            1            5            4            1            1 \n\n\nI also want to check for any outliers as well…\n\n# Create a boxplot for each numeric variable in the dataframe\nnumeric_vars &lt;- sapply(cleaned_data, is.numeric)\nif(any(numeric_vars)) {\n  # Filter only numeric columns\n  numeric_data &lt;- cleaned_data[, numeric_vars]\n\n  # Melt the data for easy plotting with ggplot2\n  library(reshape2)\n  long_data &lt;- melt(numeric_data)\n\n  # Plot\n  ggplot(long_data, aes(x = variable, y = value)) +\n    geom_boxplot(outlier.colour = \"red\", outlier.shape = 1) +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n    labs(title = \"Boxplot for Each Numeric Variable\", x = \"Variables\", y = \"Values\")\n} else {\n  print(\"No numeric variables found for plotting.\")\n}\n\nWarning: package 'reshape2' was built under R version 4.3.3\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_boxplot()`).\n\n\n\n\n\nGiven my hypothesis, which aims to explore how the type of content (Movie or TV Show) and the country of origin influence the distribution of age ratings on Netflix titles, regression testing does not seem like the best testing method because I using 2 categorical variables (Type and Rating) as target variables.My ‘rating’ variable is not ordinal either which means we’d also leave out performing logistic regression.\nI believe the best testing methods in this case are either Chi-square (to test independence) or Multinomial Logistic Regression. MLR would be useful since the ‘Rating’ has multiple categories without order and would allow me to model the probability of each rating category as a function of ‘Type’ and ‘Country’.\nI will need install the following packages for the next part of my analysis\n\nlibrary(nnet)\nlibrary(forcats)\n\nInitially, when I ran the model it gave an error due to the complexity in the number of parameters it created based on the variations of categories I have in my variables.\nChatGPT recommended I use the code below to create a decay term for regularization, which helps to manage the complexity of the model by shrinking the regression coefficients.\nWith the convergence of my multinomial logistic regression as indicated by “converged” in the output below, the next steps involve interpreting the model’s results and using them to validate my hypothesis or make further decisions.\n\n# Assuming 'Country' has many categories, we reduce them\ncleaned_data$country &lt;- fct_lump_n(cleaned_data$country, n = 10)  # Keeps the top 10 countries, others lumped into \"Other\"\ncleaned_data$country &lt;- factor(cleaned_data$country)\n\n# Fit the model with increased decay for regularization\n\nfit &lt;- multinom(type ~ country + rating, data = cleaned_data, MaxNWts = 10000, decay = 0.1)\n\nWarning in multinom(type ~ country + rating, data = cleaned_data, MaxNWts =\n10000, : group 'William Wyler' is empty\n\n\n# weights:  29 (28 variable)\ninitial  value 6102.467778 \niter  10 value 4551.137216\niter  20 value 4300.674269\niter  30 value 4276.943332\nfinal  value 4276.909270 \nconverged\n\n\nNow to run my multinomial model….\n\ncleaned_data$country &lt;- as.factor(cleaned_data$country)\ncleaned_data$type &lt;- as.factor(cleaned_data$type)\ncleaned_data$rating &lt;- as.factor(cleaned_data$rating)\n\n\nmultinom_model &lt;- multinom(rating ~ country + type, data = cleaned_data)\n\n# weights:  252 (221 variable)\ninitial  value 25446.832957 \niter  10 value 18728.030041\niter  20 value 16906.955079\niter  30 value 15526.132442\niter  40 value 14980.668736\niter  50 value 14724.154902\niter  60 value 14656.735665\niter  70 value 14624.490886\niter  80 value 14615.654822\niter  90 value 14612.398803\niter 100 value 14611.228013\nfinal  value 14611.228013 \nstopped after 100 iterations\n\n# Summary of the model\nhead(multinom_model)\n\n$n\n[1] 13  0 18\n\n$nunits\n[1] 32\n\n$nconn\n [1]   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  14  28  42  56\n[20]  70  84  98 112 126 140 154 168 182 196 210 224 238 252\n\n$conn\n  [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13  0  1  2  3  4  5  6  7  8  9 10\n [26] 11 12 13  0  1  2  3  4  5  6  7  8  9 10 11 12 13  0  1  2  3  4  5  6  7\n [51]  8  9 10 11 12 13  0  1  2  3  4  5  6  7  8  9 10 11 12 13  0  1  2  3  4\n [76]  5  6  7  8  9 10 11 12 13  0  1  2  3  4  5  6  7  8  9 10 11 12 13  0  1\n[101]  2  3  4  5  6  7  8  9 10 11 12 13  0  1  2  3  4  5  6  7  8  9 10 11 12\n[126] 13  0  1  2  3  4  5  6  7  8  9 10 11 12 13  0  1  2  3  4  5  6  7  8  9\n[151] 10 11 12 13  0  1  2  3  4  5  6  7  8  9 10 11 12 13  0  1  2  3  4  5  6\n[176]  7  8  9 10 11 12 13  0  1  2  3  4  5  6  7  8  9 10 11 12 13  0  1  2  3\n[201]  4  5  6  7  8  9 10 11 12 13  0  1  2  3  4  5  6  7  8  9 10 11 12 13  0\n[226]  1  2  3  4  5  6  7  8  9 10 11 12 13  0  1  2  3  4  5  6  7  8  9 10 11\n[251] 12 13\n\n$nsunits\n[1] 14\n\n$decay\n[1] 0\n\n\n\nprobabilities &lt;- predict(multinom_model, type = \"probs\")\n\nhead(probabilities)\n\n        66 min       74 min       84 min            A            G        NC-17\n1 2.197696e-04 3.977743e-04 3.977743e-04 3.977743e-04 1.149018e-02 3.866288e-04\n2 1.046572e-07 2.200441e-08 2.200441e-08 2.200441e-08 1.654515e-06 1.734750e-06\n3 1.258220e-05 5.534332e-06 5.534332e-06 5.534332e-06 3.762838e-06 1.485806e-06\n4 1.258220e-05 5.534332e-06 5.534332e-06 5.534332e-06 3.762838e-06 1.485806e-06\n5 4.468941e-08 6.704574e-10 6.704574e-10 6.704574e-10 2.364318e-10 2.754968e-10\n6 1.258220e-05 5.534332e-06 5.534332e-06 5.534332e-06 3.762838e-06 1.485806e-06\n            NR           PG        PG-13            R     TV-14       TV-G\n1 0.0131985545 6.803521e-02 1.189387e-01 1.802553e-01 0.1459130 0.02467179\n2 0.0016490449 1.866344e-06 4.479769e-06 8.648112e-04 0.2888059 0.02839096\n3 0.0018656069 2.736761e-06 6.625572e-06 1.151475e-03 0.2363820 0.04348793\n4 0.0018656069 2.736761e-06 6.625572e-06 1.151475e-03 0.2363820 0.04348793\n5 0.0004001956 5.796439e-08 1.573624e-07 4.749477e-06 0.5875946 0.01035897\n6 0.0018656069 2.736761e-06 6.625572e-06 1.151475e-03 0.2363820 0.04348793\n      TV-MA     TV-PG        TV-Y      TV-Y7     TV-Y7-FV           UR\n1 0.2964644 0.0793353 0.027272531 0.03160530 0.0006127349 4.072383e-04\n2 0.4582647 0.1051285 0.059006494 0.05739596 0.0004821809 1.587896e-06\n3 0.4033705 0.1232277 0.085883178 0.10420632 0.0003801792 1.367047e-06\n4 0.4033705 0.1232277 0.085883178 0.10420632 0.0003801792 1.367047e-06\n5 0.2257690 0.1376917 0.009731089 0.02802596 0.0004235554 1.988237e-10\n6 0.4033705 0.1232277 0.085883178 0.10420632 0.0003801792 1.367047e-06"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 Exploratory/Descriptive Analysis",
    "text": "3.1 Exploratory/Descriptive Analysis\nDistribution between Movies and TV Shows:\n\n# Perform the group_by and summarise operations\ntype_distribution &lt;- aggregate(. ~ type, data = cleaned_data, FUN = length)\nnames(type_distribution)[2] &lt;- \"count\"\n\n# Plot the distribution of content types\nbarplot(height = type_distribution$count,\n        names.arg = type_distribution$type,\n        col = c(\"purple\", \"orange\"),\n        main = \"Distribution of Content Types - Movies v TV Shows\",\n        xlab = \"Type\",\n        ylab = \"Count\",\n        las = 1) # las = 1 makes axis labels horizontal\n\n\n\n\nWe see from the distribution that it appears movies are being streamed substantially more than TV shows, but to get a better sense of this let’s represent it as a percentage instead\n\ntype_distribution &lt;- aggregate(. ~ type, data = cleaned_data, FUN = length)\nnames(type_distribution)[2] &lt;- \"count\"\n\ntype_distribution$percentage &lt;- round((type_distribution$count / sum(type_distribution$count)) * 100, 1)\n\nlabels &lt;- paste(type_distribution$type, type_distribution$percentage, \"%\")\n\npie(type_distribution$count,\n    labels = labels,\n    col = c(\"purple\", \"orange\"),\n    main = \"Percentage of Movies vs TV Shows\")\n\n\n\n\nNow I’d like to see which countries the Movie/TV show content originate from\n\nlibrary(ggplot2)\n\ncountry_counts &lt;- head(sort(table(cleaned_data$country), decreasing = TRUE), 10)\n\n# Convert to data frame for ggplot\ncountry_df &lt;- data.frame(\n  country = names(country_counts),\n  count = as.numeric(country_counts)\n)\n\n# Create the bar chart\np &lt;- ggplot(country_df, aes(x = reorder(country, -count), y = count)) +\n  geom_bar(stat = \"identity\", fill = \"purple\") +\n  geom_text(aes(label = count), vjust = -0.3) +\n  labs(x = \"Country\", y = \"Count\", title = \"Top 10 Countries (Top 3 Highlighted)\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  geom_bar(data = country_df[1:3, ], aes(x = country, y = count), stat = \"identity\", fill = \"orange\")\n\nprint(p)\n\n\n\n\nLet’s also explore what the content consumption is like between countries…\n\n# Count occurrences of each country and type\ncount_data &lt;- count(cleaned_data, country, type)\n\n# Group by country\ngrouped_data &lt;- group_by(count_data, country)\n\n# Calculate percentage within each group\ngrouped_data &lt;- mutate(grouped_data, percentage = n / sum(n) * 100)\n\n# Ungroup the data\npercentage_data &lt;- ungroup(grouped_data)\n\n# Show percentage_data\npercentage_data\n\n# A tibble: 22 × 4\n   country type        n percentage\n   &lt;fct&gt;   &lt;fct&gt;   &lt;int&gt;      &lt;dbl&gt;\n 1 Canada  Movie     122      67.4 \n 2 Canada  TV Show    59      32.6 \n 3 Egypt   Movie      92      86.8 \n 4 Egypt   TV Show    14      13.2 \n 5 France  Movie      75      60.5 \n 6 France  TV Show    49      39.5 \n 7 India   Movie     893      91.9 \n 8 India   TV Show    79       8.13\n 9 Japan   Movie      76      31.0 \n10 Japan   TV Show   169      69.0 \n# ℹ 12 more rows\n\n\n\n# Calculate total count of each country\ncountry_totals &lt;- aggregate(percentage_data$n, by = list(percentage_data$country), FUN = sum)\n\n# Select top 10 countries by total count\ntop_10_countries &lt;- country_totals[order(country_totals$x, decreasing = TRUE), ]$Group.1[1:10]\n\n# Subset data for top 10 countries only\ntop_10_data &lt;- subset(percentage_data, country %in% top_10_countries)\n\n# Create pie chart for top 10 countries with improved readability\nlibrary(ggplot2)\n\n# Plotting the pie chart\npie_plot &lt;- ggplot(top_10_data, aes(x = \"\", y = n, fill = type)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"white\") +\n  coord_polar(theta = \"y\") +\n  facet_wrap(~ country, scales = \"free_y\") +\n  geom_text(aes(label = paste0(round(percentage, 2), \"%\")),\n            position = position_stack(vjust = 0.5), color = \"white\", size = 4, family = \"sans\") +  # Adjust text size, color, and font family\n  theme_void() +\n  scale_fill_manual(values = c(\"purple\", \"orange\"), labels = c(\"Movie\", \"TV Show\")) +  # Adjust colors and labels\n  theme(legend.position = \"bottom\", legend.text = element_text(size = 10, family = \"sans\"), plot.title = element_text(hjust = 0.5, size = 14, family = \"sans\")) +  # Adjust legend and title text\n  labs(fill = \"Type\", title = \"Percentage of Movies vs TV Shows in Top 10 Countries\")  # Adjust title\n\n# Show the plot\nprint(pie_plot)\n\n\n\n\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Basic Statistical Analysis",
    "text": "3.2 Basic Statistical Analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nGiven my hypothesis…\nMy predictor Variables (Independent Variables):\n\ntype (Movie or TV Show)\ncountry (Country of origin)\n\nand my response Variable (Dependent Variable):\n\nrating (Age rating assigned to Netflix titles)\n\nLet’s perform some basic statistics to examine the associations between the outcome variable (rating) and each individual predictor variable (type and country).\n\n# Cross-tabulation of type and rating\ntype_rating_table &lt;- table(rawdata$type, rawdata$rating)\nprint(type_rating_table)\n\n               \n                66 min 74 min 84 min    A Classic Movies, Documentaries    G\n  Movie              1      1      1    1                             0   41\n  TV Show            0      0      0    0                             0    0\n  William Wyler      0      0      0    0                             1    0\n               \n                NC-17   NR   PG PG-13    R TV-14 TV-G TV-MA TV-PG TV-Y TV-Y7\n  Movie             3   75  287   490  797  1427  126  2062   539  131   139\n  TV Show           0    5    0     0    2   733   94  1146   323  176   195\n  William Wyler     0    0    0     0    0     0    0     0     0    0     0\n               \n                TV-Y7-FV   UR\n  Movie                5    3\n  TV Show              1    0\n  William Wyler        0    0\n\n\n\n# Chi-square test of independence\ntype_rating_chi2 &lt;- chisq.test(type_rating_table)\n\nWarning in chisq.test(type_rating_table): Chi-squared approximation may be\nincorrect\n\nprint(type_rating_chi2)\n\n\n    Pearson's Chi-squared test\n\ndata:  type_rating_table\nX-squared = 9853.7, df = 36, p-value &lt; 2.2e-16\n\n\nInterpretation of results:\nThe chi-square test statistic is very large (9853.7), and the p-value is extremely small (less than 2.2e-16). This indicates that there is a significant association between type and rating. In other words, the type of content (Movie or TV Show) and the rating are not independent; they are related\nwhat about the association between country and rating though…\n\n# Frequency distribution of rating by country\ncountry_rating_table &lt;- table(rawdata$country, rawdata$rating)\n\n\ncountry_rating_chi2 &lt;- chisq.test(country_rating_table)\n\nWarning in chisq.test(country_rating_table): Chi-squared approximation may be\nincorrect\n\nprint(country_rating_chi2)\n\n\n    Pearson's Chi-squared test\n\ndata:  country_rating_table\nX-squared = 26495, df = 13464, p-value &lt; 2.2e-16\n\n\nInterpretation of results:\nIt appears, again, that the chi-square test statistic is very large (26495), and the p-value is extremely small (less than 2.2e-16). Which indicates that there is a significant association between country and rating. In other words, the country of origin and the rating are not independent; they are related.\nGiven the high association between all my variables (type, country, and rating), I will do some a bit more exploring.\nFirst, I could visually explore the association between my categorical variables using a stacked bar chart, and then run a random forest model to understand the importance of using different predictors and their relationships\n\nrawdata$country &lt;- sapply(strsplit(rawdata$country, \", \"), `[`, 1)\n\n\nunique(rawdata$country)\n\n [1] \"United States\"        \"South Africa\"         \"India\"               \n [4] \"United Kingdom\"       \"Germany\"              \"Mexico\"              \n [7] \"Turkey\"               \"Australia\"            \"Finland\"             \n[10] \"China\"                \"Nigeria\"              \"Japan\"               \n[13] \"Spain\"                \"France\"               \"Belgium\"             \n[16] \"South Korea\"          \"Argentina\"            \"Russia\"              \n[19] \"Canada\"               \"Hong Kong\"            \"Italy\"               \n[22] \"\"                     \"Ireland\"              \"New Zealand\"         \n[25] \"Jordan\"               \"Colombia\"             \"Switzerland\"         \n[28] \"Israel\"               \"Brazil\"               \"Taiwan\"              \n[31] \"Bulgaria\"             \"Poland\"               \"Saudi Arabia\"        \n[34] \"Thailand\"             \"Indonesia\"            \"Egypt\"               \n[37] \"Kuwait\"               \"Malaysia\"             \"Vietnam\"             \n[40] \"Sweden\"               \"Lebanon\"              \"Romania\"             \n[43] \"Philippines\"          \"Iceland\"              \"Denmark\"             \n[46] \"United Arab Emirates\" \"Netherlands\"          \"Norway\"              \n[49] \"Syria\"                \"Mauritius\"            \"Austria\"             \n[52] \"Czech Republic\"       \"Cameroon\"             \"Uruguay\"             \n[55] \"United Kingdom,\"      \"Kenya\"                \"Chile\"               \n[58] \"Luxembourg\"           \"Bangladesh\"           \"Portugal\"            \n[61] \"Hungary\"              \"Senegal\"              \"Singapore\"           \n[64] \"Serbia\"               \"Namibia\"              \"Peru\"                \n[67] \"Mozambique\"           \"Belarus\"              \"Ghana\"               \n[70] \"Zimbabwe\"             \"Puerto Rico\"          \"Pakistan\"            \n[73] \"Cyprus\"               \"Paraguay\"             \"Croatia\"             \n[76] \"United States,\"       \"Cambodia\"             \"Georgia\"             \n[79] \"Soviet Union\"         \"Greece\"               \"West Germany\"        \n[82] \"Iran\"                 \"Venezuela\"            \"Poland,\"             \n[85] \"Slovenia\"             \"Guatemala\"            \"Ukraine\"             \n[88] \"Jamaica\"              \"1944\"                 \"Somalia\"             \n\n\n\nfiltered_data &lt;- rawdata %&gt;%\n  filter(!is.na(type))\n\n# Optionally, filter the data to include only a subset of countries\n# For example, top 10 countries by count of titles\ntop_countries &lt;- filtered_data %&gt;%\n  count(country, sort = TRUE) %&gt;%\n  top_n(10, n) %&gt;%\n  pull(country)\n\nfiltered_data &lt;- filtered_data %&gt;%\n  filter(country %in% top_countries)\n\n# Create the bar chart\nggplot(filtered_data, aes(x = country, fill = rating)) +\n  geom_bar(position = \"fill\") +\n  facet_wrap(~ type, scales = \"free_y\") +\n  labs(title = \"Proportion of Ratings by Type and Country\", x = \"Country\", y = \"Proportion\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"bottom\")\n\n\n\n\nBefore getting started with our random forest we’ll need to install some necessary packages\n\nlibrary(caret)\n\nWarning: package 'caret' was built under R version 4.3.3\n\n\nLoading required package: lattice\n\nlibrary(randomForest)\n\nWarning: package 'randomForest' was built under R version 4.3.3\n\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nlibrary(mice)\n\nWarning: package 'mice' was built under R version 4.3.3\n\n\n\nAttaching package: 'mice'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following objects are masked from 'package:base':\n\n    cbind, rbind\n\n\nIn this step we convert our categorical variables into factors:\n\nfiltered_data$type &lt;- as.factor(filtered_data$type)\nfiltered_data$country &lt;- as.factor(filtered_data$country)\nfiltered_data$rating &lt;- as.factor(filtered_data$rating)\n\nIn this step we somewhat use imputation (replace ‘na’ in our data with mode):\n\ncalculate_mode &lt;- function(x) {\n  uniq_x &lt;- unique(x)\n  uniq_x[which.max(tabulate(match(x, uniq_x)))]\n}\n\n# Function to impute missing values\nimpute_missing &lt;- function(df) {\n  df[] &lt;- lapply(df, function(col) {\n    if (is.numeric(col)) {\n      # Impute numeric columns with median\n      col[is.na(col)] &lt;- median(col, na.rm = TRUE)\n    } else {\n      # Impute categorical columns with mode\n      col[is.na(col)] &lt;- calculate_mode(col)\n    }\n    return(col)\n  })\n  return(df)\n}\n\nNow to split the data into training and test sets…\n\nimputed_data &lt;- impute_missing(filtered_data)\n\n\nset.seed(123)\ntrain_index &lt;- createDataPartition(imputed_data$rating, p = 0.8, list = FALSE)\n\nWarning in createDataPartition(imputed_data$rating, p = 0.8, list = FALSE):\nSome classes have a single record ( 66 min, 74 min, 84 min, A ) and these will\nbe selected for the sample\n\ntrain_data &lt;- imputed_data[train_index, ]\ntest_data &lt;- imputed_data[-train_index, ]\n\nNow to train the random forest model…\n\nset.seed(321) \nrf_model &lt;- randomForest(rating ~ type + country, data = train_data, ntree = 500, mtry = 2, importance = TRUE)\n\n\nprint(rf_model)\n\n\nCall:\n randomForest(formula = rating ~ type + country, data = train_data,      ntree = 500, mtry = 2, importance = TRUE) \n               Type of random forest: classification\n                     Number of trees: 500\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 60.89%\nConfusion matrix:\n         66 min 74 min 84 min A G NC-17 NR PG PG-13 R TV-14 TV-G TV-MA TV-PG\n66 min        0      0      0 0 0     0  0  0     0 0     0    0     1     0\n74 min        0      0      0 0 0     0  0  0     0 0     0    0     1     0\n84 min        0      0      0 0 0     0  0  0     0 0     0    0     1     0\nA             0      0      0 0 0     0  0  0     0 0     0    0     1     0\nG             0      0      0 0 0     0  0  0     0 0     0    0    32     0\nNC-17         0      0      0 0 0     0  0  0     0 0     0    0     3     0\nNR            0      0      0 0 0     0  0  0     0 2     3    0    47     0\nPG            0      0      0 0 0     0  0  0     0 6     4    0   207     0\nPG-13         0      0      0 0 0     0  0  0     0 3     5    0   354     0\nR             0      0      0 0 0     0  0  0     0 6     2    0   566     0\nTV-14         0      0      0 0 0     0  0  0     0 6   555    0   729     0\nTV-G          0      0      0 0 0     0  0  0     0 1     6    0   137     0\nTV-MA         0      0      0 0 0     0  0  0     0 8   266    0  1651     0\nTV-PG         0      0      0 0 0     0  0  0     0 5   131    0   432     0\nTV-Y          0      0      0 0 0     0  0  0     0 4     6    0   216     0\nTV-Y7         0      0      0 0 0     0  0  0     0 1    29    0   222     0\nTV-Y7-FV      0      0      0 0 0     0  0  0     0 0     1    0     3     0\nUR            0      0      0 0 0     0  0  0     0 0     0    0     3     0\n         TV-Y TV-Y7 TV-Y7-FV UR class.error\n66 min      0     0        0  0   1.0000000\n74 min      0     0        0  0   1.0000000\n84 min      0     0        0  0   1.0000000\nA           0     0        0  0   1.0000000\nG           0     0        0  0   1.0000000\nNC-17       0     0        0  0   1.0000000\nNR          0     0        0  0   1.0000000\nPG          0     0        0  0   1.0000000\nPG-13       0     0        0  0   1.0000000\nR           0     0        0  0   0.9895470\nTV-14       0     0        0  0   0.5697674\nTV-G        0     0        0  0   1.0000000\nTV-MA       0     0        0  0   0.1423377\nTV-PG       0     0        0  0   1.0000000\nTV-Y        0     0        0  0   1.0000000\nTV-Y7       0     0        0  0   1.0000000\nTV-Y7-FV    0     0        0  0   1.0000000\nUR          0     0        0  0   1.0000000\n\n\nAnd lastly, let’s evaluate the performance of our random forest:\n\npredictions &lt;- predict(rf_model, test_data)\nconfusion_matrix &lt;- confusionMatrix(predictions, test_data$rating)\n\nhead(confusion_matrix)\n\n$positive\nNULL\n\n$table\n          Reference\nPrediction 66 min 74 min 84 min   A   G NC-17  NR  PG PG-13   R TV-14 TV-G\n  66 min        0      0      0   0   0     0   0   0     0   0     0    0\n  74 min        0      0      0   0   0     0   0   0     0   0     0    0\n  84 min        0      0      0   0   0     0   0   0     0   0     0    0\n  A             0      0      0   0   0     0   0   0     0   0     0    0\n  G             0      0      0   0   0     0   0   0     0   0     0    0\n  NC-17         0      0      0   0   0     0   0   0     0   0     0    0\n  NR            0      0      0   0   0     0   0   0     0   0     0    0\n  PG            0      0      0   0   0     0   0   0     0   0     0    0\n  PG-13         0      0      0   0   0     0   0   0     0   0     0    0\n  R             0      0      0   0   0     0   0   1     0   0     1    2\n  TV-14         0      0      0   0   0     0   1   0     0   0   137    2\n  TV-G          0      0      0   0   0     0   0   0     0   0     0    0\n  TV-MA         0      0      0   0   7     0  11  53    90 143   184   31\n  TV-PG         0      0      0   0   0     0   0   0     0   0     0    0\n  TV-Y          0      0      0   0   0     0   0   0     0   0     0    0\n  TV-Y7         0      0      0   0   0     0   0   0     0   0     0    0\n  TV-Y7-FV      0      0      0   0   0     0   0   0     0   0     0    0\n  UR            0      0      0   0   0     0   0   0     0   0     0    0\n          Reference\nPrediction TV-MA TV-PG TV-Y TV-Y7 TV-Y7-FV  UR\n  66 min       0     0    0     0        0   0\n  74 min       0     0    0     0        0   0\n  84 min       0     0    0     0        0   0\n  A            0     0    0     0        0   0\n  G            0     0    0     0        0   0\n  NC-17        0     0    0     0        0   0\n  NR           0     0    0     0        0   0\n  PG           0     0    0     0        0   0\n  PG-13        0     0    0     0        0   0\n  R            9     0    1     1        0   0\n  TV-14       80    32    2     4        0   0\n  TV-G         0     0    0     0        0   0\n  TV-MA      392   110   53    58        1   0\n  TV-PG        0     0    0     0        0   0\n  TV-Y         0     0    0     0        0   0\n  TV-Y7        0     0    0     0        0   0\n  TV-Y7-FV     0     0    0     0        0   0\n  UR           0     0    0     0        0   0\n\n$overall\n      Accuracy          Kappa  AccuracyLower  AccuracyUpper   AccuracyNull \n   0.376244666    0.084343129    0.350848019    0.402159713    0.342105263 \nAccuracyPValue  McnemarPValue \n   0.003984687            NaN \n\n$byClass\n                Sensitivity Specificity Pos Pred Value Neg Pred Value Precision\nClass: 66 min            NA   1.0000000             NA             NA        NA\nClass: 74 min            NA   1.0000000             NA             NA        NA\nClass: 84 min            NA   1.0000000             NA             NA        NA\nClass: A                 NA   1.0000000             NA             NA        NA\nClass: G          0.0000000   1.0000000            NaN      0.9950213        NA\nClass: NC-17             NA   1.0000000             NA             NA        NA\nClass: NR         0.0000000   1.0000000            NaN      0.9914651        NA\nClass: PG         0.0000000   1.0000000            NaN      0.9615932        NA\nClass: PG-13      0.0000000   1.0000000            NaN      0.9359886        NA\nClass: R          0.0000000   0.9881235      0.0000000      0.8971963 0.0000000\nClass: TV-14      0.4254658   0.8883764      0.5310078      0.8388502 0.5310078\nClass: TV-G       0.0000000   1.0000000            NaN      0.9751067        NA\nClass: TV-MA      0.8149688   0.1989189      0.3459841      0.6739927 0.3459841\nClass: TV-PG      0.0000000   1.0000000            NaN      0.8990043        NA\nClass: TV-Y       0.0000000   1.0000000            NaN      0.9601707        NA\nClass: TV-Y7      0.0000000   1.0000000            NaN      0.9551920        NA\nClass: TV-Y7-FV   0.0000000   1.0000000            NaN      0.9992888        NA\nClass: UR                NA   1.0000000             NA             NA        NA\n                   Recall        F1   Prevalence Detection Rate\nClass: 66 min          NA        NA 0.0000000000     0.00000000\nClass: 74 min          NA        NA 0.0000000000     0.00000000\nClass: 84 min          NA        NA 0.0000000000     0.00000000\nClass: A               NA        NA 0.0000000000     0.00000000\nClass: G        0.0000000        NA 0.0049786629     0.00000000\nClass: NC-17           NA        NA 0.0000000000     0.00000000\nClass: NR       0.0000000        NA 0.0085348506     0.00000000\nClass: PG       0.0000000        NA 0.0384068279     0.00000000\nClass: PG-13    0.0000000        NA 0.0640113798     0.00000000\nClass: R        0.0000000       NaN 0.1017069701     0.00000000\nClass: TV-14    0.4254658 0.4724138 0.2290184922     0.09743954\nClass: TV-G     0.0000000        NA 0.0248933144     0.00000000\nClass: TV-MA    0.8149688 0.4857497 0.3421052632     0.27880512\nClass: TV-PG    0.0000000        NA 0.1009957326     0.00000000\nClass: TV-Y     0.0000000        NA 0.0398293030     0.00000000\nClass: TV-Y7    0.0000000        NA 0.0448079659     0.00000000\nClass: TV-Y7-FV 0.0000000        NA 0.0007112376     0.00000000\nClass: UR              NA        NA 0.0000000000     0.00000000\n                Detection Prevalence Balanced Accuracy\nClass: 66 min             0.00000000                NA\nClass: 74 min             0.00000000                NA\nClass: 84 min             0.00000000                NA\nClass: A                  0.00000000                NA\nClass: G                  0.00000000         0.5000000\nClass: NC-17              0.00000000                NA\nClass: NR                 0.00000000         0.5000000\nClass: PG                 0.00000000         0.5000000\nClass: PG-13              0.00000000         0.5000000\nClass: R                  0.01066856         0.4940618\nClass: TV-14              0.18349929         0.6569211\nClass: TV-G               0.00000000         0.5000000\nClass: TV-MA              0.80583215         0.5069439\nClass: TV-PG              0.00000000         0.5000000\nClass: TV-Y               0.00000000         0.5000000\nClass: TV-Y7              0.00000000         0.5000000\nClass: TV-Y7-FV           0.00000000         0.5000000\nClass: UR                 0.00000000                NA\n\n$mode\n[1] \"sens_spec\"\n\n$dots\nlist()\n\n\n\nvarImpPlot(rf_model)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Interpretation",
    "text": "3.3 Interpretation\nThe results from the random forest model’s performance are summarized in the confusion matrix and accompanying statistics. The overall accuracy of the model is 37.62%, indicating that it correctly classified approximately 38% of the instances. This accuracy is slightly better than the No Information Rate (NIR) of 34.21%, which represents the accuracy we would get by always predicting the most frequent class. The confidence interval for the model’s accuracy ranges from 35.08% to 40.22%, suggesting the true accuracy lies within this interval. The p-value (0.003985) indicates that the model’s accuracy is significantly better than random guessing.\nHowever, the Kappa value of 0.0843 reveals poor agreement between the predicted and actual classifications when accounting for chance. The statistics by class show varying performance across different rating categories. For instance, the sensitivity for “TV-14” is 42.55%, meaning the model correctly identified about 43% of “TV-14” ratings. The specificity for “TV-14” is higher at 88.84%, indicating that the model correctly identified about 89% of non-“TV-14” ratings.\nThe precision for “TV-14” is 53.10%, meaning that just over half of the predicted “TV-14” ratings were correct. In contrast, the model struggles with classes that have low prevalence in the dataset, resulting in low sensitivity and precision for those classes.\nOverall, the model performs moderately well, particularly for certain classes like “TV-MA” which have higher sensitivity and balanced accuracy. However, the model’s performance could be improved by addressing class imbalance, adding more features, tuning hyperparameters, and potentially using more complex ensemble methods. These steps could help achieve better classification accuracy and agreement between predicted and actual ratings."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusion",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusion",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.4 Conclusion",
    "text": "3.4 Conclusion\nThe results indicate that the country of origin significantly influences the distribution of age ratings assigned to Netflix titles, as shown by the high importance of the country variable in the random forest model. The substantial decrease in model accuracy and increase in node impurity when the country variable is excluded further supports this finding. While the type of content (Movie or TV Show) also affects the distribution of age ratings, its impact is less pronounced compared to the country of origin. Excluding the type variable results in a smaller decrease in model performance. Overall, both predictor variables—type and country—contribute to the prediction of age ratings, but the country of origin has a more dominant influence, highlighting that while both factors are important, the country’s impact is stronger."
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "Holly Milazzo Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "Holly Milazzo Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "Holly Milazzo Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "Placeholder file for the future Tidy Tuesday exercise.\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com\"))\n\n\nif (!requireNamespace(\"tidytuesdayR\", quietly = TRUE)) {\n    install.packages(\"tidytuesdayR\")\n}\nlibrary(tidytuesdayR)\n\nWarning: package 'tidytuesdayR' was built under R version 4.3.3\n\n\n\ninstall.packages(\"tidytuesdayR\")\n\nWarning: package 'tidytuesdayR' is in use and will not be installed\n\ntuesdata &lt;- tidytuesdayR::tt_load('2024-07-23')\n\n--- Compiling #TidyTuesday Information for 2024-07-23 ----\n\n\n--- There are 6 files available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 6: `auditions.csv`\n    Downloading file 2 of 6: `eliminations.csv`\n    Downloading file 3 of 6: `finalists.csv`\n    Downloading file 4 of 6: `ratings.csv`\n    Downloading file 5 of 6: `seasons.csv`\n    Downloading file 6 of 6: `songs.csv`\n\n\n--- Download complete ---\n\n\nPART 1: Load, wrangle and explore the data.\n\nhead(tuesdata)\n\n$auditions\n# A tibble: 142 × 12\n   season audition_date_start audition_date_end audition_city     audition_venue\n    &lt;dbl&gt; &lt;date&gt;              &lt;date&gt;            &lt;chr&gt;             &lt;chr&gt;         \n 1      1 2002-04-20          2002-04-22        Los Angeles, Cal… Westin Bonave…\n 2      1 2002-04-23          2002-04-25        Seattle, Washing… Hyatt Regency…\n 3      1 2002-04-26          2002-04-28        Chicago, Illinois Congress Plaz…\n 4      1 2002-04-29          2002-05-01        New York City, N… Millenium Hil…\n 5      1 2002-05-03          2002-05-05        Atlanta, Georgia  AmericasMart/…\n 6      1 2002-05-05          2002-05-07        Dallas, Texas     Wyndham Anato…\n 7      1 2002-05-11          2002-05-11        Miami, Florida    Fontainebleau…\n 8      2 2002-10-21          2002-10-21        Detroit, Michigan Atheneum Suit…\n 9      2 2002-10-24          2002-10-28        New York, New Yo… Regent Wall S…\n10      2 2002-10-27          2002-10-27        Atlanta, Georgia  AmericasMart  \n# ℹ 132 more rows\n# ℹ 7 more variables: episodes &lt;chr&gt;, episode_air_date &lt;chr&gt;,\n#   callback_venue &lt;chr&gt;, callback_date_start &lt;date&gt;, callback_date_end &lt;date&gt;,\n#   tickets_to_hollywood &lt;dbl&gt;, guest_judge &lt;chr&gt;\n\n$eliminations\n# A tibble: 456 × 46\n   season place gender contestant       top_36 top_36_2 top_36_3 top_36_4 top_32\n    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;            &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; \n 1      1 1     Female Kelly Clarkson   &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n 2      1 2     Male   Justin Guarini   &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n 3      1 3     Female Nikki McKibbin   &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n 4      1 4     Female Tamyra Gray      &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n 5      1 5     Male   R. J. Helton     &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n 6      1 6     Female Christina Chris… &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n 7      1 7     Female Ryan Starr       &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n 8      1 8     Male   AJ Gil           &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n 9      1 9–10  Male   EJay Day         &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n10      1 9–10  Male   Jim Verraros     &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n# ℹ 446 more rows\n# ℹ 37 more variables: top_32_2 &lt;chr&gt;, top_32_3 &lt;chr&gt;, top_32_4 &lt;chr&gt;,\n#   top_30 &lt;chr&gt;, top_30_2 &lt;chr&gt;, top_30_3 &lt;chr&gt;, top_25 &lt;chr&gt;, top_25_2 &lt;chr&gt;,\n#   top_25_3 &lt;chr&gt;, top_24 &lt;chr&gt;, top_24_2 &lt;chr&gt;, top_24_3 &lt;chr&gt;, top_20 &lt;chr&gt;,\n#   top_20_2 &lt;chr&gt;, top_16 &lt;chr&gt;, top_14 &lt;chr&gt;, top_13 &lt;chr&gt;, top_12 &lt;chr&gt;,\n#   top_11 &lt;chr&gt;, top_11_2 &lt;chr&gt;, wildcard &lt;chr&gt;, comeback &lt;lgl&gt;, top_10 &lt;chr&gt;,\n#   top_9 &lt;chr&gt;, top_9_2 &lt;chr&gt;, top_8 &lt;chr&gt;, top_8_2 &lt;chr&gt;, top_7 &lt;chr&gt;, …\n\n$finalists\n# A tibble: 190 × 6\n   Contestant          Birthday  Birthplace          Hometown Description Season\n   &lt;chr&gt;               &lt;chr&gt;     &lt;chr&gt;               &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt;\n 1 Kelly Clarkson      24-Apr-82 Fort Worth, Texas   Burleso… \"She perfo…      1\n 2 Justin Guarini      28-Oct-78 Columbus, Georgia   Doylest… \"He perfor…      1\n 3 Nikki McKibbin      28-Sep-78 Grand Prairie, Tex… &lt;NA&gt;     \"She had p…      1\n 4 Tamyra Gray         26-Jul-79 Takoma Park, Maryl… Atlanta… \"She had a…      1\n 5 R. J. Helton        17-May-81 Pasadena, Texas     Cumming… \"J. Helton…      1\n 6 Christina Christian 21-Jun-81 Brooklyn, New York  &lt;NA&gt;     \".Christin…      1\n 7 Ryan Starr          21-Nov-82 Sunland, California &lt;NA&gt;     \"Her audit…      1\n 8 AJ Gil              5-Jul-84  San Diego, Califor… Tacoma,…  &lt;NA&gt;            1\n 9 Jim Verraros        8-Feb-83  Chicago, Illinois   &lt;NA&gt;     \"He grew u…      1\n10 EJay Day            13-Sep-81 Lawrenceville, Geo… &lt;NA&gt;     \"He auditi…      1\n# ℹ 180 more rows\n\n$ratings\n# A tibble: 593 × 17\n   season show_number episode   airdate `18_49_rating_share` viewers_in_millions\n    &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;   &lt;chr&gt;                              &lt;dbl&gt;\n 1      1           1 Auditions June 1… 4.8                                 9.85\n 2      1           2 Hollywoo… June 1… 5.2                                11.2 \n 3      1           3 Top 30: … June 1… 5.2                                10.3 \n 4      1           4 Top 30: … June 1… 4.7                                 9.47\n 5      1           5 Top 30: … June 2… 4.5                                 9.08\n 6      1           6 Top 30: … June 2… 4.2                                 8.53\n 7      1           7 Top 30: … July 2… 5.3                                10.3 \n 8      1           8 Top 30: … July 3… N/A                                 7.5 \n 9      1           9 Wildcard… July 1… 4.1                                 8.97\n10      1          10 Top 10 P… July 1… 5.3                                10.3 \n# ℹ 583 more rows\n# ℹ 11 more variables: timeslot_et &lt;chr&gt;, dvr_18_49 &lt;chr&gt;,\n#   dvr_viewers_millions &lt;chr&gt;, total_18_49 &lt;chr&gt;,\n#   total_viewers_millions &lt;chr&gt;, weekrank &lt;chr&gt;, ref &lt;lgl&gt;, share &lt;chr&gt;,\n#   nightlyrank &lt;dbl&gt;, rating_share_households &lt;chr&gt;, rating_share &lt;chr&gt;\n\n$seasons\n# A tibble: 18 × 10\n   season winner    runner_up original_release original_network hosted_by judges\n    &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;            &lt;chr&gt;     &lt;chr&gt; \n 1      1 Kelly Cl… Justin G… June 11 (2002-0… Fox              Ryan Sea… Paula…\n 2      2 Ruben St… Clay Aik… January 21 (200… Fox              Ryan Sea… Paula…\n 3      3 Fantasia… Diana De… January 19 (200… Fox              Ryan Sea… Paula…\n 4      4 Carrie U… Bo Bice   January 18 (200… Fox              Ryan Sea… Paula…\n 5      5 Taylor H… Katharin… January 17 (200… Fox              Ryan Sea… Paula…\n 6      6 Jordin S… Blake Le… January 16 (200… Fox              Ryan Sea… Paula…\n 7      7 David Co… David Ar… January 15 (200… Fox              Ryan Sea… Paula…\n 8      8 Kris All… Adam Lam… January 13 (200… Fox              Ryan Sea… Paula…\n 9      9 Lee DeWy… Crystal … January 12 (201… Fox              Ryan Sea… Simon…\n10     10 Scotty M… Lauren A… January 19 (201… Fox              Ryan Sea… Randy…\n11     11 Phillip … Jessica … January 18 (201… Fox              Ryan Sea… Randy…\n12     12 Candice … Kree Har… January 16 (201… Fox              Ryan Sea… Randy…\n13     13 Caleb Jo… Jena Ire… January 15 (201… Fox              Ryan Sea… Harry…\n14     14 Nick Fra… Clark Be… January 7 (2015… Fox              Ryan Sea… Harry…\n15     15 Trent Ha… La'Porsh… January 6 (2016… Fox              Ryan Sea… Harry…\n16     16 Maddie P… Caleb Le… March 11 (2018-… ABC              Ryan Sea… Katy …\n17     17 Laine Ha… Alejandr… March 3 (2019-0… ABC              Ryan Sea… Katy …\n18     18 Just Sam  Arthur G… February 16 (20… ABC              Ryan Sea… Katy …\n# ℹ 3 more variables: no_of_episodes &lt;dbl&gt;, finals_venue &lt;chr&gt;, mentor &lt;chr&gt;\n\n$songs\n# A tibble: 2,429 × 8\n   season    week                order contestant song  artist song_theme result\n   &lt;chr&gt;     &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;      &lt;chr&gt; \n 1 Season_01 20020618_top_30_gr…     1 Tamyra Gr… And … Jenni… &lt;NA&gt;       Advan…\n 2 Season_01 20020618_top_30_gr…     2 Jim Verra… When… Doris… &lt;NA&gt;       Advan…\n 3 Season_01 20020618_top_30_gr…     3 Adriel He… I'll… Edwin… &lt;NA&gt;       Elimi…\n 4 Season_01 20020618_top_30_gr…     4 Rodesia E… Dayd… The M… &lt;NA&gt;       Elimi…\n 5 Season_01 20020618_top_30_gr…     5 Natalie B… Crazy Patsy… &lt;NA&gt;       Elimi…\n 6 Season_01 20020618_top_30_gr…     6 Brad Estr… Just… James… &lt;NA&gt;       Elimi…\n 7 Season_01 20020618_top_30_gr…     7 Ryan Starr The … The K… &lt;NA&gt;       Advan…\n 8 Season_01 20020618_top_30_gr…     8 Justinn W… When… Percy… &lt;NA&gt;       Elimi…\n 9 Season_01 20020618_top_30_gr…     9 Kelli Glo… I Wi… Dolly… &lt;NA&gt;       Wild …\n10 Season_01 20020618_top_30_gr…    10 Christoph… Stil… Brian… &lt;NA&gt;       Wild …\n# ℹ 2,419 more rows\n\n\n\nsummary(tuesdata)\n\n             Length Class       Mode\nauditions    12     spec_tbl_df list\neliminations 46     spec_tbl_df list\nfinalists     6     spec_tbl_df list\nratings      17     spec_tbl_df list\nseasons      10     spec_tbl_df list\nsongs         8     spec_tbl_df list\n\n\nChecking for missing values\n\nsapply(tuesdata, function(df) sum(is.na(df)))\n\n   auditions eliminations    finalists      ratings      seasons        songs \n         418        16763          107         5185           33         1686 \n\n\nRunning some summary statisics\n\nsapply(tuesdata, function(df) if(is.data.frame(df)) { summary(df) } else { NULL })\n\n$auditions\n     season      audition_date_start  audition_date_end    audition_city     \n Min.   : 1.00   Min.   :2002-04-20   Min.   :2002-04-22   Length:142        \n 1st Qu.: 6.00   1st Qu.:2006-08-11   1st Qu.:2006-08-11   Class :character  \n Median :10.00   Median :2010-09-05   Median :2010-09-05   Mode  :character  \n Mean   :10.37   Mean   :2011-04-14   Mean   :2011-04-14                     \n 3rd Qu.:15.00   3rd Qu.:2015-09-05   3rd Qu.:2015-09-05                     \n Max.   :18.00   Max.   :2019-09-21   Max.   :2019-09-21                     \n                                                                             \n audition_venue       episodes         episode_air_date   callback_venue    \n Length:142         Length:142         Length:142         Length:142        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n callback_date_start  callback_date_end    tickets_to_hollywood\n Min.   :2002-02-06   Min.   :2002-02-06   Min.   :  6.0       \n 1st Qu.:2006-10-02   1st Qu.:2006-10-03   1st Qu.: 20.0       \n Median :2010-11-09   Median :2010-11-10   Median : 29.0       \n Mean   :2011-06-11   Mean   :2011-06-12   Mean   : 41.8       \n 3rd Qu.:2015-09-13   3rd Qu.:2015-09-14   3rd Qu.: 37.0       \n Max.   :2019-09-21   Max.   :2019-09-21   Max.   :561.0       \n NA's   :13           NA's   :13           NA's   :48          \n guest_judge       \n Length:142        \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\n$eliminations\n     season         place              gender           contestant       \n Min.   : 1.00   Length:456         Length:456         Length:456        \n 1st Qu.: 4.00   Class :character   Class :character   Class :character  \n Median : 8.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   : 8.86                                                           \n 3rd Qu.:13.00                                                           \n Max.   :18.00                                                           \n    top_36            top_36_2           top_36_3           top_36_4        \n Length:456         Length:456         Length:456         Length:456        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    top_32            top_32_2           top_32_3           top_32_4        \n Length:456         Length:456         Length:456         Length:456        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    top_30            top_30_2           top_30_3            top_25         \n Length:456         Length:456         Length:456         Length:456        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   top_25_2           top_25_3            top_24            top_24_2        \n Length:456         Length:456         Length:456         Length:456        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   top_24_3            top_20            top_20_2            top_16         \n Length:456         Length:456         Length:456         Length:456        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    top_14             top_13             top_12             top_11         \n Length:456         Length:456         Length:456         Length:456        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n   top_11_2           wildcard         comeback          top_10         \n Length:456         Length:456         Mode:logical   Length:456        \n Class :character   Class :character   NA's:456       Class :character  \n Mode  :character   Mode  :character                  Mode  :character  \n                                                                        \n                                                                        \n                                                                        \n    top_9             top_9_2             top_8             top_8_2         \n Length:456         Length:456         Length:456         Length:456        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    top_7             top_7_2             top_6             top_6_2         \n Length:456         Length:456         Length:456         Length:456        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    top_5             top_5_2             top_4             top_4_2         \n Length:456         Length:456         Length:456         Length:456        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    top_3              finale         \n Length:456         Length:456        \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\n$finalists\n  Contestant          Birthday          Birthplace          Hometown        \n Length:190         Length:190         Length:190         Length:190        \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n Description            Season      \n Length:190         Min.   : 1.000  \n Class :character   1st Qu.: 5.000  \n Mode  :character   Median : 9.000  \n                    Mean   : 8.863  \n                    3rd Qu.:13.000  \n                    Max.   :17.000  \n\n$ratings\n     season        show_number      episode            airdate         \n Min.   : 1.000   Min.   : 1.00   Length:593         Length:593        \n 1st Qu.: 4.000   1st Qu.: 9.00   Class :character   Class :character  \n Median : 8.000   Median :18.00   Mode  :character   Mode  :character  \n Mean   : 8.295   Mean   :19.24                                        \n 3rd Qu.:12.000   3rd Qu.:29.00                                        \n Max.   :18.000   Max.   :44.00                                        \n                                                                       \n 18_49_rating_share viewers_in_millions timeslot_et         dvr_18_49        \n Length:593         Min.   : 5.38       Length:593         Length:593        \n Class :character   1st Qu.:12.57       Class :character   Class :character  \n Mode  :character   Median :21.76       Mode  :character   Mode  :character  \n                    Mean   :19.88                                            \n                    3rd Qu.:26.09                                            \n                    Max.   :38.10                                            \n                    NA's   :3                                                \n dvr_viewers_millions total_18_49        total_viewers_millions\n Length:593           Length:593         Length:593            \n Class :character     Class :character   Class :character      \n Mode  :character     Mode  :character   Mode  :character      \n                                                               \n                                                               \n                                                               \n                                                               \n   weekrank           ref             share            nightlyrank   \n Length:593         Mode:logical   Length:593         Min.   :1.000  \n Class :character   NA's:593       Class :character   1st Qu.:1.000  \n Mode  :character                  Mode  :character   Median :2.000  \n                                                      Mean   :2.083  \n                                                      3rd Qu.:3.000  \n                                                      Max.   :4.000  \n                                                      NA's   :569    \n rating_share_households rating_share      \n Length:593              Length:593        \n Class :character        Class :character  \n Mode  :character        Mode  :character  \n                                           \n                                           \n                                           \n                                           \n\n$seasons\n     season         winner           runner_up         original_release  \n Min.   : 1.00   Length:18          Length:18          Length:18         \n 1st Qu.: 5.25   Class :character   Class :character   Class :character  \n Median : 9.50   Mode  :character   Mode  :character   Mode  :character  \n Mean   : 9.50                                                           \n 3rd Qu.:13.75                                                           \n Max.   :18.00                                                           \n                                                                         \n original_network    hosted_by            judges          no_of_episodes \n Length:18          Length:18          Length:18          Min.   :16.00  \n Class :character   Class :character   Class :character   1st Qu.:18.25  \n Mode  :character   Mode  :character   Mode  :character   Median :19.00  \n                                                          Mean   :19.50  \n                                                          3rd Qu.:20.25  \n                                                          Max.   :24.00  \n                                                          NA's   :14     \n finals_venue          mentor         \n Length:18          Length:18         \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n                                      \n\n$songs\n    season              week               order         contestant       \n Length:2429        Length:2429        Min.   : 1.000   Length:2429       \n Class :character   Class :character   1st Qu.: 3.000   Class :character  \n Mode  :character   Mode  :character   Median : 5.000   Mode  :character  \n                                       Mean   : 5.931                     \n                                       3rd Qu.: 8.000                     \n                                       Max.   :40.000                     \n     song              artist           song_theme           result         \n Length:2429        Length:2429        Length:2429        Length:2429       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n\n\nInstalling some libraries I’ll need for EDA\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\nlibrary(caTools)\n\nWarning: package 'caTools' was built under R version 4.3.3\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\n\nggplot(tuesdata$auditions, aes(x = season)) +\n  geom_histogram(binwidth = 1, fill = \"#2c3e50\", color = \"#ecf0f1\", alpha = 0.8) +\n  theme_minimal() +\n  labs(\n    title = \"Distribution of Auditions by Season\",\n    x = \"Season\",\n    y = \"Count\"\n  ) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    axis.title.x = element_text(size = 12, face = \"bold\"),\n    axis.title.y = element_text(size = 12, face = \"bold\"),\n    axis.text = element_text(size = 10),\n    panel.grid.major = element_line(color = \"#bdc3c7\"),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\nIs there any relationship, let’s say, between ratings and\n\nggplot(tuesdata$ratings, aes(x = viewers_in_millions, y = season)) +\n  geom_point() +\n  theme_minimal() +\n  labs(title = \"Relationship between Viewer and Season\", x = \"viewers_in_millions\", y = \"season\")\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nIt appears from this output that Season 5 was a peak in viewership. From what Wikipedia says: The fifth season of American Idol began on January 17, 2006, and concluded on May 24, 2006. Paula Abdul, Simon Cowell, and Randy Jackson returned as judges, while Ryan Seacrest returned as host. Taylor Hicks was named the winner, while Katharine McPhee was the runner-up. It also says that Season 6 set a new record of “74 million votes were cast in the finale round, and a new record of 609 million votes were cast in the entire season”.\nPART 2: Once you understand the data sufficiently, formulate a question/hypothesis.\nHypothesis:\nThe number of auditions per season is positively correlated with the ratings of the show for that season. Higher audition counts might indicate greater public interest, leading to higher viewership and ratings.Greater participation in auditions might also indicate a more engaged viewer base, which could translate to higher viewership numbers when during viewer voting.\nPART 3: Once you determine the question and thus your outcome and main predictors, further pre-process and clean the data as needed.\nFirst let’s merge ‘auditions’ and ‘ratings’"
  },
  {
    "objectID": "data-exercise/machine-learning-project.html",
    "href": "data-exercise/machine-learning-project.html",
    "title": "DA6813 - Machine Learning Project",
    "section": "",
    "text": "Project Proposal: Understanding Consumer Behavior for Bridgerton Estate Financial Using Online Interaction Data\nBackground Bridgerton Estate Financial is a one-of-a-kind banking institution that offers a range of financial products, including personal loans, credit cards, mortgages, and savings accounts. As a forward-thinking company, Bridgerton Estate Financial has invested heavily in its digital channels to engage with customers online, through mobile applications, and via web portals. Customers can explore products, ask questions, and complete applications directly through these platforms. To optimize the customer journey, Bridgerton Estate Financial tracks key digital events from their customers’ online behaviors. These events include interactions like product page visits, application starts, and completions or abandonments of forms. Our goal is to analyze this event tracking data to better understand consumer behavior and determine which factors drive customers to complete an application for a financial product or abandon the process.\nMotivation As a data-driven organization, Bridgerton Estate Financial wants to enhance its understanding of consumer behavior on digital platforms to: • Improve Conversion Rates: Identify the key factors that lead to successful application completions and reduce application drop-offs. • Enhance the Customer Experience: Pinpoint areas of friction in the application process and improve the user interface to streamline the experience. • Optimize Marketing and Customer Engagement: Gain insights into when and how to engage customers more effectively, such as understanding peak interaction times or preferred devices. By leveraging data from customer interactions, the goal is to answer critical questions about the online behavior of Bridgerton Estate Financial’s customers, which would enable the company to make more informed decisions and improve customer conversion rates.\n\npacman::p_load(readr, here, dplyr, lubridate, corrplot, caret, ggplot2, car, e1071, vcd)\n\n\ndata_location &lt;- here::here(\"data-exercise\",\"Synthetic_Event_Data.csv\")\nrawdata &lt;- read_csv(data_location, show_col_types = FALSE)\n\n\nstr(rawdata)\n\nspc_tbl_ [1,000 × 17] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ EVENT_DT                      : chr [1:1000] \"10/17/2024\" \"10/17/2024\" \"10/17/2024\" \"10/17/2024\" ...\n $ EVENT_EFFECTIVE_GMT_TS        : 'hms' num [1:1000] 11:48:00 13:39:00 13:13:00 06:56:00 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ SOURCE_CHANNEL_CD             : chr [1:1000] \"www\" \"www\" \"www\" \"mob\" ...\n $ SOURCE_EVENT_ID               : chr [1:1000] \"bk-auto-loan-app-0\" \"bk-lending-cloe-fe-closing-1\" \"bk-card-cco-apply-2\" \"bk-auto-loan-app-3\" ...\n $ SOURCE_EVENT_DESC_TXT         : chr [1:1000] \"cco_approve_edit_card\" \"auto_loan_app_approved\" \"auto_loan_app_approved\" \"auto_loan_app_approved\" ...\n $ CORE_EVENT_TYPE_NM            : chr [1:1000] \"CSL\" \"ASL\" \"CSL\" \"CRC\" ...\n $ CORE_EVENT_SUBTYPE_NM         : chr [1:1000] \"DEC\" \"DEC\" \"DEC\" \"ACQ\" ...\n $ CORE_EVENT_NM_NOUN_NM         : num [1:1000] 655 690 760 771 776 603 600 786 661 719 ...\n $ ORIGINATING_CONTACT_POINT     : chr [1:1000] \"contact_point_6\" \"contact_point_5\" \"contact_point_4\" \"contact_point_3\" ...\n $ DESTINATION_CONTACT_POINT     : chr [1:1000] \"contact_point_3\" \"contact_point_4\" \"contact_point_9\" \"contact_point_6\" ...\n $ OTHER_ATTRIBUTES              : chr [1:1000] \"attribute_10\" \"attribute_10\" \"attribute_10\" \"attribute_4\" ...\n $ RELATED_PRODUCT               : chr [1:1000] \"product_4\" \"product_8\" \"product_10\" \"product_6\" ...\n $ COMMUNICATION_EVENT_ATTRIBUTES: chr [1:1000] \"comm_event_9\" \"comm_event_2\" \"comm_event_4\" \"comm_event_4\" ...\n $ CORE_EVENT_SUBTYPE_2_NM       : chr [1:1000] \"CLM\" \"CLM\" \"PRC\" \"CLM\" ...\n $ CORE_EVENT_SUBTYPE_3_NM       : chr [1:1000] \"APPLY FOR LOAN\" \"APPLY FOR LOAN\" \"APPLY FOR CONSUMER LOAN\" \"OPEN CREDIT CARD ACCOUNT\" ...\n $ POST_EVENT                    : chr [1:1000] \"App Not Complete\" \"App Not Complete\" \"App Not Complete\" \"App Not Complete\" ...\n $ CUSTOM_ATTRIBUTE              : chr [1:1000] \"custom_attr_6\" \"custom_attr_3\" \"custom_attr_4\" \"custom_attr_8\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   EVENT_DT = col_character(),\n  ..   EVENT_EFFECTIVE_GMT_TS = col_time(format = \"\"),\n  ..   SOURCE_CHANNEL_CD = col_character(),\n  ..   SOURCE_EVENT_ID = col_character(),\n  ..   SOURCE_EVENT_DESC_TXT = col_character(),\n  ..   CORE_EVENT_TYPE_NM = col_character(),\n  ..   CORE_EVENT_SUBTYPE_NM = col_character(),\n  ..   CORE_EVENT_NM_NOUN_NM = col_double(),\n  ..   ORIGINATING_CONTACT_POINT = col_character(),\n  ..   DESTINATION_CONTACT_POINT = col_character(),\n  ..   OTHER_ATTRIBUTES = col_character(),\n  ..   RELATED_PRODUCT = col_character(),\n  ..   COMMUNICATION_EVENT_ATTRIBUTES = col_character(),\n  ..   CORE_EVENT_SUBTYPE_2_NM = col_character(),\n  ..   CORE_EVENT_SUBTYPE_3_NM = col_character(),\n  ..   POST_EVENT = col_character(),\n  ..   CUSTOM_ATTRIBUTE = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nI will need to convert EVENT_DT to a date type and POST_EVENT to a factor as this is my target variable. I also think there will be some use in know specific day of the week and hour for consumer’s digital activity so I’m going to break this out into their own variables. To do the ‘HOUR’ parsing I will need the package ‘Lubridate’\n\nrawdata &lt;- rawdata %&gt;%\n  mutate(\n    # Converting EVENT_DT to a Date format\n    EVENT_DT = as.Date(EVENT_DT, format = \"%m/%d/%Y\"), #redundant because it's only 1 day of data\n    \n    # Properly encoding POST_EVENT as a binary numeric variable (0 for \"App Not Complete\", 1 for \"App Complete\")\n    POST_EVENT = ifelse(POST_EVENT == \"App Complete\", 1, 0),\n    \n    # Extracting the day of the week from EVENT_DT --for possible future use if I get more observations\n    #DAY_OF_WEEK = weekdays(EVENT_DT),\n    \n    # Extracting the hour of interaction from EVENT_EFFECTIVE_GMT_TS\n    HOUR = hour(EVENT_EFFECTIVE_GMT_TS),\n    \n    # Creating an interaction term between HOUR and SOURCE_CHANNEL_CD\n    HOUR_CHANNEL_INTERACTION = HOUR * as.numeric(as.factor(SOURCE_CHANNEL_CD))\n  ) %&gt;%\n  mutate_if(is.character, as.factor) # Converting all character variables to factors\n\nNow to inspect data for missing values\n\ncolSums(is.na(rawdata))\n\n                      EVENT_DT         EVENT_EFFECTIVE_GMT_TS \n                             0                              0 \n             SOURCE_CHANNEL_CD                SOURCE_EVENT_ID \n                             0                              0 \n         SOURCE_EVENT_DESC_TXT             CORE_EVENT_TYPE_NM \n                             0                              0 \n         CORE_EVENT_SUBTYPE_NM          CORE_EVENT_NM_NOUN_NM \n                             0                              0 \n     ORIGINATING_CONTACT_POINT      DESTINATION_CONTACT_POINT \n                             0                              0 \n              OTHER_ATTRIBUTES                RELATED_PRODUCT \n                             0                              0 \nCOMMUNICATION_EVENT_ATTRIBUTES        CORE_EVENT_SUBTYPE_2_NM \n                             0                              0 \n       CORE_EVENT_SUBTYPE_3_NM                     POST_EVENT \n                             0                              0 \n              CUSTOM_ATTRIBUTE                           HOUR \n                             0                              0 \n      HOUR_CHANNEL_INTERACTION \n                             0 \n\n\nChecking to see how balanced my response variable is\n\npost_event_balance &lt;- rawdata %&gt;%\n  count(POST_EVENT) %&gt;%\n  mutate(Proportion = n / sum(n))\n\nggplot(post_event_balance, aes(x = POST_EVENT, y = Proportion, fill = POST_EVENT)) +\n  geom_bar(stat = \"identity\", alpha = 0.8) +\n  geom_text(aes(label = paste0(round(Proportion * 100, 1), \"%\")), \n            position = position_stack(vjust = 0.5), \n            color = \"white\", \n            size = 5) +\n  labs(title = \"Distribution of Response (POST_EVENT): App Complete v App Not Complete\",\n       x = \"POST_EVENT\",\n       y = \"Proportion\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n#FEATURE ENGINEERING #Principal Component Analysis (PCA) SECTION for reducing and simplifying\n\nsingle_level_cols &lt;- names(Filter(function(x) length(unique(x)) == 1, rawdata))\n\nsingle_level_cols\n\n[1] \"EVENT_DT\"\n\n\nremove variable with only one level, EVENT_DT only had one day anyway\n\n# Removing Event_DT\nrawdata &lt;- rawdata[, !(names(rawdata) %in% single_level_cols)]\n\n\n# Subsetting the numeric variables in prep for PCA\nnumeric_data &lt;- rawdata %&gt;% select_if(is.numeric)\n\nscaled_numeric_data &lt;- scale(numeric_data)\n\n\n# Running PCA\npca_result &lt;- prcomp(scaled_numeric_data, scale. = TRUE)\n\nsummary(pca_result)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4\nStandard deviation     1.3263 1.0065 0.9947 0.48817\nProportion of Variance 0.4398 0.2533 0.2474 0.05958\nCumulative Proportion  0.4398 0.6931 0.9404 1.00000\n\n\nInterpretation: PC1 has the highest standard deviation, meaning it captures the most variance, it explains 43.98% of the variance alone so combining it with PC2 and PCA3 explainS 94.04% of the variance. Running the Scree plot below confirms this, I only need to retain 3.\n\ncumulative_variance &lt;- cumsum(pca_result$sdev^2 / sum(pca_result$sdev^2))\nplot(cumulative_variance, type = \"b\", xlab = \"Number of Components\", \n     ylab = \"Cumulative Variance Explained\", \n     main = \"Scree Plot on Cumulative Variance\")\nabline(h = 0.95, col = \"red\", lty = 2) # shows the 95% threshold\n\n\n\n\nI would like to retain these 3 components moving forward because one of the benefits of doing this would be that the PCA components are uncorrelated (orthogonal), which should resolve any multicollinearity issues. Unfortunately, the downside would be that by transforming these original variables they would no longer be interpretable. While a logistic/SVM model might benefit from this, I don’t think I’ll need such a transformation anyway when using random forest down the line.\n#Checking collinearity in numerical variables\nRunning VIF\n\nvif_model &lt;- lm(POST_EVENT ~ ., data = numeric_data)\nvif_values &lt;- vif(vif_model)\nprint(vif_values)\n\n   CORE_EVENT_NM_NOUN_NM                     HOUR HOUR_CHANNEL_INTERACTION \n                1.002510                 2.359174                 2.360680 \n\n\nThe VIF results indicate that all the variables have very low VIF values, with none exceeding the commonly accepted thresholds for multicollinearity (e.g., 5 for moderate multicollinearity or 10 for high multicollinearity). This means that multicollinearity for the numeric values is not a concern in my dataset and can idealy remain in the model.\n#Checking collinearity in Categorical variables\n\ncategorical_columns &lt;- rawdata %&gt;% select_if(is.factor)\n\n# Running pairwise Chi-Square + Cramer's V to cross-check all highly associated variable pairings\npairwise_cramers_v &lt;- combn(names(categorical_columns), 2, function(cols) {\n  var1 &lt;- categorical_columns[[cols[1]]]\n  var2 &lt;- categorical_columns[[cols[2]]]\n  cramer_val &lt;- assocstats(table(var1, var2))$cramer\n  data.frame(Var1 = cols[1], Var2 = cols[2], CramersV = cramer_val)\n}, simplify = FALSE) %&gt;% bind_rows()\n\nhigh_association &lt;- pairwise_cramers_v %&gt;% filter(CramersV &gt; 0.7) # Threshold for strong association\nprint(high_association)\n\n                Var1                           Var2 CramersV\n1  SOURCE_CHANNEL_CD                SOURCE_EVENT_ID        1\n2    SOURCE_EVENT_ID          SOURCE_EVENT_DESC_TXT        1\n3    SOURCE_EVENT_ID             CORE_EVENT_TYPE_NM        1\n4    SOURCE_EVENT_ID          CORE_EVENT_SUBTYPE_NM        1\n5    SOURCE_EVENT_ID      ORIGINATING_CONTACT_POINT        1\n6    SOURCE_EVENT_ID      DESTINATION_CONTACT_POINT        1\n7    SOURCE_EVENT_ID               OTHER_ATTRIBUTES        1\n8    SOURCE_EVENT_ID                RELATED_PRODUCT        1\n9    SOURCE_EVENT_ID COMMUNICATION_EVENT_ATTRIBUTES        1\n10   SOURCE_EVENT_ID        CORE_EVENT_SUBTYPE_2_NM        1\n11   SOURCE_EVENT_ID        CORE_EVENT_SUBTYPE_3_NM        1\n12   SOURCE_EVENT_ID               CUSTOM_ATTRIBUTE        1\n\n\nInterpretation of results: The results of the Cramer’s V test indicate a perfect associations (Cramer’s V = 1) between SOURCE_EVENT_ID and over half of the other categorical variables. This means that SOURCE_EVENT_ID is highly predictive of these variables or vice versa, suggesting redundancy. I’m going to remove this variable from my dataset moving forward. Dropping EVENT_EFFECTIVE_GMT_TS variable as we have already created an ‘Hour’ variable out of it and don’t want to use both.\n\nrawdata &lt;- rawdata %&gt;% select(-SOURCE_EVENT_ID) # Dropping SOURCE_EVENT_ID from dataset\n\nDouble checking my data set to see if everything is ready for modeling\n\nstr(rawdata)\n\ntibble [1,000 × 17] (S3: tbl_df/tbl/data.frame)\n $ EVENT_EFFECTIVE_GMT_TS        : 'hms' num [1:1000] 11:48:00 13:39:00 13:13:00 06:56:00 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ SOURCE_CHANNEL_CD             : Factor w/ 4 levels \"and\",\"iph\",\"mob\",..: 4 4 4 3 2 2 1 1 2 4 ...\n $ SOURCE_EVENT_DESC_TXT         : Factor w/ 3 levels \"auto_loan_app_approved\",..: 2 1 1 1 3 3 2 1 3 3 ...\n $ CORE_EVENT_TYPE_NM            : Factor w/ 3 levels \"ASL\",\"CRC\",\"CSL\": 3 1 3 2 1 2 1 2 3 1 ...\n $ CORE_EVENT_SUBTYPE_NM         : Factor w/ 3 levels \"ACQ\",\"ACQPL\",..: 3 3 3 1 2 2 1 3 1 1 ...\n $ CORE_EVENT_NM_NOUN_NM         : num [1:1000] 655 690 760 771 776 603 600 786 661 719 ...\n $ ORIGINATING_CONTACT_POINT     : Factor w/ 10 levels \"contact_point_1\",..: 7 6 5 4 4 8 8 3 9 8 ...\n $ DESTINATION_CONTACT_POINT     : Factor w/ 10 levels \"contact_point_1\",..: 4 5 10 7 1 3 4 2 8 7 ...\n $ OTHER_ATTRIBUTES              : Factor w/ 10 levels \"attribute_1\",..: 2 2 2 5 2 10 3 1 7 2 ...\n $ RELATED_PRODUCT               : Factor w/ 10 levels \"product_1\",\"product_10\",..: 5 9 2 7 8 3 3 7 6 1 ...\n $ COMMUNICATION_EVENT_ATTRIBUTES: Factor w/ 10 levels \"comm_event_1\",..: 10 3 5 5 4 1 4 6 10 5 ...\n $ CORE_EVENT_SUBTYPE_2_NM       : Factor w/ 3 levels \"CLM\",\"DEC\",\"PRC\": 1 1 3 1 1 2 3 1 3 2 ...\n $ CORE_EVENT_SUBTYPE_3_NM       : Factor w/ 3 levels \"APPLY FOR CONSUMER LOAN\",..: 2 2 1 3 3 2 1 3 2 3 ...\n $ POST_EVENT                    : num [1:1000] 0 0 0 0 1 0 1 0 1 0 ...\n $ CUSTOM_ATTRIBUTE              : Factor w/ 10 levels \"custom_attr_1\",..: 7 4 5 9 7 2 1 5 5 5 ...\n $ HOUR                          : int [1:1000] 11 13 13 6 0 21 12 10 19 14 ...\n $ HOUR_CHANNEL_INTERACTION      : num [1:1000] 44 52 52 18 0 42 12 10 38 56 ...\n\n\n#BEGIN MODELING SECTION**********************************************************\nIf I’m going to try models like SVM and Logistic Regression I may need to scale my data - (is this step needed?)\n\nrawdata_scaled &lt;- rawdata %&gt;%\n  mutate(across(where(is.numeric) & !all_of(\"POST_EVENT\"), scale))\n\n#SPLIT TRAIN/TEST\n\nset.seed(321)\n\ntrain_indices &lt;- createDataPartition(rawdata_scaled$POST_EVENT, p = 0.8, list = FALSE)\ntrain_data &lt;- rawdata_scaled[train_indices, ]\ntest_data &lt;- rawdata_scaled[-train_indices, ]\n\n#Logistic Regression (using PCA components dataset)\n\nnumeric_data &lt;- rawdata %&gt;% select_if(is.numeric)\nscaled_numeric_data &lt;- scale(numeric_data)\npca_result &lt;- prcomp(scaled_numeric_data, scale. = TRUE)\npca_transformed &lt;- as.data.frame(predict(pca_result, newdata = scaled_numeric_data)[, 1:3]) # Using only top 3 components\n\nrawdata_pca &lt;- cbind(POST_EVENT = rawdata$POST_EVENT, pca_transformed)\n\n\nlogistic_model &lt;- glm(POST_EVENT ~ ., data = rawdata_pca[train_indices, ], family = binomial)\n\nWarning: glm.fit: algorithm did not converge\n\nsummary(logistic_model)\n\n\nCall:\nglm(formula = POST_EVENT ~ ., family = binomial, data = rawdata_pca[train_indices, \n    ])\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)    -0.3743 12465.8147   0.000    1.000\nPC1             0.6449  9243.8807   0.000    1.000\nPC2            19.5005 12317.5677   0.002    0.999\nPC3            18.3588 12766.6379   0.001    0.999\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1.109e+03  on 799  degrees of freedom\nResidual deviance: 4.779e-09  on 796  degrees of freedom\nAIC: 8\n\nNumber of Fisher Scoring iterations: 25\n\n\nResults Logistic Model 1: model shit the bed - Write something here about using PCA components not being a working method for this\n#Logistic Regression (using rawdata set)\n\nset.seed(321)\ntrain_indices &lt;- createDataPartition(rawdata$POST_EVENT, p = 0.8, list = FALSE)\ntrain_data &lt;- rawdata[train_indices, ]\ntest_data &lt;- rawdata[-train_indices, ]\n\n\nlogistic_model2 &lt;- glm(POST_EVENT ~ ., data = train_data, family = binomial)\n\nsummary(logistic_model2)\n\n\nCall:\nglm(formula = POST_EVENT ~ ., family = binomial, data = train_data)\n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                           -5.473e-01  1.203e+00\nEVENT_EFFECTIVE_GMT_TS                                -9.053e-05  7.501e-05\nSOURCE_CHANNEL_CDiph                                   8.395e-02  2.436e-01\nSOURCE_CHANNEL_CDmob                                   1.788e-01  3.220e-01\nSOURCE_CHANNEL_CDwww                                   3.563e-01  4.166e-01\nSOURCE_EVENT_DESC_TXTcco_approve_edit_card            -2.519e-01  1.920e-01\nSOURCE_EVENT_DESC_TXTpul_loan_app_post_bureau_decline -1.706e-01  1.909e-01\nCORE_EVENT_TYPE_NMCRC                                  8.732e-02  1.913e-01\nCORE_EVENT_TYPE_NMCSL                                  2.405e-01  1.951e-01\nCORE_EVENT_SUBTYPE_NMACQPL                             2.132e-02  1.853e-01\nCORE_EVENT_SUBTYPE_NMDEC                               1.075e-01  1.878e-01\nCORE_EVENT_NM_NOUN_NM                                  1.283e-03  1.365e-03\nORIGINATING_CONTACT_POINTcontact_point_10             -5.536e-01  3.373e-01\nORIGINATING_CONTACT_POINTcontact_point_2              -3.244e-01  3.449e-01\nORIGINATING_CONTACT_POINTcontact_point_3              -2.714e-02  3.593e-01\nORIGINATING_CONTACT_POINTcontact_point_4              -4.972e-01  3.684e-01\nORIGINATING_CONTACT_POINTcontact_point_5              -1.950e-01  3.287e-01\nORIGINATING_CONTACT_POINTcontact_point_6              -4.380e-01  3.333e-01\nORIGINATING_CONTACT_POINTcontact_point_7               3.325e-02  3.458e-01\nORIGINATING_CONTACT_POINTcontact_point_8              -8.129e-01  3.503e-01\nORIGINATING_CONTACT_POINTcontact_point_9              -5.126e-01  3.565e-01\nDESTINATION_CONTACT_POINTcontact_point_10             -1.431e-01  3.430e-01\nDESTINATION_CONTACT_POINTcontact_point_2               8.348e-02  3.399e-01\nDESTINATION_CONTACT_POINTcontact_point_3               1.153e-01  3.528e-01\nDESTINATION_CONTACT_POINTcontact_point_4              -2.408e-02  3.574e-01\nDESTINATION_CONTACT_POINTcontact_point_5               3.675e-02  3.535e-01\nDESTINATION_CONTACT_POINTcontact_point_6               5.276e-01  3.597e-01\nDESTINATION_CONTACT_POINTcontact_point_7               8.805e-02  3.567e-01\nDESTINATION_CONTACT_POINTcontact_point_8              -9.194e-02  3.520e-01\nDESTINATION_CONTACT_POINTcontact_point_9              -3.673e-01  3.432e-01\nOTHER_ATTRIBUTESattribute_10                          -6.796e-02  3.212e-01\nOTHER_ATTRIBUTESattribute_2                            2.586e-01  3.425e-01\nOTHER_ATTRIBUTESattribute_3                            1.835e-01  3.105e-01\nOTHER_ATTRIBUTESattribute_4                            8.968e-02  3.374e-01\nOTHER_ATTRIBUTESattribute_5                           -2.108e-01  3.435e-01\nOTHER_ATTRIBUTESattribute_6                           -2.760e-01  3.298e-01\nOTHER_ATTRIBUTESattribute_7                            6.711e-01  3.348e-01\nOTHER_ATTRIBUTESattribute_8                            1.511e-01  3.388e-01\nOTHER_ATTRIBUTESattribute_9                            3.594e-01  3.249e-01\nRELATED_PRODUCTproduct_10                              8.978e-01  3.496e-01\nRELATED_PRODUCTproduct_2                               5.074e-01  3.412e-01\nRELATED_PRODUCTproduct_3                               9.146e-02  3.552e-01\nRELATED_PRODUCTproduct_4                               3.756e-01  3.406e-01\nRELATED_PRODUCTproduct_5                               5.396e-01  3.578e-01\nRELATED_PRODUCTproduct_6                               1.073e+00  3.750e-01\nRELATED_PRODUCTproduct_7                               2.789e-01  3.593e-01\nRELATED_PRODUCTproduct_8                              -2.059e-01  3.619e-01\nRELATED_PRODUCTproduct_9                               3.406e-01  3.756e-01\nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_10           -8.177e-01  3.564e-01\nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_2            -3.083e-01  3.316e-01\nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_3            -2.630e-01  3.579e-01\nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_4            -7.216e-01  3.372e-01\nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_5            -6.123e-01  3.428e-01\nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_6            -8.269e-02  3.501e-01\nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_7            -8.027e-01  3.554e-01\nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_8            -2.135e-01  3.436e-01\nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_9            -5.039e-01  3.486e-01\nCORE_EVENT_SUBTYPE_2_NMDEC                            -3.048e-01  1.960e-01\nCORE_EVENT_SUBTYPE_2_NMPRC                            -6.515e-02  1.931e-01\nCORE_EVENT_SUBTYPE_3_NMAPPLY FOR LOAN                  7.126e-02  1.859e-01\nCORE_EVENT_SUBTYPE_3_NMOPEN CREDIT CARD ACCOUNT        2.286e-01  1.971e-01\nCUSTOM_ATTRIBUTEcustom_attr_10                        -6.731e-02  3.771e-01\nCUSTOM_ATTRIBUTEcustom_attr_2                         -1.335e-01  3.632e-01\nCUSTOM_ATTRIBUTEcustom_attr_3                         -4.031e-01  3.454e-01\nCUSTOM_ATTRIBUTEcustom_attr_4                         -3.308e-01  3.395e-01\nCUSTOM_ATTRIBUTEcustom_attr_5                         -1.720e-01  3.733e-01\nCUSTOM_ATTRIBUTEcustom_attr_6                         -3.132e-01  3.497e-01\nCUSTOM_ATTRIBUTEcustom_attr_7                         -1.263e-01  3.662e-01\nCUSTOM_ATTRIBUTEcustom_attr_8                         -3.190e-01  3.516e-01\nCUSTOM_ATTRIBUTEcustom_attr_9                         -2.851e-02  3.643e-01\nHOUR                                                   3.821e-01  2.723e-01\nHOUR_CHANNEL_INTERACTION                              -1.887e-02  1.010e-02\n                                                      z value Pr(&gt;|z|)   \n(Intercept)                                            -0.455  0.64920   \nEVENT_EFFECTIVE_GMT_TS                                 -1.207  0.22744   \nSOURCE_CHANNEL_CDiph                                    0.345  0.73035   \nSOURCE_CHANNEL_CDmob                                    0.555  0.57866   \nSOURCE_CHANNEL_CDwww                                    0.855  0.39243   \nSOURCE_EVENT_DESC_TXTcco_approve_edit_card             -1.312  0.18943   \nSOURCE_EVENT_DESC_TXTpul_loan_app_post_bureau_decline  -0.893  0.37169   \nCORE_EVENT_TYPE_NMCRC                                   0.457  0.64801   \nCORE_EVENT_TYPE_NMCSL                                   1.233  0.21763   \nCORE_EVENT_SUBTYPE_NMACQPL                              0.115  0.90840   \nCORE_EVENT_SUBTYPE_NMDEC                                0.572  0.56708   \nCORE_EVENT_NM_NOUN_NM                                   0.940  0.34698   \nORIGINATING_CONTACT_POINTcontact_point_10              -1.641  0.10076   \nORIGINATING_CONTACT_POINTcontact_point_2               -0.940  0.34699   \nORIGINATING_CONTACT_POINTcontact_point_3               -0.076  0.93980   \nORIGINATING_CONTACT_POINTcontact_point_4               -1.350  0.17712   \nORIGINATING_CONTACT_POINTcontact_point_5               -0.593  0.55302   \nORIGINATING_CONTACT_POINTcontact_point_6               -1.314  0.18885   \nORIGINATING_CONTACT_POINTcontact_point_7                0.096  0.92339   \nORIGINATING_CONTACT_POINTcontact_point_8               -2.321  0.02029 * \nORIGINATING_CONTACT_POINTcontact_point_9               -1.438  0.15049   \nDESTINATION_CONTACT_POINTcontact_point_10              -0.417  0.67655   \nDESTINATION_CONTACT_POINTcontact_point_2                0.246  0.80598   \nDESTINATION_CONTACT_POINTcontact_point_3                0.327  0.74376   \nDESTINATION_CONTACT_POINTcontact_point_4               -0.067  0.94627   \nDESTINATION_CONTACT_POINTcontact_point_5                0.104  0.91722   \nDESTINATION_CONTACT_POINTcontact_point_6                1.467  0.14241   \nDESTINATION_CONTACT_POINTcontact_point_7                0.247  0.80501   \nDESTINATION_CONTACT_POINTcontact_point_8               -0.261  0.79396   \nDESTINATION_CONTACT_POINTcontact_point_9               -1.070  0.28457   \nOTHER_ATTRIBUTESattribute_10                           -0.212  0.83241   \nOTHER_ATTRIBUTESattribute_2                             0.755  0.45024   \nOTHER_ATTRIBUTESattribute_3                             0.591  0.55459   \nOTHER_ATTRIBUTESattribute_4                             0.266  0.79037   \nOTHER_ATTRIBUTESattribute_5                            -0.614  0.53954   \nOTHER_ATTRIBUTESattribute_6                            -0.837  0.40262   \nOTHER_ATTRIBUTESattribute_7                             2.005  0.04499 * \nOTHER_ATTRIBUTESattribute_8                             0.446  0.65549   \nOTHER_ATTRIBUTESattribute_9                             1.106  0.26862   \nRELATED_PRODUCTproduct_10                               2.568  0.01023 * \nRELATED_PRODUCTproduct_2                                1.487  0.13693   \nRELATED_PRODUCTproduct_3                                0.257  0.79681   \nRELATED_PRODUCTproduct_4                                1.103  0.27019   \nRELATED_PRODUCTproduct_5                                1.508  0.13149   \nRELATED_PRODUCTproduct_6                                2.862  0.00421 **\nRELATED_PRODUCTproduct_7                                0.776  0.43769   \nRELATED_PRODUCTproduct_8                               -0.569  0.56948   \nRELATED_PRODUCTproduct_9                                0.907  0.36454   \nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_10            -2.295  0.02176 * \nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_2             -0.930  0.35251   \nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_3             -0.735  0.46238   \nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_4             -2.140  0.03237 * \nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_5             -1.786  0.07409 . \nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_6             -0.236  0.81330   \nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_7             -2.259  0.02390 * \nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_8             -0.621  0.53433   \nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_9             -1.445  0.14833   \nCORE_EVENT_SUBTYPE_2_NMDEC                             -1.555  0.11992   \nCORE_EVENT_SUBTYPE_2_NMPRC                             -0.337  0.73579   \nCORE_EVENT_SUBTYPE_3_NMAPPLY FOR LOAN                   0.383  0.70155   \nCORE_EVENT_SUBTYPE_3_NMOPEN CREDIT CARD ACCOUNT         1.160  0.24624   \nCUSTOM_ATTRIBUTEcustom_attr_10                         -0.178  0.85834   \nCUSTOM_ATTRIBUTEcustom_attr_2                          -0.368  0.71324   \nCUSTOM_ATTRIBUTEcustom_attr_3                          -1.167  0.24317   \nCUSTOM_ATTRIBUTEcustom_attr_4                          -0.974  0.32990   \nCUSTOM_ATTRIBUTEcustom_attr_5                          -0.461  0.64501   \nCUSTOM_ATTRIBUTEcustom_attr_6                          -0.896  0.37049   \nCUSTOM_ATTRIBUTEcustom_attr_7                          -0.345  0.73021   \nCUSTOM_ATTRIBUTEcustom_attr_8                          -0.907  0.36423   \nCUSTOM_ATTRIBUTEcustom_attr_9                          -0.078  0.93762   \nHOUR                                                    1.403  0.16056   \nHOUR_CHANNEL_INTERACTION                               -1.868  0.06178 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1109.0  on 799  degrees of freedom\nResidual deviance: 1027.6  on 728  degrees of freedom\nAIC: 1171.6\n\nNumber of Fisher Scoring iterations: 4\n\n\nResults Logistic Model 2: There appears to be a lot of variables with no significance, let’s apply some stepwise/backward elimination to our model\n\nlogistic_model3 &lt;- step(logistic_model2, direction = \"backward\")\n\nStart:  AIC=1171.64\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + SOURCE_CHANNEL_CD + SOURCE_EVENT_DESC_TXT + \n    CORE_EVENT_TYPE_NM + CORE_EVENT_SUBTYPE_NM + CORE_EVENT_NM_NOUN_NM + \n    ORIGINATING_CONTACT_POINT + DESTINATION_CONTACT_POINT + OTHER_ATTRIBUTES + \n    RELATED_PRODUCT + COMMUNICATION_EVENT_ATTRIBUTES + CORE_EVENT_SUBTYPE_2_NM + \n    CORE_EVENT_SUBTYPE_3_NM + CUSTOM_ATTRIBUTE + HOUR + HOUR_CHANNEL_INTERACTION\n\n                                 Df Deviance    AIC\n- CUSTOM_ATTRIBUTE                9   1030.8 1156.8\n- DESTINATION_CONTACT_POINT       9   1035.5 1161.5\n- OTHER_ATTRIBUTES                9   1038.7 1164.7\n- ORIGINATING_CONTACT_POINT       9   1039.4 1165.4\n- SOURCE_CHANNEL_CD               3   1028.4 1166.4\n- COMMUNICATION_EVENT_ATTRIBUTES  9   1040.5 1166.5\n- CORE_EVENT_SUBTYPE_NM           2   1028.0 1168.0\n- CORE_EVENT_SUBTYPE_3_NM         2   1029.0 1169.0\n- CORE_EVENT_TYPE_NM              2   1029.2 1169.2\n- SOURCE_EVENT_DESC_TXT           2   1029.4 1169.4\n- CORE_EVENT_SUBTYPE_2_NM         2   1030.4 1170.4\n- CORE_EVENT_NM_NOUN_NM           1   1028.5 1170.5\n- EVENT_EFFECTIVE_GMT_TS          1   1029.1 1171.1\n- HOUR                            1   1029.6 1171.6\n&lt;none&gt;                                1027.6 1171.6\n- HOUR_CHANNEL_INTERACTION        1   1031.2 1173.2\n- RELATED_PRODUCT                 9   1049.5 1175.5\n\nStep:  AIC=1156.76\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + SOURCE_CHANNEL_CD + SOURCE_EVENT_DESC_TXT + \n    CORE_EVENT_TYPE_NM + CORE_EVENT_SUBTYPE_NM + CORE_EVENT_NM_NOUN_NM + \n    ORIGINATING_CONTACT_POINT + DESTINATION_CONTACT_POINT + OTHER_ATTRIBUTES + \n    RELATED_PRODUCT + COMMUNICATION_EVENT_ATTRIBUTES + CORE_EVENT_SUBTYPE_2_NM + \n    CORE_EVENT_SUBTYPE_3_NM + HOUR + HOUR_CHANNEL_INTERACTION\n\n                                 Df Deviance    AIC\n- DESTINATION_CONTACT_POINT       9   1038.7 1146.7\n- OTHER_ATTRIBUTES                9   1042.2 1150.2\n- ORIGINATING_CONTACT_POINT       9   1042.9 1150.9\n- SOURCE_CHANNEL_CD               3   1031.6 1151.6\n- COMMUNICATION_EVENT_ATTRIBUTES  9   1044.0 1152.0\n- CORE_EVENT_SUBTYPE_NM           2   1031.2 1153.2\n- CORE_EVENT_SUBTYPE_3_NM         2   1032.1 1154.1\n- SOURCE_EVENT_DESC_TXT           2   1032.4 1154.4\n- CORE_EVENT_TYPE_NM              2   1032.6 1154.6\n- CORE_EVENT_SUBTYPE_2_NM         2   1033.4 1155.4\n- CORE_EVENT_NM_NOUN_NM           1   1031.8 1155.8\n- EVENT_EFFECTIVE_GMT_TS          1   1032.0 1156.0\n- HOUR                            1   1032.5 1156.5\n&lt;none&gt;                                1030.8 1156.8\n- HOUR_CHANNEL_INTERACTION        1   1034.4 1158.4\n- RELATED_PRODUCT                 9   1052.6 1160.6\n\nStep:  AIC=1146.7\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + SOURCE_CHANNEL_CD + SOURCE_EVENT_DESC_TXT + \n    CORE_EVENT_TYPE_NM + CORE_EVENT_SUBTYPE_NM + CORE_EVENT_NM_NOUN_NM + \n    ORIGINATING_CONTACT_POINT + OTHER_ATTRIBUTES + RELATED_PRODUCT + \n    COMMUNICATION_EVENT_ATTRIBUTES + CORE_EVENT_SUBTYPE_2_NM + \n    CORE_EVENT_SUBTYPE_3_NM + HOUR + HOUR_CHANNEL_INTERACTION\n\n                                 Df Deviance    AIC\n- OTHER_ATTRIBUTES                9   1049.8 1139.8\n- ORIGINATING_CONTACT_POINT       9   1050.8 1140.8\n- COMMUNICATION_EVENT_ATTRIBUTES  9   1051.4 1141.4\n- SOURCE_CHANNEL_CD               3   1040.0 1142.0\n- CORE_EVENT_SUBTYPE_NM           2   1039.0 1143.0\n- SOURCE_EVENT_DESC_TXT           2   1040.2 1144.2\n- CORE_EVENT_SUBTYPE_3_NM         2   1040.4 1144.4\n- CORE_EVENT_TYPE_NM              2   1040.5 1144.5\n- CORE_EVENT_NM_NOUN_NM           1   1039.6 1145.6\n- EVENT_EFFECTIVE_GMT_TS          1   1039.9 1145.9\n- CORE_EVENT_SUBTYPE_2_NM         2   1042.2 1146.2\n- HOUR                            1   1040.4 1146.4\n&lt;none&gt;                                1038.7 1146.7\n- HOUR_CHANNEL_INTERACTION        1   1043.2 1149.2\n- RELATED_PRODUCT                 9   1059.5 1149.5\n\nStep:  AIC=1139.76\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + SOURCE_CHANNEL_CD + SOURCE_EVENT_DESC_TXT + \n    CORE_EVENT_TYPE_NM + CORE_EVENT_SUBTYPE_NM + CORE_EVENT_NM_NOUN_NM + \n    ORIGINATING_CONTACT_POINT + RELATED_PRODUCT + COMMUNICATION_EVENT_ATTRIBUTES + \n    CORE_EVENT_SUBTYPE_2_NM + CORE_EVENT_SUBTYPE_3_NM + HOUR + \n    HOUR_CHANNEL_INTERACTION\n\n                                 Df Deviance    AIC\n- ORIGINATING_CONTACT_POINT       9   1062.0 1134.0\n- SOURCE_CHANNEL_CD               3   1050.6 1134.6\n- COMMUNICATION_EVENT_ATTRIBUTES  9   1063.4 1135.4\n- CORE_EVENT_SUBTYPE_NM           2   1050.3 1136.3\n- SOURCE_EVENT_DESC_TXT           2   1051.1 1137.1\n- CORE_EVENT_TYPE_NM              2   1051.2 1137.2\n- CORE_EVENT_SUBTYPE_3_NM         2   1051.5 1137.5\n- CORE_EVENT_SUBTYPE_2_NM         2   1052.2 1138.2\n- CORE_EVENT_NM_NOUN_NM           1   1050.8 1138.8\n- EVENT_EFFECTIVE_GMT_TS          1   1051.0 1139.0\n- HOUR                            1   1051.5 1139.5\n&lt;none&gt;                                1049.8 1139.8\n- RELATED_PRODUCT                 9   1069.0 1141.0\n- HOUR_CHANNEL_INTERACTION        1   1053.2 1141.2\n\nStep:  AIC=1134.01\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + SOURCE_CHANNEL_CD + SOURCE_EVENT_DESC_TXT + \n    CORE_EVENT_TYPE_NM + CORE_EVENT_SUBTYPE_NM + CORE_EVENT_NM_NOUN_NM + \n    RELATED_PRODUCT + COMMUNICATION_EVENT_ATTRIBUTES + CORE_EVENT_SUBTYPE_2_NM + \n    CORE_EVENT_SUBTYPE_3_NM + HOUR + HOUR_CHANNEL_INTERACTION\n\n                                 Df Deviance    AIC\n- SOURCE_CHANNEL_CD               3   1062.8 1128.8\n- COMMUNICATION_EVENT_ATTRIBUTES  9   1076.1 1130.1\n- CORE_EVENT_SUBTYPE_NM           2   1062.7 1130.7\n- SOURCE_EVENT_DESC_TXT           2   1062.8 1130.8\n- CORE_EVENT_TYPE_NM              2   1063.6 1131.6\n- CORE_EVENT_SUBTYPE_2_NM         2   1064.0 1132.0\n- CORE_EVENT_SUBTYPE_3_NM         2   1064.2 1132.2\n- EVENT_EFFECTIVE_GMT_TS          1   1063.2 1133.2\n- CORE_EVENT_NM_NOUN_NM           1   1063.4 1133.4\n- HOUR                            1   1063.6 1133.6\n&lt;none&gt;                                1062.0 1134.0\n- HOUR_CHANNEL_INTERACTION        1   1065.5 1135.5\n- RELATED_PRODUCT                 9   1081.8 1135.8\n\nStep:  AIC=1128.79\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + SOURCE_EVENT_DESC_TXT + \n    CORE_EVENT_TYPE_NM + CORE_EVENT_SUBTYPE_NM + CORE_EVENT_NM_NOUN_NM + \n    RELATED_PRODUCT + COMMUNICATION_EVENT_ATTRIBUTES + CORE_EVENT_SUBTYPE_2_NM + \n    CORE_EVENT_SUBTYPE_3_NM + HOUR + HOUR_CHANNEL_INTERACTION\n\n                                 Df Deviance    AIC\n- COMMUNICATION_EVENT_ATTRIBUTES  9   1077.2 1125.2\n- CORE_EVENT_SUBTYPE_NM           2   1063.5 1125.5\n- SOURCE_EVENT_DESC_TXT           2   1063.6 1125.6\n- CORE_EVENT_TYPE_NM              2   1064.5 1126.5\n- CORE_EVENT_SUBTYPE_2_NM         2   1064.8 1126.8\n- CORE_EVENT_SUBTYPE_3_NM         2   1065.0 1127.0\n- EVENT_EFFECTIVE_GMT_TS          1   1063.8 1127.8\n- CORE_EVENT_NM_NOUN_NM           1   1064.1 1128.1\n- HOUR                            1   1064.2 1128.2\n&lt;none&gt;                                1062.8 1128.8\n- RELATED_PRODUCT                 9   1082.4 1130.4\n- HOUR_CHANNEL_INTERACTION        1   1068.4 1132.4\n\nStep:  AIC=1125.25\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + SOURCE_EVENT_DESC_TXT + \n    CORE_EVENT_TYPE_NM + CORE_EVENT_SUBTYPE_NM + CORE_EVENT_NM_NOUN_NM + \n    RELATED_PRODUCT + CORE_EVENT_SUBTYPE_2_NM + CORE_EVENT_SUBTYPE_3_NM + \n    HOUR + HOUR_CHANNEL_INTERACTION\n\n                           Df Deviance    AIC\n- CORE_EVENT_SUBTYPE_NM     2   1077.7 1121.7\n- SOURCE_EVENT_DESC_TXT     2   1078.2 1122.2\n- CORE_EVENT_TYPE_NM        2   1078.6 1122.6\n- CORE_EVENT_SUBTYPE_3_NM   2   1079.2 1123.2\n- CORE_EVENT_SUBTYPE_2_NM   2   1080.3 1124.3\n- CORE_EVENT_NM_NOUN_NM     1   1078.3 1124.3\n- EVENT_EFFECTIVE_GMT_TS    1   1078.6 1124.6\n- HOUR                      1   1079.0 1125.0\n&lt;none&gt;                          1077.2 1125.2\n- RELATED_PRODUCT           9   1095.7 1125.7\n- HOUR_CHANNEL_INTERACTION  1   1082.5 1128.5\n\nStep:  AIC=1121.72\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + SOURCE_EVENT_DESC_TXT + \n    CORE_EVENT_TYPE_NM + CORE_EVENT_NM_NOUN_NM + RELATED_PRODUCT + \n    CORE_EVENT_SUBTYPE_2_NM + CORE_EVENT_SUBTYPE_3_NM + HOUR + \n    HOUR_CHANNEL_INTERACTION\n\n                           Df Deviance    AIC\n- SOURCE_EVENT_DESC_TXT     2   1078.7 1118.7\n- CORE_EVENT_TYPE_NM        2   1079.1 1119.1\n- CORE_EVENT_SUBTYPE_3_NM   2   1079.7 1119.7\n- CORE_EVENT_SUBTYPE_2_NM   2   1080.7 1120.7\n- CORE_EVENT_NM_NOUN_NM     1   1078.8 1120.8\n- EVENT_EFFECTIVE_GMT_TS    1   1079.1 1121.1\n- HOUR                      1   1079.5 1121.5\n&lt;none&gt;                          1077.7 1121.7\n- RELATED_PRODUCT           9   1096.0 1122.0\n- HOUR_CHANNEL_INTERACTION  1   1083.1 1125.1\n\nStep:  AIC=1118.73\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + CORE_EVENT_TYPE_NM + CORE_EVENT_NM_NOUN_NM + \n    RELATED_PRODUCT + CORE_EVENT_SUBTYPE_2_NM + CORE_EVENT_SUBTYPE_3_NM + \n    HOUR + HOUR_CHANNEL_INTERACTION\n\n                           Df Deviance    AIC\n- CORE_EVENT_TYPE_NM        2   1080.1 1116.1\n- CORE_EVENT_SUBTYPE_3_NM   2   1080.9 1116.9\n- CORE_EVENT_SUBTYPE_2_NM   2   1081.5 1117.5\n- CORE_EVENT_NM_NOUN_NM     1   1079.8 1117.8\n- EVENT_EFFECTIVE_GMT_TS    1   1080.1 1118.1\n- HOUR                      1   1080.5 1118.5\n&lt;none&gt;                          1078.7 1118.7\n- RELATED_PRODUCT           9   1096.8 1118.8\n- HOUR_CHANNEL_INTERACTION  1   1084.4 1122.4\n\nStep:  AIC=1116.12\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + CORE_EVENT_NM_NOUN_NM + \n    RELATED_PRODUCT + CORE_EVENT_SUBTYPE_2_NM + CORE_EVENT_SUBTYPE_3_NM + \n    HOUR + HOUR_CHANNEL_INTERACTION\n\n                           Df Deviance    AIC\n- CORE_EVENT_SUBTYPE_3_NM   2   1082.3 1114.3\n- CORE_EVENT_SUBTYPE_2_NM   2   1082.9 1114.9\n- CORE_EVENT_NM_NOUN_NM     1   1081.3 1115.3\n- EVENT_EFFECTIVE_GMT_TS    1   1081.4 1115.4\n- HOUR                      1   1081.7 1115.7\n&lt;none&gt;                          1080.1 1116.1\n- RELATED_PRODUCT           9   1098.2 1116.2\n- HOUR_CHANNEL_INTERACTION  1   1085.5 1119.5\n\nStep:  AIC=1114.28\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + CORE_EVENT_NM_NOUN_NM + \n    RELATED_PRODUCT + CORE_EVENT_SUBTYPE_2_NM + HOUR + HOUR_CHANNEL_INTERACTION\n\n                           Df Deviance    AIC\n- CORE_EVENT_SUBTYPE_2_NM   2   1084.7 1112.7\n- EVENT_EFFECTIVE_GMT_TS    1   1083.3 1113.3\n- CORE_EVENT_NM_NOUN_NM     1   1083.4 1113.4\n- HOUR                      1   1083.6 1113.6\n- RELATED_PRODUCT           9   1100.0 1114.0\n&lt;none&gt;                          1082.3 1114.3\n- HOUR_CHANNEL_INTERACTION  1   1087.1 1117.1\n\nStep:  AIC=1112.7\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + CORE_EVENT_NM_NOUN_NM + \n    RELATED_PRODUCT + HOUR + HOUR_CHANNEL_INTERACTION\n\n                           Df Deviance    AIC\n- EVENT_EFFECTIVE_GMT_TS    1   1085.7 1111.7\n- RELATED_PRODUCT           9   1101.9 1111.9\n- HOUR                      1   1086.0 1112.0\n- CORE_EVENT_NM_NOUN_NM     1   1086.0 1112.0\n&lt;none&gt;                          1084.7 1112.7\n- HOUR_CHANNEL_INTERACTION  1   1089.1 1115.1\n\nStep:  AIC=1111.74\nPOST_EVENT ~ CORE_EVENT_NM_NOUN_NM + RELATED_PRODUCT + HOUR + \n    HOUR_CHANNEL_INTERACTION\n\n                           Df Deviance    AIC\n- RELATED_PRODUCT           9   1103.1 1111.1\n- CORE_EVENT_NM_NOUN_NM     1   1087.2 1111.2\n&lt;none&gt;                          1085.7 1111.7\n- HOUR_CHANNEL_INTERACTION  1   1090.3 1114.3\n- HOUR                      1   1090.5 1114.5\n\nStep:  AIC=1111.06\nPOST_EVENT ~ CORE_EVENT_NM_NOUN_NM + HOUR + HOUR_CHANNEL_INTERACTION\n\n                           Df Deviance    AIC\n- CORE_EVENT_NM_NOUN_NM     1   1104.5 1110.5\n&lt;none&gt;                          1103.1 1111.1\n- HOUR_CHANNEL_INTERACTION  1   1107.2 1113.2\n- HOUR                      1   1107.2 1113.2\n\nStep:  AIC=1110.48\nPOST_EVENT ~ HOUR + HOUR_CHANNEL_INTERACTION\n\n                           Df Deviance    AIC\n&lt;none&gt;                          1104.5 1110.5\n- HOUR_CHANNEL_INTERACTION  1   1108.4 1112.4\n- HOUR                      1   1108.5 1112.5\n\nsummary(logistic_model3)\n\n\nCall:\nglm(formula = POST_EVENT ~ HOUR + HOUR_CHANNEL_INTERACTION, family = binomial, \n    data = train_data)\n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)              -0.119246   0.141714  -0.841   0.4001  \nHOUR                      0.031739   0.015996   1.984   0.0472 *\nHOUR_CHANNEL_INTERACTION -0.009245   0.004688  -1.972   0.0486 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1109.0  on 799  degrees of freedom\nResidual deviance: 1104.5  on 797  degrees of freedom\nAIC: 1110.5\n\nNumber of Fisher Scoring iterations: 3\n\n\nResults Logistic Model 3:The backward elimination process significantly simplified the model leaving only two predictors: HOUR and HOUR_CHANNEL_INTERACTION\nModel strength: Predictors are statistically significant, providing meaningful insights into how time-related factors influence application completion Both predictors are statistically significant (𝑝&lt;0.05 p&lt;0.05): HOUR:𝑝=0.0472 HOUR_CHANNEL_INTERACTION:𝑝=0.0486 Model Weakness: The model does not explain much variability for the target variable and only has two predictors and lacks any complexity for achieving any real accuracy\n\nlogistic_pred3 &lt;- predict(logistic_model3, newdata = test_data, type = \"response\")\nlogistic_class3 &lt;- ifelse(logistic_pred3 &gt; 0.5, 1, 0)\n\n\nconfusionMatrix(as.factor(logistic_class3), as.factor(test_data$POST_EVENT))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 75 59\n         1 28 38\n                                          \n               Accuracy : 0.565           \n                 95% CI : (0.4933, 0.6348)\n    No Information Rate : 0.515           \n    P-Value [Acc &gt; NIR] : 0.089291        \n                                          \n                  Kappa : 0.121           \n                                          \n Mcnemar's Test P-Value : 0.001298        \n                                          \n            Sensitivity : 0.7282          \n            Specificity : 0.3918          \n         Pos Pred Value : 0.5597          \n         Neg Pred Value : 0.5758          \n             Prevalence : 0.5150          \n         Detection Rate : 0.3750          \n   Detection Prevalence : 0.6700          \n      Balanced Accuracy : 0.5600          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nRunning this model on the test data and seeing the results in the confusion matrix confirms the logistic regression model achieved an accuracy of 56.5%, which is only slightly better than chance. The model performed well in identifying non-application completers (Class 0), with a sensitivity of 72.8%, but struggled to detect application completers (Class 1), with a specificity of only 39.2%. The positive predictive value (PPV) for non-completers was moderate at 55.97%, while the negative predictive value (NPV) for completers was slightly higher at 57.58%, indicating weakness in predicting the positive class accurately.\nThe Kappa score of 0.121 suggests minimal agreement between predicted and actual classes beyond chance, and a balanced accuracy of 56.0% reflects mediocre performance across both classes.\n#SVM (support vector machine) model\n\nset.seed(321)\n\nsvmfit1 &lt;- svm(POST_EVENT ~ ., \n               data = train_data, \n               type = \"C-classification\", \n               kernel = \"radial\",\n               cost = 1,   # Cost parameter, can adjust for tuning\n               scale = TRUE # Scales data to zero mean and unit variance\n)\n\n\nsummary(svmfit1)\n\n\nCall:\nsvm(formula = POST_EVENT ~ ., data = train_data, type = \"C-classification\", \n    kernel = \"radial\", cost = 1, scale = TRUE)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  1 \n\nNumber of Support Vectors:  782\n\n ( 391 391 )\n\n\nNumber of Classes:  2 \n\nLevels: \n 0 1\n\n\nrunning on test data and reviewing performance\n\ntest_data$POST_EVENT &lt;- as.factor(test_data$POST_EVENT)\n\nsvm_pred &lt;- factor(predict(svmfit1, newdata = test_data), levels = levels(test_data$POST_EVENT))\n\n\nsvm_cm &lt;- confusionMatrix(svm_pred, test_data$POST_EVENT)\nprint(svm_cm)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 60 52\n         1 43 45\n                                          \n               Accuracy : 0.525           \n                 95% CI : (0.4534, 0.5959)\n    No Information Rate : 0.515           \n    P-Value [Acc &gt; NIR] : 0.4163          \n                                          \n                  Kappa : 0.0466          \n                                          \n Mcnemar's Test P-Value : 0.4118          \n                                          \n            Sensitivity : 0.5825          \n            Specificity : 0.4639          \n         Pos Pred Value : 0.5357          \n         Neg Pred Value : 0.5114          \n             Prevalence : 0.5150          \n         Detection Rate : 0.3000          \n   Detection Prevalence : 0.5600          \n      Balanced Accuracy : 0.5232          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nResults SVM Model 1: The SVM model achieved an accuracy of 52.5%, which is only slightly better than random guessing. It showed moderate sensitivity (58.25%) in identifying non-application completers (Class 0) and struggled with specificity (46.39%) which indicates difficulty in detecting application completers (Class 1). The 53.57% shows that the model has limited precision for predicting non-application completers, while the negative predictive value (51.14%) further shows its weak ability to predict application completions. A Kappa score of 0.0466 suggests minimal agreement between predicted and actual classes beyond chance, and the balanced accuracy of 52.32% reflects poor performance for both classes.\nOverall, this model has limited predictive power and may be in need for hyperparameter tuning or exploration of alternative approaches such as Random Forest.\nLet’s first trying tuning the SVM model to see if we can improve the performance…\n\ntune_svm &lt;- tune(\n  svm, \n  POST_EVENT ~ ., \n  data = train_data, \n  kernel = \"radial\", \n  ranges = list(\n    cost = c(0.1, 1, 10, 100),  \n    gamma = c(0.01, 0.1, 1)     \n  )\n)\n\nprint(tune_svm)\n\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n cost gamma\n   10     1\n\n- best performance: 0.2509784 \n\n\n\nsvmfit2 &lt;- svm(\n  POST_EVENT ~ ., \n  data = train_data, \n  type = \"C-classification\", \n  kernel = \"radial\", \n  cost = 10, \n  gamma = 1, \n  scale = TRUE\n)\n\nsvmfit2_pred &lt;- predict(svmfit2, newdata = test_data, type = \"class\")\n\ntest_data$POST_EVENT &lt;- as.factor(test_data$POST_EVENT)\nsvmfit2_pred &lt;- as.factor(svmfit2_pred)\n\n\nsvmfit2_cm &lt;- confusionMatrix(svmfit2_pred, test_data$POST_EVENT)\nprint(svmfit2_cm)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 103  97\n         1   0   0\n                                          \n               Accuracy : 0.515           \n                 95% CI : (0.4435, 0.5861)\n    No Information Rate : 0.515           \n    P-Value [Acc &gt; NIR] : 0.5285          \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 1.000           \n            Specificity : 0.000           \n         Pos Pred Value : 0.515           \n         Neg Pred Value :   NaN           \n             Prevalence : 0.515           \n         Detection Rate : 0.515           \n   Detection Prevalence : 1.000           \n      Balanced Accuracy : 0.500           \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nResults SVM Model 2: look way over-correcting for penalization\n#Ensemble method: Random Forest\n\n# Random Forest on original data\n#rf_model &lt;- randomForest(POST_EVENT ~ ., data = train_data, ntree = 500, mtry = 3, importance = TRUE)\n#print(rf_model)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#references",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#references",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.5 References",
    "text": "3.5 References\nI did get some of my EDA inspiration from : Link\nHere’s a video tutorial on how to say “Netflix and Chill” in 26 different languages:Link"
  },
  {
    "objectID": "DA6813_machine-learning-project/DA6813_machine-learning-project.html",
    "href": "DA6813_machine-learning-project/DA6813_machine-learning-project.html",
    "title": "DA6813 - Machine Learning Project",
    "section": "",
    "text": "Project Proposal: What Factors Influence Whether Bridgerton Estate Financial Customers Complete a Product Application?\nBackground Bridgerton Estate Financial is a one-of-a-kind banking institution that offers a range of financial products, including personal loans, credit cards, mortgages, and savings accounts. As a forward-thinking company, Bridgerton Estate Financial has invested heavily in its digital channels to engage with customers online, through mobile applications, and via web portals. Customers can explore products, ask questions, and complete applications directly through these platforms. To optimize the customer journey, Bridgerton Estate Financial tracks key digital events from their customers’ online behaviors. These events include interactions like product page visits, application starts, and completions or abandonments of forms. Our goal is to analyze this event tracking data to better understand consumer behavior and determine which factors drive customers to complete an application for a financial product or abandon the process.\nMotivation As a data-driven organization, Bridgerton Estate Financial wants to enhance its understanding of consumer behavior on digital platforms to: • Improve Conversion Rates: Identify the key factors that lead to successful application completions and reduce application drop-offs. • Enhance the Customer Experience: Pinpoint areas of friction in the application process and improve the user interface to streamline the experience. • Optimize Marketing and Customer Engagement: Gain insights into when and how to engage customers more effectively, such as understanding peak interaction times or preferred devices. By leveraging data from customer interactions, the goal is to answer critical questions about the online behavior of Bridgerton Estate Financial’s customers which would enable the company to make more informed decisions and improve customer conversion rates.\n\npacman::p_load(readr, here, dplyr, lubridate, corrplot, caret, ggplot2, car, e1071, vcd)\n\n\ndata_location &lt;- here::here(\"data-exercise\",\"Synthetic_Event_Data.csv\")\nrawdata &lt;- read_csv(data_location, show_col_types = FALSE)\n\n\nstr(rawdata)\n\nspc_tbl_ [1,000 × 17] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ EVENT_DT                      : chr [1:1000] \"10/17/2024\" \"10/17/2024\" \"10/17/2024\" \"10/17/2024\" ...\n $ EVENT_EFFECTIVE_GMT_TS        : 'hms' num [1:1000] 11:48:00 13:39:00 13:13:00 06:56:00 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ SOURCE_CHANNEL_CD             : chr [1:1000] \"www\" \"www\" \"www\" \"mob\" ...\n $ SOURCE_EVENT_ID               : chr [1:1000] \"bk-auto-loan-app-0\" \"bk-lending-cloe-fe-closing-1\" \"bk-card-cco-apply-2\" \"bk-auto-loan-app-3\" ...\n $ SOURCE_EVENT_DESC_TXT         : chr [1:1000] \"cco_approve_edit_card\" \"auto_loan_app_approved\" \"auto_loan_app_approved\" \"auto_loan_app_approved\" ...\n $ CORE_EVENT_TYPE_NM            : chr [1:1000] \"CSL\" \"ASL\" \"CSL\" \"CRC\" ...\n $ CORE_EVENT_SUBTYPE_NM         : chr [1:1000] \"DEC\" \"DEC\" \"DEC\" \"ACQ\" ...\n $ CORE_EVENT_NM_NOUN_NM         : num [1:1000] 655 690 760 771 776 603 600 786 661 719 ...\n $ ORIGINATING_CONTACT_POINT     : chr [1:1000] \"contact_point_6\" \"contact_point_5\" \"contact_point_4\" \"contact_point_3\" ...\n $ DESTINATION_CONTACT_POINT     : chr [1:1000] \"contact_point_3\" \"contact_point_4\" \"contact_point_9\" \"contact_point_6\" ...\n $ OTHER_ATTRIBUTES              : chr [1:1000] \"attribute_10\" \"attribute_10\" \"attribute_10\" \"attribute_4\" ...\n $ RELATED_PRODUCT               : chr [1:1000] \"product_4\" \"product_8\" \"product_10\" \"product_6\" ...\n $ COMMUNICATION_EVENT_ATTRIBUTES: chr [1:1000] \"comm_event_9\" \"comm_event_2\" \"comm_event_4\" \"comm_event_4\" ...\n $ CORE_EVENT_SUBTYPE_2_NM       : chr [1:1000] \"CLM\" \"CLM\" \"PRC\" \"CLM\" ...\n $ CORE_EVENT_SUBTYPE_3_NM       : chr [1:1000] \"APPLY FOR LOAN\" \"APPLY FOR LOAN\" \"APPLY FOR CONSUMER LOAN\" \"OPEN CREDIT CARD ACCOUNT\" ...\n $ POST_EVENT                    : chr [1:1000] \"App Not Complete\" \"App Not Complete\" \"App Not Complete\" \"App Not Complete\" ...\n $ CUSTOM_ATTRIBUTE              : chr [1:1000] \"custom_attr_6\" \"custom_attr_3\" \"custom_attr_4\" \"custom_attr_8\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   EVENT_DT = col_character(),\n  ..   EVENT_EFFECTIVE_GMT_TS = col_time(format = \"\"),\n  ..   SOURCE_CHANNEL_CD = col_character(),\n  ..   SOURCE_EVENT_ID = col_character(),\n  ..   SOURCE_EVENT_DESC_TXT = col_character(),\n  ..   CORE_EVENT_TYPE_NM = col_character(),\n  ..   CORE_EVENT_SUBTYPE_NM = col_character(),\n  ..   CORE_EVENT_NM_NOUN_NM = col_double(),\n  ..   ORIGINATING_CONTACT_POINT = col_character(),\n  ..   DESTINATION_CONTACT_POINT = col_character(),\n  ..   OTHER_ATTRIBUTES = col_character(),\n  ..   RELATED_PRODUCT = col_character(),\n  ..   COMMUNICATION_EVENT_ATTRIBUTES = col_character(),\n  ..   CORE_EVENT_SUBTYPE_2_NM = col_character(),\n  ..   CORE_EVENT_SUBTYPE_3_NM = col_character(),\n  ..   POST_EVENT = col_character(),\n  ..   CUSTOM_ATTRIBUTE = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nI will need to convert EVENT_DT to a date type and POST_EVENT to a factor as this is my target variable. I also think there will be some use in know specific day of the week and hour for consumer’s digital activity so I’m going to break this out into their own variables. To do the ‘HOUR’ parsing I will need the package ‘Lubridate’\n\nrawdata &lt;- rawdata %&gt;%\n  mutate(\n    # Converting EVENT_DT to a Date format\n    EVENT_DT = as.Date(EVENT_DT, format = \"%m/%d/%Y\"), #redundant because it's only 1 day of data\n    \n    # Properly encoding POST_EVENT as a binary numeric variable (0 for \"App Not Complete\", 1 for \"App Complete\")\n    POST_EVENT = ifelse(POST_EVENT == \"App Complete\", 1, 0),\n    \n    # Extracting the day of the week from EVENT_DT --for possible future use if I get more observations\n    #DAY_OF_WEEK = weekdays(EVENT_DT),\n    \n    # Extracting the hour of interaction from EVENT_EFFECTIVE_GMT_TS\n    HOUR = hour(EVENT_EFFECTIVE_GMT_TS),\n    \n    # Creating an interaction term between HOUR and SOURCE_CHANNEL_CD\n    HOUR_CHANNEL_INTERACTION = HOUR * as.numeric(as.factor(SOURCE_CHANNEL_CD))\n  ) %&gt;%\n  mutate_if(is.character, as.factor) # Converting all character variables to factors\n\nNow to inspect data for missing values\n\ncolSums(is.na(rawdata))\n\n                      EVENT_DT         EVENT_EFFECTIVE_GMT_TS \n                             0                              0 \n             SOURCE_CHANNEL_CD                SOURCE_EVENT_ID \n                             0                              0 \n         SOURCE_EVENT_DESC_TXT             CORE_EVENT_TYPE_NM \n                             0                              0 \n         CORE_EVENT_SUBTYPE_NM          CORE_EVENT_NM_NOUN_NM \n                             0                              0 \n     ORIGINATING_CONTACT_POINT      DESTINATION_CONTACT_POINT \n                             0                              0 \n              OTHER_ATTRIBUTES                RELATED_PRODUCT \n                             0                              0 \nCOMMUNICATION_EVENT_ATTRIBUTES        CORE_EVENT_SUBTYPE_2_NM \n                             0                              0 \n       CORE_EVENT_SUBTYPE_3_NM                     POST_EVENT \n                             0                              0 \n              CUSTOM_ATTRIBUTE                           HOUR \n                             0                              0 \n      HOUR_CHANNEL_INTERACTION \n                             0 \n\n\nChecking to see how balanced my response variable is\n\npost_event_balance &lt;- rawdata %&gt;%\n  count(POST_EVENT) %&gt;%\n  mutate(Proportion = n / sum(n))\n\nggplot(post_event_balance, aes(x = POST_EVENT, y = Proportion, fill = POST_EVENT)) +\n  geom_bar(stat = \"identity\", alpha = 0.8) +\n  geom_text(aes(label = paste0(round(Proportion * 100, 1), \"%\")), \n            position = position_stack(vjust = 0.5), \n            color = \"white\", \n            size = 5) +\n  labs(title = \"Distribution of Response (POST_EVENT): App Complete v App Not Complete\",\n       x = \"POST_EVENT\",\n       y = \"Proportion\") +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n#FEATURE ENGINEERING #Principal Component Analysis (PCA) SECTION for reducing and simplifying\n\nsingle_level_cols &lt;- names(Filter(function(x) length(unique(x)) == 1, rawdata))\n\nsingle_level_cols\n\n[1] \"EVENT_DT\"\n\n\nremove variable with only one level, EVENT_DT only had one day anyway\n\n# Removing Event_DT\nrawdata &lt;- rawdata[, !(names(rawdata) %in% single_level_cols)]\n\n\n# Subsetting the numeric variables in prep for PCA\nnumeric_data &lt;- rawdata %&gt;% select_if(is.numeric)\n\nscaled_numeric_data &lt;- scale(numeric_data)\n\n\n# Running PCA\npca_result &lt;- prcomp(scaled_numeric_data, scale. = TRUE)\n\nsummary(pca_result)\n\nImportance of components:\n                          PC1    PC2    PC3     PC4\nStandard deviation     1.3263 1.0065 0.9947 0.48817\nProportion of Variance 0.4398 0.2533 0.2474 0.05958\nCumulative Proportion  0.4398 0.6931 0.9404 1.00000\n\n\nInterpretation: PC1 has the highest standard deviation, meaning it captures the most variance, it explains 43.98% of the variance alone so combining it with PC2 and PCA3 explainS 94.04% of the variance. Running the Scree plot below confirms this, I only need to retain 3.\n\ncumulative_variance &lt;- cumsum(pca_result$sdev^2 / sum(pca_result$sdev^2))\nplot(cumulative_variance, type = \"b\", xlab = \"Number of Components\", \n     ylab = \"Cumulative Variance Explained\", \n     main = \"Scree Plot on Cumulative Variance\")\nabline(h = 0.95, col = \"red\", lty = 2) # shows the 95% threshold\n\n\n\n\nI would like to retain these 3 components moving forward because one of the benefits of doing this would be that the PCA components are uncorrelated (orthogonal), which should resolve any multicollinearity issues. Unfortunately, the downside would be that by transforming these original variables they would no longer be interpretable. While a logistic/SVM model might benefit from this, I don’t think I’ll need such a transformation anyway when using random forest down the line.\n#Checking collinearity in numerical variables\nRunning VIF\n\nvif_model &lt;- lm(POST_EVENT ~ ., data = numeric_data)\nvif_values &lt;- vif(vif_model)\nprint(vif_values)\n\n   CORE_EVENT_NM_NOUN_NM                     HOUR HOUR_CHANNEL_INTERACTION \n                1.002510                 2.359174                 2.360680 \n\n\nThe VIF results indicate that all the variables have very low VIF values, with none exceeding the commonly accepted thresholds for multicollinearity (e.g., 5 for moderate multicollinearity or 10 for high multicollinearity). This means that multicollinearity for the numeric values is not a concern in my dataset and can idealy remain in the model.\n#Checking collinearity in Categorical variables\n\ncategorical_columns &lt;- rawdata %&gt;% select_if(is.factor)\n\n# Running pairwise Chi-Square + Cramer's V to cross-check all highly associated variable pairings\npairwise_cramers_v &lt;- combn(names(categorical_columns), 2, function(cols) {\n  var1 &lt;- categorical_columns[[cols[1]]]\n  var2 &lt;- categorical_columns[[cols[2]]]\n  cramer_val &lt;- assocstats(table(var1, var2))$cramer\n  data.frame(Var1 = cols[1], Var2 = cols[2], CramersV = cramer_val)\n}, simplify = FALSE) %&gt;% bind_rows()\n\nhigh_association &lt;- pairwise_cramers_v %&gt;% filter(CramersV &gt; 0.7) # Threshold for strong association\nprint(high_association)\n\n                Var1                           Var2 CramersV\n1  SOURCE_CHANNEL_CD                SOURCE_EVENT_ID        1\n2    SOURCE_EVENT_ID          SOURCE_EVENT_DESC_TXT        1\n3    SOURCE_EVENT_ID             CORE_EVENT_TYPE_NM        1\n4    SOURCE_EVENT_ID          CORE_EVENT_SUBTYPE_NM        1\n5    SOURCE_EVENT_ID      ORIGINATING_CONTACT_POINT        1\n6    SOURCE_EVENT_ID      DESTINATION_CONTACT_POINT        1\n7    SOURCE_EVENT_ID               OTHER_ATTRIBUTES        1\n8    SOURCE_EVENT_ID                RELATED_PRODUCT        1\n9    SOURCE_EVENT_ID COMMUNICATION_EVENT_ATTRIBUTES        1\n10   SOURCE_EVENT_ID        CORE_EVENT_SUBTYPE_2_NM        1\n11   SOURCE_EVENT_ID        CORE_EVENT_SUBTYPE_3_NM        1\n12   SOURCE_EVENT_ID               CUSTOM_ATTRIBUTE        1\n\n\nInterpretation of results: The results of the Cramer’s V test indicate a perfect associations (Cramer’s V = 1) between SOURCE_EVENT_ID and over half of the other categorical variables. This means that SOURCE_EVENT_ID is highly predictive of these variables or vice versa, suggesting redundancy. I’m going to remove this variable from my dataset moving forward. Dropping EVENT_EFFECTIVE_GMT_TS variable as we have already created an ‘Hour’ variable out of it and don’t want to use both.\n\nrawdata &lt;- rawdata %&gt;% select(-SOURCE_EVENT_ID) # Dropping SOURCE_EVENT_ID from dataset\n\nDouble checking my data set to see if everything is ready for modeling\n\nstr(rawdata)\n\ntibble [1,000 × 17] (S3: tbl_df/tbl/data.frame)\n $ EVENT_EFFECTIVE_GMT_TS        : 'hms' num [1:1000] 11:48:00 13:39:00 13:13:00 06:56:00 ...\n  ..- attr(*, \"units\")= chr \"secs\"\n $ SOURCE_CHANNEL_CD             : Factor w/ 4 levels \"and\",\"iph\",\"mob\",..: 4 4 4 3 2 2 1 1 2 4 ...\n $ SOURCE_EVENT_DESC_TXT         : Factor w/ 3 levels \"auto_loan_app_approved\",..: 2 1 1 1 3 3 2 1 3 3 ...\n $ CORE_EVENT_TYPE_NM            : Factor w/ 3 levels \"ASL\",\"CRC\",\"CSL\": 3 1 3 2 1 2 1 2 3 1 ...\n $ CORE_EVENT_SUBTYPE_NM         : Factor w/ 3 levels \"ACQ\",\"ACQPL\",..: 3 3 3 1 2 2 1 3 1 1 ...\n $ CORE_EVENT_NM_NOUN_NM         : num [1:1000] 655 690 760 771 776 603 600 786 661 719 ...\n $ ORIGINATING_CONTACT_POINT     : Factor w/ 10 levels \"contact_point_1\",..: 7 6 5 4 4 8 8 3 9 8 ...\n $ DESTINATION_CONTACT_POINT     : Factor w/ 10 levels \"contact_point_1\",..: 4 5 10 7 1 3 4 2 8 7 ...\n $ OTHER_ATTRIBUTES              : Factor w/ 10 levels \"attribute_1\",..: 2 2 2 5 2 10 3 1 7 2 ...\n $ RELATED_PRODUCT               : Factor w/ 10 levels \"product_1\",\"product_10\",..: 5 9 2 7 8 3 3 7 6 1 ...\n $ COMMUNICATION_EVENT_ATTRIBUTES: Factor w/ 10 levels \"comm_event_1\",..: 10 3 5 5 4 1 4 6 10 5 ...\n $ CORE_EVENT_SUBTYPE_2_NM       : Factor w/ 3 levels \"CLM\",\"DEC\",\"PRC\": 1 1 3 1 1 2 3 1 3 2 ...\n $ CORE_EVENT_SUBTYPE_3_NM       : Factor w/ 3 levels \"APPLY FOR CONSUMER LOAN\",..: 2 2 1 3 3 2 1 3 2 3 ...\n $ POST_EVENT                    : num [1:1000] 0 0 0 0 1 0 1 0 1 0 ...\n $ CUSTOM_ATTRIBUTE              : Factor w/ 10 levels \"custom_attr_1\",..: 7 4 5 9 7 2 1 5 5 5 ...\n $ HOUR                          : int [1:1000] 11 13 13 6 0 21 12 10 19 14 ...\n $ HOUR_CHANNEL_INTERACTION      : num [1:1000] 44 52 52 18 0 42 12 10 38 56 ...\n\n\n#BEGIN MODELING SECTION**********************************************************\nIf I’m going to try models like SVM and Logistic Regression I may need to scale my data - (is this step needed?)\n\nrawdata_scaled &lt;- rawdata %&gt;%\n  mutate(across(where(is.numeric) & !all_of(\"POST_EVENT\"), scale))\n\n#SPLIT TRAIN/TEST\n\nset.seed(321)\n\ntrain_indices &lt;- createDataPartition(rawdata_scaled$POST_EVENT, p = 0.8, list = FALSE)\ntrain_data &lt;- rawdata_scaled[train_indices, ]\ntest_data &lt;- rawdata_scaled[-train_indices, ]\n\n#Logistic Regression (using PCA components dataset)\n\nnumeric_data &lt;- rawdata %&gt;% select_if(is.numeric)\nscaled_numeric_data &lt;- scale(numeric_data)\npca_result &lt;- prcomp(scaled_numeric_data, scale. = TRUE)\npca_transformed &lt;- as.data.frame(predict(pca_result, newdata = scaled_numeric_data)[, 1:3]) # Using only top 3 components\n\nrawdata_pca &lt;- cbind(POST_EVENT = rawdata$POST_EVENT, pca_transformed)\n\n\nlogistic_model &lt;- glm(POST_EVENT ~ ., data = rawdata_pca[train_indices, ], family = binomial)\n\nWarning: glm.fit: algorithm did not converge\n\nsummary(logistic_model)\n\n\nCall:\nglm(formula = POST_EVENT ~ ., family = binomial, data = rawdata_pca[train_indices, \n    ])\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)    -0.3743 12465.8147   0.000    1.000\nPC1             0.6449  9243.8807   0.000    1.000\nPC2            19.5005 12317.5677   0.002    0.999\nPC3            18.3588 12766.6379   0.001    0.999\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1.109e+03  on 799  degrees of freedom\nResidual deviance: 4.779e-09  on 796  degrees of freedom\nAIC: 8\n\nNumber of Fisher Scoring iterations: 25\n\n\nResults Logistic Model 1: model shit the bed - Write something here about using PCA components not being a working method for this\n#Logistic Regression (using rawdata set)\n\nset.seed(321)\ntrain_indices &lt;- createDataPartition(rawdata$POST_EVENT, p = 0.8, list = FALSE)\ntrain_data &lt;- rawdata[train_indices, ]\ntest_data &lt;- rawdata[-train_indices, ]\n\n\nlogistic_model2 &lt;- glm(POST_EVENT ~ ., data = train_data, family = binomial)\n\nsummary(logistic_model2)\n\n\nCall:\nglm(formula = POST_EVENT ~ ., family = binomial, data = train_data)\n\nCoefficients:\n                                                        Estimate Std. Error\n(Intercept)                                           -5.473e-01  1.203e+00\nEVENT_EFFECTIVE_GMT_TS                                -9.053e-05  7.501e-05\nSOURCE_CHANNEL_CDiph                                   8.395e-02  2.436e-01\nSOURCE_CHANNEL_CDmob                                   1.788e-01  3.220e-01\nSOURCE_CHANNEL_CDwww                                   3.563e-01  4.166e-01\nSOURCE_EVENT_DESC_TXTcco_approve_edit_card            -2.519e-01  1.920e-01\nSOURCE_EVENT_DESC_TXTpul_loan_app_post_bureau_decline -1.706e-01  1.909e-01\nCORE_EVENT_TYPE_NMCRC                                  8.732e-02  1.913e-01\nCORE_EVENT_TYPE_NMCSL                                  2.405e-01  1.951e-01\nCORE_EVENT_SUBTYPE_NMACQPL                             2.132e-02  1.853e-01\nCORE_EVENT_SUBTYPE_NMDEC                               1.075e-01  1.878e-01\nCORE_EVENT_NM_NOUN_NM                                  1.283e-03  1.365e-03\nORIGINATING_CONTACT_POINTcontact_point_10             -5.536e-01  3.373e-01\nORIGINATING_CONTACT_POINTcontact_point_2              -3.244e-01  3.449e-01\nORIGINATING_CONTACT_POINTcontact_point_3              -2.714e-02  3.593e-01\nORIGINATING_CONTACT_POINTcontact_point_4              -4.972e-01  3.684e-01\nORIGINATING_CONTACT_POINTcontact_point_5              -1.950e-01  3.287e-01\nORIGINATING_CONTACT_POINTcontact_point_6              -4.380e-01  3.333e-01\nORIGINATING_CONTACT_POINTcontact_point_7               3.325e-02  3.458e-01\nORIGINATING_CONTACT_POINTcontact_point_8              -8.129e-01  3.503e-01\nORIGINATING_CONTACT_POINTcontact_point_9              -5.126e-01  3.565e-01\nDESTINATION_CONTACT_POINTcontact_point_10             -1.431e-01  3.430e-01\nDESTINATION_CONTACT_POINTcontact_point_2               8.348e-02  3.399e-01\nDESTINATION_CONTACT_POINTcontact_point_3               1.153e-01  3.528e-01\nDESTINATION_CONTACT_POINTcontact_point_4              -2.408e-02  3.574e-01\nDESTINATION_CONTACT_POINTcontact_point_5               3.675e-02  3.535e-01\nDESTINATION_CONTACT_POINTcontact_point_6               5.276e-01  3.597e-01\nDESTINATION_CONTACT_POINTcontact_point_7               8.805e-02  3.567e-01\nDESTINATION_CONTACT_POINTcontact_point_8              -9.194e-02  3.520e-01\nDESTINATION_CONTACT_POINTcontact_point_9              -3.673e-01  3.432e-01\nOTHER_ATTRIBUTESattribute_10                          -6.796e-02  3.212e-01\nOTHER_ATTRIBUTESattribute_2                            2.586e-01  3.425e-01\nOTHER_ATTRIBUTESattribute_3                            1.835e-01  3.105e-01\nOTHER_ATTRIBUTESattribute_4                            8.968e-02  3.374e-01\nOTHER_ATTRIBUTESattribute_5                           -2.108e-01  3.435e-01\nOTHER_ATTRIBUTESattribute_6                           -2.760e-01  3.298e-01\nOTHER_ATTRIBUTESattribute_7                            6.711e-01  3.348e-01\nOTHER_ATTRIBUTESattribute_8                            1.511e-01  3.388e-01\nOTHER_ATTRIBUTESattribute_9                            3.594e-01  3.249e-01\nRELATED_PRODUCTproduct_10                              8.978e-01  3.496e-01\nRELATED_PRODUCTproduct_2                               5.074e-01  3.412e-01\nRELATED_PRODUCTproduct_3                               9.146e-02  3.552e-01\nRELATED_PRODUCTproduct_4                               3.756e-01  3.406e-01\nRELATED_PRODUCTproduct_5                               5.396e-01  3.578e-01\nRELATED_PRODUCTproduct_6                               1.073e+00  3.750e-01\nRELATED_PRODUCTproduct_7                               2.789e-01  3.593e-01\nRELATED_PRODUCTproduct_8                              -2.059e-01  3.619e-01\nRELATED_PRODUCTproduct_9                               3.406e-01  3.756e-01\nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_10           -8.177e-01  3.564e-01\nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_2            -3.083e-01  3.316e-01\nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_3            -2.630e-01  3.579e-01\nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_4            -7.216e-01  3.372e-01\nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_5            -6.123e-01  3.428e-01\nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_6            -8.269e-02  3.501e-01\nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_7            -8.027e-01  3.554e-01\nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_8            -2.135e-01  3.436e-01\nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_9            -5.039e-01  3.486e-01\nCORE_EVENT_SUBTYPE_2_NMDEC                            -3.048e-01  1.960e-01\nCORE_EVENT_SUBTYPE_2_NMPRC                            -6.515e-02  1.931e-01\nCORE_EVENT_SUBTYPE_3_NMAPPLY FOR LOAN                  7.126e-02  1.859e-01\nCORE_EVENT_SUBTYPE_3_NMOPEN CREDIT CARD ACCOUNT        2.286e-01  1.971e-01\nCUSTOM_ATTRIBUTEcustom_attr_10                        -6.731e-02  3.771e-01\nCUSTOM_ATTRIBUTEcustom_attr_2                         -1.335e-01  3.632e-01\nCUSTOM_ATTRIBUTEcustom_attr_3                         -4.031e-01  3.454e-01\nCUSTOM_ATTRIBUTEcustom_attr_4                         -3.308e-01  3.395e-01\nCUSTOM_ATTRIBUTEcustom_attr_5                         -1.720e-01  3.733e-01\nCUSTOM_ATTRIBUTEcustom_attr_6                         -3.132e-01  3.497e-01\nCUSTOM_ATTRIBUTEcustom_attr_7                         -1.263e-01  3.662e-01\nCUSTOM_ATTRIBUTEcustom_attr_8                         -3.190e-01  3.516e-01\nCUSTOM_ATTRIBUTEcustom_attr_9                         -2.851e-02  3.643e-01\nHOUR                                                   3.821e-01  2.723e-01\nHOUR_CHANNEL_INTERACTION                              -1.887e-02  1.010e-02\n                                                      z value Pr(&gt;|z|)   \n(Intercept)                                            -0.455  0.64920   \nEVENT_EFFECTIVE_GMT_TS                                 -1.207  0.22744   \nSOURCE_CHANNEL_CDiph                                    0.345  0.73035   \nSOURCE_CHANNEL_CDmob                                    0.555  0.57866   \nSOURCE_CHANNEL_CDwww                                    0.855  0.39243   \nSOURCE_EVENT_DESC_TXTcco_approve_edit_card             -1.312  0.18943   \nSOURCE_EVENT_DESC_TXTpul_loan_app_post_bureau_decline  -0.893  0.37169   \nCORE_EVENT_TYPE_NMCRC                                   0.457  0.64801   \nCORE_EVENT_TYPE_NMCSL                                   1.233  0.21763   \nCORE_EVENT_SUBTYPE_NMACQPL                              0.115  0.90840   \nCORE_EVENT_SUBTYPE_NMDEC                                0.572  0.56708   \nCORE_EVENT_NM_NOUN_NM                                   0.940  0.34698   \nORIGINATING_CONTACT_POINTcontact_point_10              -1.641  0.10076   \nORIGINATING_CONTACT_POINTcontact_point_2               -0.940  0.34699   \nORIGINATING_CONTACT_POINTcontact_point_3               -0.076  0.93980   \nORIGINATING_CONTACT_POINTcontact_point_4               -1.350  0.17712   \nORIGINATING_CONTACT_POINTcontact_point_5               -0.593  0.55302   \nORIGINATING_CONTACT_POINTcontact_point_6               -1.314  0.18885   \nORIGINATING_CONTACT_POINTcontact_point_7                0.096  0.92339   \nORIGINATING_CONTACT_POINTcontact_point_8               -2.321  0.02029 * \nORIGINATING_CONTACT_POINTcontact_point_9               -1.438  0.15049   \nDESTINATION_CONTACT_POINTcontact_point_10              -0.417  0.67655   \nDESTINATION_CONTACT_POINTcontact_point_2                0.246  0.80598   \nDESTINATION_CONTACT_POINTcontact_point_3                0.327  0.74376   \nDESTINATION_CONTACT_POINTcontact_point_4               -0.067  0.94627   \nDESTINATION_CONTACT_POINTcontact_point_5                0.104  0.91722   \nDESTINATION_CONTACT_POINTcontact_point_6                1.467  0.14241   \nDESTINATION_CONTACT_POINTcontact_point_7                0.247  0.80501   \nDESTINATION_CONTACT_POINTcontact_point_8               -0.261  0.79396   \nDESTINATION_CONTACT_POINTcontact_point_9               -1.070  0.28457   \nOTHER_ATTRIBUTESattribute_10                           -0.212  0.83241   \nOTHER_ATTRIBUTESattribute_2                             0.755  0.45024   \nOTHER_ATTRIBUTESattribute_3                             0.591  0.55459   \nOTHER_ATTRIBUTESattribute_4                             0.266  0.79037   \nOTHER_ATTRIBUTESattribute_5                            -0.614  0.53954   \nOTHER_ATTRIBUTESattribute_6                            -0.837  0.40262   \nOTHER_ATTRIBUTESattribute_7                             2.005  0.04499 * \nOTHER_ATTRIBUTESattribute_8                             0.446  0.65549   \nOTHER_ATTRIBUTESattribute_9                             1.106  0.26862   \nRELATED_PRODUCTproduct_10                               2.568  0.01023 * \nRELATED_PRODUCTproduct_2                                1.487  0.13693   \nRELATED_PRODUCTproduct_3                                0.257  0.79681   \nRELATED_PRODUCTproduct_4                                1.103  0.27019   \nRELATED_PRODUCTproduct_5                                1.508  0.13149   \nRELATED_PRODUCTproduct_6                                2.862  0.00421 **\nRELATED_PRODUCTproduct_7                                0.776  0.43769   \nRELATED_PRODUCTproduct_8                               -0.569  0.56948   \nRELATED_PRODUCTproduct_9                                0.907  0.36454   \nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_10            -2.295  0.02176 * \nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_2             -0.930  0.35251   \nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_3             -0.735  0.46238   \nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_4             -2.140  0.03237 * \nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_5             -1.786  0.07409 . \nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_6             -0.236  0.81330   \nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_7             -2.259  0.02390 * \nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_8             -0.621  0.53433   \nCOMMUNICATION_EVENT_ATTRIBUTEScomm_event_9             -1.445  0.14833   \nCORE_EVENT_SUBTYPE_2_NMDEC                             -1.555  0.11992   \nCORE_EVENT_SUBTYPE_2_NMPRC                             -0.337  0.73579   \nCORE_EVENT_SUBTYPE_3_NMAPPLY FOR LOAN                   0.383  0.70155   \nCORE_EVENT_SUBTYPE_3_NMOPEN CREDIT CARD ACCOUNT         1.160  0.24624   \nCUSTOM_ATTRIBUTEcustom_attr_10                         -0.178  0.85834   \nCUSTOM_ATTRIBUTEcustom_attr_2                          -0.368  0.71324   \nCUSTOM_ATTRIBUTEcustom_attr_3                          -1.167  0.24317   \nCUSTOM_ATTRIBUTEcustom_attr_4                          -0.974  0.32990   \nCUSTOM_ATTRIBUTEcustom_attr_5                          -0.461  0.64501   \nCUSTOM_ATTRIBUTEcustom_attr_6                          -0.896  0.37049   \nCUSTOM_ATTRIBUTEcustom_attr_7                          -0.345  0.73021   \nCUSTOM_ATTRIBUTEcustom_attr_8                          -0.907  0.36423   \nCUSTOM_ATTRIBUTEcustom_attr_9                          -0.078  0.93762   \nHOUR                                                    1.403  0.16056   \nHOUR_CHANNEL_INTERACTION                               -1.868  0.06178 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1109.0  on 799  degrees of freedom\nResidual deviance: 1027.6  on 728  degrees of freedom\nAIC: 1171.6\n\nNumber of Fisher Scoring iterations: 4\n\n\nResults Logistic Model 2: There appears to be a lot of variables with no significance, let’s apply some stepwise/backward elimination to our model\n\nlogistic_model3 &lt;- step(logistic_model2, direction = \"backward\")\n\nStart:  AIC=1171.64\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + SOURCE_CHANNEL_CD + SOURCE_EVENT_DESC_TXT + \n    CORE_EVENT_TYPE_NM + CORE_EVENT_SUBTYPE_NM + CORE_EVENT_NM_NOUN_NM + \n    ORIGINATING_CONTACT_POINT + DESTINATION_CONTACT_POINT + OTHER_ATTRIBUTES + \n    RELATED_PRODUCT + COMMUNICATION_EVENT_ATTRIBUTES + CORE_EVENT_SUBTYPE_2_NM + \n    CORE_EVENT_SUBTYPE_3_NM + CUSTOM_ATTRIBUTE + HOUR + HOUR_CHANNEL_INTERACTION\n\n                                 Df Deviance    AIC\n- CUSTOM_ATTRIBUTE                9   1030.8 1156.8\n- DESTINATION_CONTACT_POINT       9   1035.5 1161.5\n- OTHER_ATTRIBUTES                9   1038.7 1164.7\n- ORIGINATING_CONTACT_POINT       9   1039.4 1165.4\n- SOURCE_CHANNEL_CD               3   1028.4 1166.4\n- COMMUNICATION_EVENT_ATTRIBUTES  9   1040.5 1166.5\n- CORE_EVENT_SUBTYPE_NM           2   1028.0 1168.0\n- CORE_EVENT_SUBTYPE_3_NM         2   1029.0 1169.0\n- CORE_EVENT_TYPE_NM              2   1029.2 1169.2\n- SOURCE_EVENT_DESC_TXT           2   1029.4 1169.4\n- CORE_EVENT_SUBTYPE_2_NM         2   1030.4 1170.4\n- CORE_EVENT_NM_NOUN_NM           1   1028.5 1170.5\n- EVENT_EFFECTIVE_GMT_TS          1   1029.1 1171.1\n- HOUR                            1   1029.6 1171.6\n&lt;none&gt;                                1027.6 1171.6\n- HOUR_CHANNEL_INTERACTION        1   1031.2 1173.2\n- RELATED_PRODUCT                 9   1049.5 1175.5\n\nStep:  AIC=1156.76\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + SOURCE_CHANNEL_CD + SOURCE_EVENT_DESC_TXT + \n    CORE_EVENT_TYPE_NM + CORE_EVENT_SUBTYPE_NM + CORE_EVENT_NM_NOUN_NM + \n    ORIGINATING_CONTACT_POINT + DESTINATION_CONTACT_POINT + OTHER_ATTRIBUTES + \n    RELATED_PRODUCT + COMMUNICATION_EVENT_ATTRIBUTES + CORE_EVENT_SUBTYPE_2_NM + \n    CORE_EVENT_SUBTYPE_3_NM + HOUR + HOUR_CHANNEL_INTERACTION\n\n                                 Df Deviance    AIC\n- DESTINATION_CONTACT_POINT       9   1038.7 1146.7\n- OTHER_ATTRIBUTES                9   1042.2 1150.2\n- ORIGINATING_CONTACT_POINT       9   1042.9 1150.9\n- SOURCE_CHANNEL_CD               3   1031.6 1151.6\n- COMMUNICATION_EVENT_ATTRIBUTES  9   1044.0 1152.0\n- CORE_EVENT_SUBTYPE_NM           2   1031.2 1153.2\n- CORE_EVENT_SUBTYPE_3_NM         2   1032.1 1154.1\n- SOURCE_EVENT_DESC_TXT           2   1032.4 1154.4\n- CORE_EVENT_TYPE_NM              2   1032.6 1154.6\n- CORE_EVENT_SUBTYPE_2_NM         2   1033.4 1155.4\n- CORE_EVENT_NM_NOUN_NM           1   1031.8 1155.8\n- EVENT_EFFECTIVE_GMT_TS          1   1032.0 1156.0\n- HOUR                            1   1032.5 1156.5\n&lt;none&gt;                                1030.8 1156.8\n- HOUR_CHANNEL_INTERACTION        1   1034.4 1158.4\n- RELATED_PRODUCT                 9   1052.6 1160.6\n\nStep:  AIC=1146.7\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + SOURCE_CHANNEL_CD + SOURCE_EVENT_DESC_TXT + \n    CORE_EVENT_TYPE_NM + CORE_EVENT_SUBTYPE_NM + CORE_EVENT_NM_NOUN_NM + \n    ORIGINATING_CONTACT_POINT + OTHER_ATTRIBUTES + RELATED_PRODUCT + \n    COMMUNICATION_EVENT_ATTRIBUTES + CORE_EVENT_SUBTYPE_2_NM + \n    CORE_EVENT_SUBTYPE_3_NM + HOUR + HOUR_CHANNEL_INTERACTION\n\n                                 Df Deviance    AIC\n- OTHER_ATTRIBUTES                9   1049.8 1139.8\n- ORIGINATING_CONTACT_POINT       9   1050.8 1140.8\n- COMMUNICATION_EVENT_ATTRIBUTES  9   1051.4 1141.4\n- SOURCE_CHANNEL_CD               3   1040.0 1142.0\n- CORE_EVENT_SUBTYPE_NM           2   1039.0 1143.0\n- SOURCE_EVENT_DESC_TXT           2   1040.2 1144.2\n- CORE_EVENT_SUBTYPE_3_NM         2   1040.4 1144.4\n- CORE_EVENT_TYPE_NM              2   1040.5 1144.5\n- CORE_EVENT_NM_NOUN_NM           1   1039.6 1145.6\n- EVENT_EFFECTIVE_GMT_TS          1   1039.9 1145.9\n- CORE_EVENT_SUBTYPE_2_NM         2   1042.2 1146.2\n- HOUR                            1   1040.4 1146.4\n&lt;none&gt;                                1038.7 1146.7\n- HOUR_CHANNEL_INTERACTION        1   1043.2 1149.2\n- RELATED_PRODUCT                 9   1059.5 1149.5\n\nStep:  AIC=1139.76\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + SOURCE_CHANNEL_CD + SOURCE_EVENT_DESC_TXT + \n    CORE_EVENT_TYPE_NM + CORE_EVENT_SUBTYPE_NM + CORE_EVENT_NM_NOUN_NM + \n    ORIGINATING_CONTACT_POINT + RELATED_PRODUCT + COMMUNICATION_EVENT_ATTRIBUTES + \n    CORE_EVENT_SUBTYPE_2_NM + CORE_EVENT_SUBTYPE_3_NM + HOUR + \n    HOUR_CHANNEL_INTERACTION\n\n                                 Df Deviance    AIC\n- ORIGINATING_CONTACT_POINT       9   1062.0 1134.0\n- SOURCE_CHANNEL_CD               3   1050.6 1134.6\n- COMMUNICATION_EVENT_ATTRIBUTES  9   1063.4 1135.4\n- CORE_EVENT_SUBTYPE_NM           2   1050.3 1136.3\n- SOURCE_EVENT_DESC_TXT           2   1051.1 1137.1\n- CORE_EVENT_TYPE_NM              2   1051.2 1137.2\n- CORE_EVENT_SUBTYPE_3_NM         2   1051.5 1137.5\n- CORE_EVENT_SUBTYPE_2_NM         2   1052.2 1138.2\n- CORE_EVENT_NM_NOUN_NM           1   1050.8 1138.8\n- EVENT_EFFECTIVE_GMT_TS          1   1051.0 1139.0\n- HOUR                            1   1051.5 1139.5\n&lt;none&gt;                                1049.8 1139.8\n- RELATED_PRODUCT                 9   1069.0 1141.0\n- HOUR_CHANNEL_INTERACTION        1   1053.2 1141.2\n\nStep:  AIC=1134.01\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + SOURCE_CHANNEL_CD + SOURCE_EVENT_DESC_TXT + \n    CORE_EVENT_TYPE_NM + CORE_EVENT_SUBTYPE_NM + CORE_EVENT_NM_NOUN_NM + \n    RELATED_PRODUCT + COMMUNICATION_EVENT_ATTRIBUTES + CORE_EVENT_SUBTYPE_2_NM + \n    CORE_EVENT_SUBTYPE_3_NM + HOUR + HOUR_CHANNEL_INTERACTION\n\n                                 Df Deviance    AIC\n- SOURCE_CHANNEL_CD               3   1062.8 1128.8\n- COMMUNICATION_EVENT_ATTRIBUTES  9   1076.1 1130.1\n- CORE_EVENT_SUBTYPE_NM           2   1062.7 1130.7\n- SOURCE_EVENT_DESC_TXT           2   1062.8 1130.8\n- CORE_EVENT_TYPE_NM              2   1063.6 1131.6\n- CORE_EVENT_SUBTYPE_2_NM         2   1064.0 1132.0\n- CORE_EVENT_SUBTYPE_3_NM         2   1064.2 1132.2\n- EVENT_EFFECTIVE_GMT_TS          1   1063.2 1133.2\n- CORE_EVENT_NM_NOUN_NM           1   1063.4 1133.4\n- HOUR                            1   1063.6 1133.6\n&lt;none&gt;                                1062.0 1134.0\n- HOUR_CHANNEL_INTERACTION        1   1065.5 1135.5\n- RELATED_PRODUCT                 9   1081.8 1135.8\n\nStep:  AIC=1128.79\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + SOURCE_EVENT_DESC_TXT + \n    CORE_EVENT_TYPE_NM + CORE_EVENT_SUBTYPE_NM + CORE_EVENT_NM_NOUN_NM + \n    RELATED_PRODUCT + COMMUNICATION_EVENT_ATTRIBUTES + CORE_EVENT_SUBTYPE_2_NM + \n    CORE_EVENT_SUBTYPE_3_NM + HOUR + HOUR_CHANNEL_INTERACTION\n\n                                 Df Deviance    AIC\n- COMMUNICATION_EVENT_ATTRIBUTES  9   1077.2 1125.2\n- CORE_EVENT_SUBTYPE_NM           2   1063.5 1125.5\n- SOURCE_EVENT_DESC_TXT           2   1063.6 1125.6\n- CORE_EVENT_TYPE_NM              2   1064.5 1126.5\n- CORE_EVENT_SUBTYPE_2_NM         2   1064.8 1126.8\n- CORE_EVENT_SUBTYPE_3_NM         2   1065.0 1127.0\n- EVENT_EFFECTIVE_GMT_TS          1   1063.8 1127.8\n- CORE_EVENT_NM_NOUN_NM           1   1064.1 1128.1\n- HOUR                            1   1064.2 1128.2\n&lt;none&gt;                                1062.8 1128.8\n- RELATED_PRODUCT                 9   1082.4 1130.4\n- HOUR_CHANNEL_INTERACTION        1   1068.4 1132.4\n\nStep:  AIC=1125.25\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + SOURCE_EVENT_DESC_TXT + \n    CORE_EVENT_TYPE_NM + CORE_EVENT_SUBTYPE_NM + CORE_EVENT_NM_NOUN_NM + \n    RELATED_PRODUCT + CORE_EVENT_SUBTYPE_2_NM + CORE_EVENT_SUBTYPE_3_NM + \n    HOUR + HOUR_CHANNEL_INTERACTION\n\n                           Df Deviance    AIC\n- CORE_EVENT_SUBTYPE_NM     2   1077.7 1121.7\n- SOURCE_EVENT_DESC_TXT     2   1078.2 1122.2\n- CORE_EVENT_TYPE_NM        2   1078.6 1122.6\n- CORE_EVENT_SUBTYPE_3_NM   2   1079.2 1123.2\n- CORE_EVENT_SUBTYPE_2_NM   2   1080.3 1124.3\n- CORE_EVENT_NM_NOUN_NM     1   1078.3 1124.3\n- EVENT_EFFECTIVE_GMT_TS    1   1078.6 1124.6\n- HOUR                      1   1079.0 1125.0\n&lt;none&gt;                          1077.2 1125.2\n- RELATED_PRODUCT           9   1095.7 1125.7\n- HOUR_CHANNEL_INTERACTION  1   1082.5 1128.5\n\nStep:  AIC=1121.72\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + SOURCE_EVENT_DESC_TXT + \n    CORE_EVENT_TYPE_NM + CORE_EVENT_NM_NOUN_NM + RELATED_PRODUCT + \n    CORE_EVENT_SUBTYPE_2_NM + CORE_EVENT_SUBTYPE_3_NM + HOUR + \n    HOUR_CHANNEL_INTERACTION\n\n                           Df Deviance    AIC\n- SOURCE_EVENT_DESC_TXT     2   1078.7 1118.7\n- CORE_EVENT_TYPE_NM        2   1079.1 1119.1\n- CORE_EVENT_SUBTYPE_3_NM   2   1079.7 1119.7\n- CORE_EVENT_SUBTYPE_2_NM   2   1080.7 1120.7\n- CORE_EVENT_NM_NOUN_NM     1   1078.8 1120.8\n- EVENT_EFFECTIVE_GMT_TS    1   1079.1 1121.1\n- HOUR                      1   1079.5 1121.5\n&lt;none&gt;                          1077.7 1121.7\n- RELATED_PRODUCT           9   1096.0 1122.0\n- HOUR_CHANNEL_INTERACTION  1   1083.1 1125.1\n\nStep:  AIC=1118.73\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + CORE_EVENT_TYPE_NM + CORE_EVENT_NM_NOUN_NM + \n    RELATED_PRODUCT + CORE_EVENT_SUBTYPE_2_NM + CORE_EVENT_SUBTYPE_3_NM + \n    HOUR + HOUR_CHANNEL_INTERACTION\n\n                           Df Deviance    AIC\n- CORE_EVENT_TYPE_NM        2   1080.1 1116.1\n- CORE_EVENT_SUBTYPE_3_NM   2   1080.9 1116.9\n- CORE_EVENT_SUBTYPE_2_NM   2   1081.5 1117.5\n- CORE_EVENT_NM_NOUN_NM     1   1079.8 1117.8\n- EVENT_EFFECTIVE_GMT_TS    1   1080.1 1118.1\n- HOUR                      1   1080.5 1118.5\n&lt;none&gt;                          1078.7 1118.7\n- RELATED_PRODUCT           9   1096.8 1118.8\n- HOUR_CHANNEL_INTERACTION  1   1084.4 1122.4\n\nStep:  AIC=1116.12\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + CORE_EVENT_NM_NOUN_NM + \n    RELATED_PRODUCT + CORE_EVENT_SUBTYPE_2_NM + CORE_EVENT_SUBTYPE_3_NM + \n    HOUR + HOUR_CHANNEL_INTERACTION\n\n                           Df Deviance    AIC\n- CORE_EVENT_SUBTYPE_3_NM   2   1082.3 1114.3\n- CORE_EVENT_SUBTYPE_2_NM   2   1082.9 1114.9\n- CORE_EVENT_NM_NOUN_NM     1   1081.3 1115.3\n- EVENT_EFFECTIVE_GMT_TS    1   1081.4 1115.4\n- HOUR                      1   1081.7 1115.7\n&lt;none&gt;                          1080.1 1116.1\n- RELATED_PRODUCT           9   1098.2 1116.2\n- HOUR_CHANNEL_INTERACTION  1   1085.5 1119.5\n\nStep:  AIC=1114.28\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + CORE_EVENT_NM_NOUN_NM + \n    RELATED_PRODUCT + CORE_EVENT_SUBTYPE_2_NM + HOUR + HOUR_CHANNEL_INTERACTION\n\n                           Df Deviance    AIC\n- CORE_EVENT_SUBTYPE_2_NM   2   1084.7 1112.7\n- EVENT_EFFECTIVE_GMT_TS    1   1083.3 1113.3\n- CORE_EVENT_NM_NOUN_NM     1   1083.4 1113.4\n- HOUR                      1   1083.6 1113.6\n- RELATED_PRODUCT           9   1100.0 1114.0\n&lt;none&gt;                          1082.3 1114.3\n- HOUR_CHANNEL_INTERACTION  1   1087.1 1117.1\n\nStep:  AIC=1112.7\nPOST_EVENT ~ EVENT_EFFECTIVE_GMT_TS + CORE_EVENT_NM_NOUN_NM + \n    RELATED_PRODUCT + HOUR + HOUR_CHANNEL_INTERACTION\n\n                           Df Deviance    AIC\n- EVENT_EFFECTIVE_GMT_TS    1   1085.7 1111.7\n- RELATED_PRODUCT           9   1101.9 1111.9\n- HOUR                      1   1086.0 1112.0\n- CORE_EVENT_NM_NOUN_NM     1   1086.0 1112.0\n&lt;none&gt;                          1084.7 1112.7\n- HOUR_CHANNEL_INTERACTION  1   1089.1 1115.1\n\nStep:  AIC=1111.74\nPOST_EVENT ~ CORE_EVENT_NM_NOUN_NM + RELATED_PRODUCT + HOUR + \n    HOUR_CHANNEL_INTERACTION\n\n                           Df Deviance    AIC\n- RELATED_PRODUCT           9   1103.1 1111.1\n- CORE_EVENT_NM_NOUN_NM     1   1087.2 1111.2\n&lt;none&gt;                          1085.7 1111.7\n- HOUR_CHANNEL_INTERACTION  1   1090.3 1114.3\n- HOUR                      1   1090.5 1114.5\n\nStep:  AIC=1111.06\nPOST_EVENT ~ CORE_EVENT_NM_NOUN_NM + HOUR + HOUR_CHANNEL_INTERACTION\n\n                           Df Deviance    AIC\n- CORE_EVENT_NM_NOUN_NM     1   1104.5 1110.5\n&lt;none&gt;                          1103.1 1111.1\n- HOUR_CHANNEL_INTERACTION  1   1107.2 1113.2\n- HOUR                      1   1107.2 1113.2\n\nStep:  AIC=1110.48\nPOST_EVENT ~ HOUR + HOUR_CHANNEL_INTERACTION\n\n                           Df Deviance    AIC\n&lt;none&gt;                          1104.5 1110.5\n- HOUR_CHANNEL_INTERACTION  1   1108.4 1112.4\n- HOUR                      1   1108.5 1112.5\n\nsummary(logistic_model3)\n\n\nCall:\nglm(formula = POST_EVENT ~ HOUR + HOUR_CHANNEL_INTERACTION, family = binomial, \n    data = train_data)\n\nCoefficients:\n                          Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)              -0.119246   0.141714  -0.841   0.4001  \nHOUR                      0.031739   0.015996   1.984   0.0472 *\nHOUR_CHANNEL_INTERACTION -0.009245   0.004688  -1.972   0.0486 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1109.0  on 799  degrees of freedom\nResidual deviance: 1104.5  on 797  degrees of freedom\nAIC: 1110.5\n\nNumber of Fisher Scoring iterations: 3\n\n\nResults Logistic Model 3:The backward elimination process significantly simplified the model leaving only two predictors: HOUR and HOUR_CHANNEL_INTERACTION\nModel strength: Predictors are statistically significant, providing meaningful insights into how time-related factors influence application completion Both predictors are statistically significant (𝑝&lt;0.05 p&lt;0.05): HOUR:𝑝=0.0472 HOUR_CHANNEL_INTERACTION:𝑝=0.0486 Model Weakness: The model does not explain much variability for the target variable and only has two predictors and lacks any complexity for achieving any real accuracy\n\nlogistic_pred3 &lt;- predict(logistic_model3, newdata = test_data, type = \"response\")\nlogistic_class3 &lt;- ifelse(logistic_pred3 &gt; 0.5, 1, 0)\n\n\nconfusionMatrix(as.factor(logistic_class3), as.factor(test_data$POST_EVENT))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 75 59\n         1 28 38\n                                          \n               Accuracy : 0.565           \n                 95% CI : (0.4933, 0.6348)\n    No Information Rate : 0.515           \n    P-Value [Acc &gt; NIR] : 0.089291        \n                                          \n                  Kappa : 0.121           \n                                          \n Mcnemar's Test P-Value : 0.001298        \n                                          \n            Sensitivity : 0.7282          \n            Specificity : 0.3918          \n         Pos Pred Value : 0.5597          \n         Neg Pred Value : 0.5758          \n             Prevalence : 0.5150          \n         Detection Rate : 0.3750          \n   Detection Prevalence : 0.6700          \n      Balanced Accuracy : 0.5600          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nRunning this model on the test data and seeing the results in the confusion matrix confirms the logistic regression model achieved an accuracy of 56.5%, which is only slightly better than chance. The model performed well in identifying non-application completers (Class 0), with a sensitivity of 72.8%, but struggled to detect application completers (Class 1), with a specificity of only 39.2%. The positive predictive value (PPV) for non-completers was moderate at 55.97%, while the negative predictive value (NPV) for completers was slightly higher at 57.58%, indicating weakness in predicting the positive class accurately.\nThe Kappa score of 0.121 suggests minimal agreement between predicted and actual classes beyond chance, and a balanced accuracy of 56.0% reflects mediocre performance across both classes.\n#SVM (support vector machine) model\n\nset.seed(321)\n\nsvmfit1 &lt;- svm(POST_EVENT ~ ., \n               data = train_data, \n               type = \"C-classification\", \n               kernel = \"radial\",\n               cost = 1,   # Cost parameter, can adjust for tuning\n               scale = TRUE # Scales data to zero mean and unit variance\n)\n\n\nsummary(svmfit1)\n\n\nCall:\nsvm(formula = POST_EVENT ~ ., data = train_data, type = \"C-classification\", \n    kernel = \"radial\", cost = 1, scale = TRUE)\n\n\nParameters:\n   SVM-Type:  C-classification \n SVM-Kernel:  radial \n       cost:  1 \n\nNumber of Support Vectors:  782\n\n ( 391 391 )\n\n\nNumber of Classes:  2 \n\nLevels: \n 0 1\n\n\nrunning on test data and reviewing performance\n\ntest_data$POST_EVENT &lt;- as.factor(test_data$POST_EVENT)\n\nsvm_pred &lt;- factor(predict(svmfit1, newdata = test_data), levels = levels(test_data$POST_EVENT))\n\n\nsvm_cm &lt;- confusionMatrix(svm_pred, test_data$POST_EVENT)\nprint(svm_cm)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 60 52\n         1 43 45\n                                          \n               Accuracy : 0.525           \n                 95% CI : (0.4534, 0.5959)\n    No Information Rate : 0.515           \n    P-Value [Acc &gt; NIR] : 0.4163          \n                                          \n                  Kappa : 0.0466          \n                                          \n Mcnemar's Test P-Value : 0.4118          \n                                          \n            Sensitivity : 0.5825          \n            Specificity : 0.4639          \n         Pos Pred Value : 0.5357          \n         Neg Pred Value : 0.5114          \n             Prevalence : 0.5150          \n         Detection Rate : 0.3000          \n   Detection Prevalence : 0.5600          \n      Balanced Accuracy : 0.5232          \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nResults SVM Model 1: The SVM model achieved an accuracy of 52.5%, which is only slightly better than random guessing. It showed moderate sensitivity (58.25%) in identifying non-application completers (Class 0) and struggled with specificity (46.39%) which indicates difficulty in detecting application completers (Class 1). The 53.57% shows that the model has limited precision for predicting non-application completers, while the negative predictive value (51.14%) further shows its weak ability to predict application completions. A Kappa score of 0.0466 suggests minimal agreement between predicted and actual classes beyond chance, and the balanced accuracy of 52.32% reflects poor performance for both classes.\nOverall, this model has limited predictive power and may be in need for hyperparameter tuning or exploration of alternative approaches such as Random Forest.\nLet’s first trying tuning the SVM model to see if we can improve the performance…\n\ntune_svm &lt;- tune(\n  svm, \n  POST_EVENT ~ ., \n  data = train_data, \n  kernel = \"radial\", \n  ranges = list(\n    cost = c(0.1, 1, 10, 100),  \n    gamma = c(0.01, 0.1, 1)     \n  )\n)\n\nprint(tune_svm)\n\n\nParameter tuning of 'svm':\n\n- sampling method: 10-fold cross validation \n\n- best parameters:\n cost gamma\n   10     1\n\n- best performance: 0.2509784 \n\n\n\nsvmfit2 &lt;- svm(\n  POST_EVENT ~ ., \n  data = train_data, \n  type = \"C-classification\", \n  kernel = \"radial\", \n  cost = 10, \n  gamma = 1, \n  scale = TRUE\n)\n\nsvmfit2_pred &lt;- predict(svmfit2, newdata = test_data, type = \"class\")\n\ntest_data$POST_EVENT &lt;- as.factor(test_data$POST_EVENT)\nsvmfit2_pred &lt;- as.factor(svmfit2_pred)\n\n\nsvmfit2_cm &lt;- confusionMatrix(svmfit2_pred, test_data$POST_EVENT)\nprint(svmfit2_cm)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 103  97\n         1   0   0\n                                          \n               Accuracy : 0.515           \n                 95% CI : (0.4435, 0.5861)\n    No Information Rate : 0.515           \n    P-Value [Acc &gt; NIR] : 0.5285          \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 1.000           \n            Specificity : 0.000           \n         Pos Pred Value : 0.515           \n         Neg Pred Value :   NaN           \n             Prevalence : 0.515           \n         Detection Rate : 0.515           \n   Detection Prevalence : 1.000           \n      Balanced Accuracy : 0.500           \n                                          \n       'Positive' Class : 0               \n                                          \n\n\nResults SVM Model 2: look way over-correcting for penalization\n#Ensemble method: Random Forest\n\n# Random Forest on original data\n#rf_model &lt;- randomForest(POST_EVENT ~ ., data = train_data, ntree = 500, mtry = 3, importance = TRUE)\n#print(rf_model)"
  }
]